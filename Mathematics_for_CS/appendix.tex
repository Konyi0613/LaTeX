\chapter{Additional Proofs}
\section{A claim in the proof of \autoref{thm: greedy best-fit}}\label{appendix}
\begin{claim}
	If \(V\) is a finite-dimensional vector space, and \(\mathbf{v_1}, \mathbf{v_2}, \dots , \mathbf{v_{k-1}}   \) are all vectors in \(V\). If \(W\) is a \(k\)-dimensional subspace of \(V\), then there exists some vectors \(v\) in \(W\)  such that \(v\) is perpendicular to \(\mathbf{v_i} \) for all \(1 \le i \le k-1\).       
\end{claim}
\begin{explanation}
	Suppose \(U= \operatorname{span}\left\{ \mathbf{v_1}, \mathbf{v_2}, \dots , \mathbf{v_{k-1}} \right\} \), then define \(U^{\perp } \) as 
	\[
		U^{\perp } = \left\{ v \in V \mid \langle v,u \rangle=0 \ \forall u \in U  \right\}  .
	\]
	Now we want to show \(W \cap U^{\perp } \neq \left\{ 0 \right\} \). We can notice that \(\dim W = k\) and \(\dim U^{\perp } = \dim V - \dim U = \dim V - (k-1)\) since orthogonal set is linearly independent. Hence,
	\[
		\dim \left( W \cap U^{\perp }  \right) \ge \dim W + \dim U^{\perp } - \dim V = k + \left( \dim V - (k-1) \right) - \dim V = 1.
	\]
\end{explanation}

\section{\(A_k\) has rank \(k\)} \label{appendix: Ak rank k}
\begin{claim}
	If \(A\) is a \(n \times d\) matrix, and  
\[
	A_k=\sum_{i=1}^{k} \sigma _i \mathbf{u_i} \mathbf{v_i}^t  , 
\] 
where \(\mathbf{u_i}, \mathbf{v_i}  \) are the left, right singular vectors, then \(A_k\) has rank \(k\).
\end{claim}
\begin{explanation}
	First notice that
	\[
		A_k = \begin{pmatrix}
    \mathbf{u_1}  & \mathbf{u_2}  & \cdots  &\mathbf{u_k}
  \end{pmatrix}
  \begin{pmatrix}
    \sigma _1 &  &  &  \\
     &  \sigma _2&   &  \\
     &  &  \ddots&  \\
     &  &  & \sigma _k \\
  \end{pmatrix}
  \begin{pmatrix}
     \mathbf{v_1}  \\
      \mathbf{v_2} \\
      \vdots\\
      \mathbf{v_k}
  \end{pmatrix}.
	\]
Since \(\mathbf{u_i} \)'s and \(\mathbf{v_i} \)'s are orthogonal and hence linearly independent, so we have 
\[
	\rank\begin{pmatrix}
    \mathbf{u_1}  & \mathbf{u_2}  & \cdots  &\mathbf{u_k}
  \end{pmatrix} = \rank \begin{pmatrix}
     \mathbf{v_1}  \\
      \mathbf{v_2} \\
      \vdots\\
      \mathbf{v_k}
  \end{pmatrix} = k.
\] 

Besides, since 
\[
  \begin{pmatrix}
    \sigma _1 &  &  &  \\
     &  \sigma _2&   &  \\
     &  &  \ddots&  \\
     &  &  & \sigma _k \\
  \end{pmatrix}	
\]
is diagonal, and hence it also has rank \(k\). 

Now we need two claims to prove that \(A_k\) is of rank \(k\). 

\begin{claim}
	If \(X\) is an \(n \times k\) matrix and \(\rank  X = k\) (full column rank), then \(\rank (XY)= \rank Y\) for all \(Y\).     
\end{claim}
\begin{explanation}
	Notice that \(\ker X = \left\{ 0 \right\} \) by rank and nullity theorem. Besides, if \(v \in \ker XY\), then 
	\[
		X(Yv) = 0.
	\] However, this means \(Yv=0\) due to the above argument, so \(v \in \ker  Y\), which means \(\ker  XY \subseteq \ker  Y\) . Also, if \(w \in \ker  Y\), then \(XYw=X0=0\), so \(\ker Y \subseteq \ker XY\). Thus, \(\ker XY=\ker Y\). Now by rank and nullity theorem we know \(\rank XY = \rank Y\).       
\end{explanation}

\begin{claim}
	If \(Z\) is a \(k \times n\) matrix and \(\rank Z = k\) (full row rank), then \(\rank YZ = \rank Y\) for all \(Y\).      
\end{claim}
\begin{explanation}
Notice that 
\[
	\rank YZ = \rank Z^{t} Y^t = \rank Y^t = \rank Y.
\]
by the first claim, and so we're done.
\end{explanation}

Now by these two claims, it is easy to see that \(\rank A_k\) is equal to the rank of the middle diagonal matrix, which is of rank \(k\), so we're done.  
\end{explanation}

\section{Symmetric matrices have same left and right singular vectors.} \label{appendix: sym matrix same l,r SV}

\begin{claim}
  Suppose \(A\) is a square symmetric matrix, then \(A\)'s left, right singular vectors are identical. 
\end{claim}
\begin{explanation}
  First notice that all right singular vectors are \(A^t A\)'s eigenvectors, and all left singular vectors are \(A A^t\)'s eigenvectors. Now since \(A\) is symmetric, so \(A A^t = A^2 = A^t A\). Notice that 
  \[
    V D^2 V^t = V D U^t U D V^t = A^t A = A^2 = A A^t = UDV^t VD U^t = U D^2 U^t.
  \]  
  Hence, 
  \[
    (U-V) D^2 (U-V)^t = 0,
  \]    
  which means 
  \[
    \sum_{i=1}^{r} \sigma _i^2 (u_i - v_i)(u_i - v_i)^t = 0. 
  \]
  However, 
  \[
   (u_i - v_i)(u_i - v_i)^t 
  \]
  is positive semi-definite since for any \(x\) we have 
  \[
    x^t (u_i - v_i)(u_i - v_i)^t x = \left\lVert (u_i - v_i)^t x \right\rVert ^2 \ge 0
  \] 
  and by this, we know \(u_i = v_i\) for every \(i\).  
\end{explanation}

\section{Outer product on same vector gives rank one matrix} \label{appendix: outer product}

\begin{definition}
  For two vectors \( u \) and \( v \), we define the matrix \( uv^t \) as the \textit{outer product} of \( u \) and \( v \).
\end{definition}

\begin{claim}
  For any column vector \( v \in \mathbb{R}^n \), the matrix \( vv^t \in \mathbb{R}^{n \times n} \) is symmetric and of rank one (unless \( v = 0 \), in which case its rank is zero).
\end{claim}

\begin{explanation}
  Since \(\left( v v^t \right)^t = v v^t \), so it is symmetric. 
  For any \( x \in \mathbb{R}^n \), we compute:
  \[
    vv^t x = v(v^t x) = (v^t x) v \in \operatorname{span}(v)
  \]
  Hence, the image of the linear transformation defined by \( vv^t \) is contained in \( \operatorname{span}(v) \), which is a one-dimensional subspace (unless \( v = 0 \)). Thus, \( vv^t \) is at most rank one. Since \( vv^t \neq 0 \) when \( v \neq 0 \), its rank is exactly one.
\end{explanation}


\section{Dividing Frobenius norm} \label{appendix: divind Frobenius norm}

\begin{claim}
  If we have \(\sigma _1 > \sigma _2 \geq \sigma _3 \geq \dots \geq \sigma _r\), and supose \(A\) is square and symmetric (and hence has same left and right singular vectors)  , then 
  \[
    \frac{A^k}{\left\lVert A^k \right\rVert _F }
  \] will converge to \(\mathbf{v_1} \mathbf{v_1}^t  \) as \(k\) growing large. 
\end{claim}
\begin{explanation}
  First notice that 
  \[
    A^k = \sum_{i=1}^r \sigma _i^k \mathbf{v_i} \mathbf{v_i}^t. 
  \]
  Thus, 
  \[
    \left\lVert A^k \right\rVert _F^2 = \left\lVert \sum_{i=1}^r \sigma _i^k \mathbf{v_i} \mathbf{v_i}^t \right\rVert _F^2 = \sum_{i=1} ^r \sigma _i^{2k},
  \]
  and we have 
  \[
    \frac{A^k}{\left\lVert A^k \right\rVert _F} = \frac{\sum_{i=1}^r \sigma _i^k \mathbf{v_i} \mathbf{v_i}^t}{\left( \sum_{i=1} ^r \sigma _i^{2k} \right)^{\frac{1}{2}} } = \sum_{i=1}^{r} \frac{\sigma _i^k}{\left( \sum_{i=1} ^r \sigma _i^{2k} \right)^{\frac{1}{2}}} \mathbf{v_i} \mathbf{v_i}^t.
  \]
  Since we know 
  \[
    \begin{dcases}
     \frac{\sigma _i^k}{\left( \sum_{i=1} ^r \sigma _i^{2k} \right)^{\frac{1}{2}}} \to 1 , &\text{ if } i = 1  ;\\
      \frac{\sigma _i^k}{\left( \sum_{i=1} ^r \sigma _i^{2k} \right)^{\frac{1}{2}}} \to 0, &\text{ otherwise} .
    \end{dcases}
  \]
  so it converges to \(\mathbf{v_1} \mathbf{v_1}^t  \). 
\end{explanation}

\section{Why optimization problem subsumes set partition problem?} \label{appendix: optmization problem set partition}

We talk about the case that \(A\) is rank one and left, right singular vectors are same. 
Hence, we can suppose \(A = \mathbf{a} \mathbf{a} ^t\) for some column vector \(\mathbf{a} \) (Use spectral decomposition and since \(A\) is positive semi-definite so the eigenvalue is positive and thus we can merge the scalar into \(\mathbf{a} \)).

By this, we know 
\begin{align*}
  \mathbf{x}^t A (\mathbf{1} - \mathbf{x}  ) &= \mathbf{x} ^t \mathbf{a} \mathbf{a} ^t (\mathbf{1} - \mathbf{x}) = \left( \mathbf{a} ^t \mathbf{x}  \right) \cdot \left( \mathbf{a} ^t ( \mathbf{1} - \mathbf{x}  ) \right) \\
  &= \left( \sum_{i=1}^n a_i x_i  \right) \left( \sum_{j=1}^n a_j (1 - x_j)  \right) = \left( \sum_{i: x_i = 1} a_i  \right) \left( \sum_{j: x_j = 0} a_j \right).      
\end{align*}

Now we want to maximize this, and this occurs when
\[
 \left( \sum_{i=1}^n a_i x_i  \right) \quad \text{is most close to} \quad  \left( \sum_{j: x_j = 0} a_j \right),
\]
so it is a partition problem in disguise and thus a NP-hard problem.

\section{Why we deine margin like this?} \label{appendix: margin}

Here we talk about why the formula of the margin is
\[
  \min _i \frac{\left( \mathbf{w} \cdot \mathbf{a_i}  \right) l_i}{\left\vert \mathbf{w}  \right\vert }.
\]

Here, we need to know a theorem first. 
\begin{theorem}
  In a hyperplane \(\mathbf{w} \cdot \mathbf{x} = 0\), the distance from a point \(\mathbf{a_i} \) to this hyperplane is 
  \[
    \frac{\left\vert \mathbf{w} \cdot \mathbf{a_i}  \right\vert}{\vert \mathbf{w}  \vert }.
  \]  
\end{theorem}

We can think of \(\mathbf{w} \) is the normal vector of this hyperplane, and thus we can use \(\mathbf{w} \cdot \mathbf{a_i} \) to find the length of the component of \(\mathbf{a_i} \) along the direction of \(\mathbf{w} \) multiplied by the length of \(\mathbf{w} \).   

In the context above, we use \(l_i\) to ensure \(\left( \mathbf{w} \cdot \mathbf{a_i}  \right) \) is positive and thus we can drop the absolute value. 

\begin{corollary}
  In a hyperplane \(\mathbf{w} \cdot \mathbf{x} = b\), the distance from a point \(\mathbf{a_i} \)  to this hyperplane is 
  \[
    \frac{\left\vert \mathbf{w} \cdot \mathbf{a_i} - b  \right\vert }{\left\vert \mathbf{w}  \right\vert }.
  \]
\end{corollary}
\begin{proof}
  Notice that moving the plane \(\mathbf{w} \cdot \mathbf{x} = 0\) to \(\mathbf{w} \cdot \mathbf{x} = b\) is to move every point on the former hyperplane along the \(\frac{\mathbf{w} }{\lVert \mathbf{w}  \rVert }\) direction by \(\frac{b}{\lVert \mathbf{w}  \rVert }\) units. This is because suppose \(\mathbf{w} \cdot \mathbf{x_0} = 0 \), then 
  \[
    \mathbf{w} \cdot \left( \mathbf{x_0} + \frac{b}{\lVert \mathbf{w}  \rVert }\frac{\mathbf{w} }{\lVert \mathbf{w}  \rVert }  \right) = 0 + b = b.
  \]      
  Hence, we can imagine this formula is correct.
\end{proof}


































\chapter{Other Topics}
\section{Spectral Decomposition} \label{appendix: spectral decomposition}

Spectral decomposition is a way to express a real symmetric (or Hermitian, in complex case) matrix as a sum of projections onto its eigenvectors. It's a special case of diagonalization.

Here we just talk about when the field is \(\mathbb{R} \) and for self-adjoint matrices, but for the complex case and Hermitian matrices the definition is similar.

\begin{theorem} \label{thm: spectral decomposition original}
  If \(A \in M_{n \times n}(\mathbb{R} )\) and \(A\) is self-adjoint , then
  \[
    A = Q \Lambda Q^t
  \] 
  for some orthogonal \(Q\) and diagonal \(\Lambda \), and this is called the \textit{spectral decomposition} or the \textit{eigendecomposition} of \(A\).  
\end{theorem}

Now we can briefly talk about projections. 

\begin{definition}
  If we say \(P\) is a projection on \(V\), then it means \(\Im P = V\).   
\end{definition}
\begin{definition}
  An operator \(P\) is called a \textit{projection} if \(P^2=P\).  
\end{definition}

From geometric view, this means if \(V\) is the target space projected onto, then for every vector in \(V\), its projection is itself. Since suppose \(v \in V = \Im P\), then 
\[
  Pv = P (Px) = P^2 x = Px = v.
\]   

\begin{definition}
  An operator \(p\) is called an \textit{orthogonal projection} if \(P^2=P\) and \(P^t = P\).  
\end{definition}

The reason that \(P^t\) has to be equal to \(P\) is that if we see this in the geometric view, orthogonal projection means projecting a vector direcly onto the target space \(V\) , so if \(Px\) is the orthogonal projection from \(x\) to \(V\) , we want the property 
\[
  x - Px \perp V.
\]   

Now let \(x \in \mathbb{R} ^n\), and \(v = Px \in \Im P\), and then for any \(y \in \Im P\) (and thus \(py = y\) ), we know 
\[
  \langle x - Px, y \rangle = \langle x, y \rangle - \langle v, y \rangle,  
\]   
and since we know 
\[
  \langle x, y \rangle = x^t y = x^t Py = \left( P^t x \right)^t y = \langle P^t x, y \rangle  = \langle Px, y \rangle  = \langle v, y \rangle 
\]
, so we can have \(\langle x, y \rangle - \langle v, y \rangle = 0  \), and thus \(x - Px \perp V\). 

Here we introduce another equivalent form of spectral decomposition.

\begin{theorem}
  Suppose \(A\) is real symmetric, then \(A\) can be written as 
  \[
    A = \sum_{i=1}^n \lambda _i \mathbf{q_i} \mathbf{q_i}^t    
  \]  
  where \(\left\{ \mathbf{q_1}, \mathbf{q_2}, \dots , \mathbf{q_n}  \right\} \)  is an orthonal basis consisting of eigenvectors of \(A\) and \(\lambda _1, \lambda _2, \dots , \lambda _n\) are the eigenvalues of \(A\). 
\end{theorem}
\begin{proof}
  By \autoref{thm: spectral decomposition original}, if we pick \(\left\{ \mathbf{q_1}, \mathbf{q_2}, \dots , \mathbf{q_n} \right\} \) as the column vectors of \(Q\), where \(\mathbf{q_i} \) means the \(i\)-th column of \(Q\), then we know 
  \[
    Q \Lambda Q^t = \begin{bmatrix}
      \lambda _1 \mathbf{q_1}  & \lambda _2 \mathbf{q_2}  & \dots  & \lambda _n \mathbf{q_n}   \\
    \end{bmatrix} Q^t = \sum_{i=1}^{n} \lambda _i \mathbf{q_i} \mathbf{q_i}^t
  \]     since \(\diag \Lambda = \begin{pmatrix}
    \lambda _1 & \lambda _2 & \dots  & \lambda _n  \\
  \end{pmatrix}\).
\end{proof}

\begin{corollary}
  For a real symmetric \(A\), we know 
  \[
    A = \sum _{i=1}^n \lambda _i T_i
  \] 
  for some orthogonal projections \(T_i\), and \(T_i\) projects vectors on its component on the direction of \(\mathbf{v_i} \), where \(\mathbf{v_i} \) is an eigenvector of \(A\) and \(\left\{ \mathbf{v_1}, \mathbf{v_2}, \dots , \mathbf{v_n} \right\} \) forms an orthonormal basis of \(\mathbb{R} ^n\)  consisiting of eigenvectors of \(A\). 
\end{corollary}

\section{Linear Programming} \label{appendix: LP}
\begin{definition}
  Linear Programming is a method to maximize or minimize target linear function subject to some linear conditions. 
\end{definition}

For example, 
\[
  \text{maximize } c_1 x_1 + c_2 x_2 + \dots + c_d x_d 
\]
subject to 
\[
  \begin{dcases}
    a_{11} x_1 + a_{12} x_2 + \dots + a_{1d} x_d \leq b_1;\\
    a_{21} x_1 + a_{22} x_2 + \dots + a_{2d} x_d \leq b_2;\\
    \vdots \\
    a_{n1} x_1 + a_{n2} x_2 + \dots + a_{nd} x_d \leq b_n;\\
  \end{dcases}
\]
is a LP problem.
