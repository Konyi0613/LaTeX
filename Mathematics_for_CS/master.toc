\babel@toc {english}{}\relax 
\contentsline {chapter}{\numberline {1}Singular Value Decomposition (SVD)}{2}{chapter.1}%
\contentsline {section}{\numberline {1.1}Introduciton}{2}{section.1.1}%
\contentsline {section}{\numberline {1.2}Singular Vectors}{3}{section.1.2}%
\contentsline {section}{\numberline {1.3}Singular Value Decomposition (SVD)}{6}{section.1.3}%
\contentsline {section}{\numberline {1.4}Best Rank \(k\) Approximations}{7}{section.1.4}%
\contentsline {section}{\numberline {1.5}Computing Singular Values}{9}{section.1.5}%
\contentsline {section}{\numberline {1.6}Power Method for Computing the Singular Value Decomposition}{11}{section.1.6}%
\contentsline {section}{\numberline {1.7}Application of Singular Value Decomposition}{15}{section.1.7}%
\contentsline {subsection}{\numberline {1.7.1}Principal Component Analysis}{15}{subsection.1.7.1}%
\contentsline {subsection}{\numberline {1.7.2}Clustering a mixture of Spherical Gaussians}{16}{subsection.1.7.2}%
\contentsline {subsection}{\numberline {1.7.3}An Application of SVD to a Discrete Optimization Problem}{16}{subsection.1.7.3}%
\contentsline {subsection}{\numberline {1.7.4}SVD as a Compression Algorithm}{18}{subsection.1.7.4}%
\contentsline {subsection}{\numberline {1.7.5}Singular Vectors and ranking Documents}{20}{subsection.1.7.5}%
\contentsline {chapter}{\numberline {2}Learning and VC-dimension}{21}{chapter.2}%
\contentsline {section}{\numberline {2.1}Learning}{21}{section.2.1}%
\contentsline {section}{\numberline {2.2}Linear Separators, the Perception Algorithm, and Margins}{22}{section.2.2}%
\contentsline {subsection}{\numberline {2.2.1}The perceptron learning algorithm}{23}{subsection.2.2.1}%
\contentsline {subsection}{\numberline {2.2.2}Maximizing the Margin}{24}{subsection.2.2.2}%
\contentsline {subsubsection}{Maximum Margin Problem}{25}{section*.8}%
\contentsline {subsection}{\numberline {2.2.3}Linear Separators that classify most examples correctly}{26}{subsection.2.2.3}%
\contentsline {section}{\numberline {2.3}Nonlinear Separators, Support Vector Machines, and Kernels}{27}{section.2.3}%
\contentsline {chapter}{\numberline {A}Additional Proofs}{29}{appendix.A}%
\contentsline {section}{\numberline {A.1}A claim in the proof of \autoref {thm: greedy best-fit}}{29}{appendix.A.1}%
\contentsline {section}{\numberline {A.2}\(A_k\) has rank \(k\)}{29}{appendix.A.2}%
\contentsline {section}{\numberline {A.3}Symmetric matrices have same left and right singular vectors.}{30}{appendix.A.3}%
\contentsline {section}{\numberline {A.4}Outer product on same vector gives rank one matrix}{30}{appendix.A.4}%
\contentsline {section}{\numberline {A.5}Dividing Frobenius norm}{31}{appendix.A.5}%
\contentsline {section}{\numberline {A.6}Why optimization problem subsumes set partition problem?}{31}{appendix.A.6}%
\contentsline {section}{\numberline {A.7}Why we deine margin like this?}{32}{appendix.A.7}%
\contentsline {chapter}{\numberline {B}Other Topics}{33}{appendix.B}%
\contentsline {section}{\numberline {B.1}Spectral Decomposition}{33}{appendix.B.1}%
\contentsline {section}{\numberline {B.2}Linear Programming}{34}{appendix.B.2}%
