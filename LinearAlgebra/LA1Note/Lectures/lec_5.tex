\lecture{5}{17 Sep. 10:20}{}
\begin{prev}
    Given such a linear map \(T:V \to W\), we define 
    \begin{align*}
        \ker T &= T^{-1}(0) \quad \text{kernel/null space of }T \\
        \Im T  &= T(V) \quad \text{image/range of } T,
    \end{align*} 
    and \(\ker T\) is a subspaces of \(V\), and \(\Im T\) is a subspace of \(W\).   
\end{prev}

\begin{definition*}
    Now we define the nullity and rank of a linear map.
    \begin{definition}[nullity] \label{def:nullity}
        The nullity of \(T\) is the number 
        \[
            \nu (T) = \dim \ker T.
        \] 
    \end{definition}
    \begin{definition}[rank] \label{def:rank}
        The rank of \(T\) is the number \(\rank T = \dim \Im T\).  
    \end{definition}
\end{definition*}

\begin{eg}
    Suppose \(T: F^n \to F^m\), where \(F^n\) is the column space of dimension \(n\), then \(T = T_A\) for a matrix \(A \in M_{m \times n}(F)\) and \(T_A(v) = Av\).     
\end{eg}
\begin{explanation}
    Suppose \(A = (c_1, c_2, \dots , c_n)\), where \(c_i\) is the \(i\)-th  column vector of \(A\). Consider the standard basis \(\left\{ e_1, e_2, \dots , e_n \right\} \) of \(F^n\), where \(e_i\) is the column vector with \(i\)-th position \(1\) and the other entries are all \(0\)'s. Then, \(T_A(e_i) = c_i \in F^m\). Explicitly, 
    \[
        T_A \begin{pmatrix}
             x_1 \\
             \vdots \\
             x_n \\
        \end{pmatrix} = \begin{pmatrix}
            c_1 & \dots  & c_n  \\
        \end{pmatrix} \begin{pmatrix}
             x_1 \\
              \vdots\\
             x_n \\
        \end{pmatrix} = x_1 c_1 + \dots + x_n c_n
    \] since we know 
    \[
       \begin{pmatrix}
             x_1 \\
             \vdots \\
             x_n \\
        \end{pmatrix} = \sum_{i}x_i e_i.  
    \] and \(T_A(e_i) = c_i\).  In this case, 
    \begin{align*}
        \ker T_A &= \text{all linear relations among } c_1, \dots ,c_n \subseteq F^n \\
        \Im T_A &= \Span \left\{ c_1, \dots , c_n \right\} \subseteq F^m. 
    \end{align*}
    If we want to solve \(\ker T_A\), then we need to solve 
    \[
        0 = x_1 \begin{pmatrix}
             a_{11}  \\
             a_{21}  \\
             \vdots \\
             a_{m1} \\
        \end{pmatrix} + x_2 \begin{pmatrix}
             a_{12}  \\
             a_{22}  \\
             \vdots \\
             a_{m2} \\
        \end{pmatrix} + \dots + x_n \begin{pmatrix}
             a_{1n}  \\
             a_{2n}  \\
             \vdots \\
             a_{mn} \\
        \end{pmatrix}.
    \] Hence, we have to solve 
    \[
        \begin{dcases}
            a_{11} x_1 + \dots + a_{1n} x_n = 0 \\
            a_{21} x_1 + \dots + a_{2n} x_n = 0 \\
            \vdots \\
            a_{m1} x_1 + \dots + a_{mn} x_n = 0.
        \end{dcases}
    \]
    Given \(A = (c_1, \dots , c_n)_{m \times n}\), then the column rank is \(\dim \langle c_1, \dots , c_m \rangle \). If we rewrite \(A = (r_1, \dots , r_m)^t\), where \(r_i\) is the \(i\)-th row of \(A\), then the row rank is \(\dim \langle r_1, r_2, \dots , r_m \rangle \). Since we can define \(S_A: F^m \to F^n\), where 
    \[
        v = (x_1, \dots , x_m) \mapsto vA.
    \]     
    \begin{remark}
        In fact, column rank is equal to row rank in a matrix, and we will prove it later.
    \end{remark}
\end{explanation}

\begin{theorem}[rank and nullity theorem] \label{thm: rank and nullity theorem}
    Suppose \(T: V \to W\) is a linear map, then 
    \[
        \nu (T) + \rank T = \dim V.
    \] 
\end{theorem}
\begin{proof}
    Since \(\ker T \subseteq V\), so take a basis \(\left\{ v_1, \dots , v_{\nu} \right\} \) of \(\ker T\), and \(\Im T \subseteq W\), so take a basis \(\left\{ w_1, \dots , w_r \right\} \) of \(\Im T\). Take \(u_j\) s.t. \(T(u_j) = w_j\). 
    \begin{claim}
        \(S = \left\{ v_1, \dots , v_{\nu}, u_1, \dots , u_r \right\} \) forms a basis of \(V\).  
    \end{claim}   
    \begin{explanation}
        We first show that \(S\) is linearly independent. Suppose \(\sum \alpha _i v_i + \sum \beta_j u_j = 0 \). Apply \(T\) on it, we get
        \[
            0 = \sum \alpha _i T(v_i) + \sum \beta _j T(u_j) = \sum \alpha _i T(v_i) + \sum \beta _j w_j = \sum \beta _j w_j.   
        \]   However, \(\left\{ w_j \right\} \) is linearly independent, so \(\beta _j = 0\) for all \(j\). Now we know \(\sum \alpha _i v_i = 0 \), which means \(\alpha _i = 0\) for all \(i\), so \(S\) is linearly independent. Now we want to show \(\langle S \rangle = V \). Given \(v \in V\), we know \(T(v) \in \Im T\), and thus we can represent it as \(T(v) = \sum \beta _j w_j \). We want to show 
        \[
            v = \sum \alpha _i v_i + \sum \beta _j u_j.  
        \]    
        Thus, we want to show \(v - \sum \beta _j u_j \in \ker T \), but note that
        \[
            T \left( v - \sum \beta _j u_j  \right) = T(v) - \sum \beta _j w_j = \sum \beta _j w_j - \sum \beta _j w_j = 0,   
        \] so we're done, and thus we have 
        \[
            v - \sum \beta _j u_j = \sum \alpha _i v_i  
        \] for some \(\alpha _i\)'s, and we're done. 
    \end{explanation}    
    Hence, \(\dim V = \vert S \vert = \nu T + \rank T \). 
\end{proof}
\begin{remark} \label{rmk: dimV bigger than dimW then nuT bigger than 0}
    If \(\dim V > \dim W\), then \(\nu (T) > 0\). Since, \(\rank T \le \dim W\), so if \(\dim V > \dim W\), then we have \(\nu (T) = \dim V - \rank T \ge \dim V - \dim W > 0\).     
\end{remark}
\begin{prev}
    A map \(f: X \to  Y\) is called one-to-one or 1-1 or injective if \(f(x_1) = f(x_2)\) implies \(x_1 = x_2\). \(f\)  is called onto, surjective if \(f(X) = Y\). \(f\) is called bijective if it is both 1-1 and onto. In this case, there is the inverse map \(f^{-1}: Y \to X\) with \(y \mapsto x\) if \(f(x) = y\).   
\end{prev}
\begin{proposition} \label{prop: injective means ker has only 0}
    Let \(T:V \to W\) be linear, then \(T\) is injective iff \(\ker T = \left\{ 0 \right\}\). 
\end{proposition}   
\begin{proof}
    \vphantom{text}
    \begin{itemize}
        \item [\((\implies )\)] If \(v \in \ker T\), then since \(T(0) = 0\), so \(v=0\).  
        \item [\((\impliedby) \)] If \(T(v_1) = T(v_2)\), then \(T(v_1 - v_2) = 0\), which means \(v_1 - v_2 \in \ker T = \left\{ 0 \right\} \), so \(v_1 = v_2\), which means \(T\) is linear.  
    \end{itemize}
\end{proof}

\begin{proposition}
    If \(T:V \to W\) is a linear map, and if \(b\) is a basis of \(V\), then \(T\) is injective if and only if \(T(b)\) is linearly independent.   
\end{proposition}
\begin{proof}
    \vphantom{text}
    \begin{itemize}
        \item [\((\implies )\)] Suppose \(v_1, v_2, \dots , v_n\) is a basis of \(V\) and we want to show \(T(v_1), \dots , T(v_n)\) is linearly independent. Suppose \(\sum \alpha _i T(v_i) = 0 \), then \(T \left( \sum \alpha _i v_i  \right) = 0 \), so \(\sum \alpha _i v_i = 0 \), and thus \(\alpha _i = 0\) for all \(i\).   
        \item [\((\impliedby )\)] \(T\) sends one particular basis \(v_1, \dots , v_n\)  to a linearly independent set. We want to show \(\ker T = \left\{ 0 \right\} \). Suppose \(v \in \ker T\), then if \(v = \sum \alpha _i v_i \), we have 
        \[
            0 = T \left( \sum \alpha _i v_i  \right) = \sum \alpha _i T (v_i),  
        \] but since \(\left\{ T(v_i) \right\} \) is linearly independent, so \(\alpha _i = 0\) for all \(i\), which means \(v = 0\).     
    \end{itemize}
\end{proof}

\begin{proposition}\label{prop: surjective TFAE}
    If \(T:V \to W\) is a linear map, then TFAE 
    \begin{itemize}
        \item [(a)] \(T\) is surjective 
        \item [(b)] \(T\) sends any basis to a generating set. 
        \item [(c)] \(T\) sends one basis to a generating set.    
    \end{itemize} 
\end{proposition}

\begin{theorem}
    Suppose \(T: V \to  W\) is linear and bijective, then there is the inverse map \(T^{-1}: W \to  V\), and \(T^{-1} \) is also linear. In this case, \(T: V \to  W\) is called an isomorphism.    
\end{theorem}
\begin{remark}
    If there is an isomorphism from \(V\) to \(W\), we say \(V\) is isomorphic to \(W\), or \(V\) and \(W\) are isomorphic.      
\end{remark}

\begin{eg}[Coordinates]
    If \(\dim V = n\), then \(V\) is isomorphic to \(F^n\), we write \(V \simeq F^n\).   
\end{eg}
\begin{explanation}
    In fact, given an order basis \(B = \left\{ v_1, \dots , v_n \right\} \) of \(V\), then we know \(v = \sum_{i=1}^n \alpha _i v_i \), where
    \[
        v = \sum_{i=1}^n \alpha _i v_i \mapsto [v]_B = \begin{pmatrix}
             \alpha _1 \\
             \vdots \\
              \alpha _n \\
        \end{pmatrix}, 
    \]   and this is a bijection. Note that this map is well-defined since any \(v\) has unique coordinate under \(B\).  Hence, we have \(v_i \mapsto [v_i]_B = e_i\).  
\end{explanation}

Hence, if \(T: V \to W\), and we know \(V \simeq F^n\) and \(W \simeq F^m\), and we know there is a matrix sends \(F^n\) to \(F^m\), called \([T]_{B^{\prime} }^B\), and we can use it to represent the transformation from \(V\) to \(W\), which is \(T\).

\begin{exercise}
    \(T^{-1}(w_1 + w_2) = T^{-1}(w_1) + T^{-1}(w_2)\). 
\end{exercise}
\begin{proof}
    Suppose \(T(v_3) = w_1 + w_2\), we want to show \(v_3 = v_1 + v_2\). Hence, we need to check 
    \[
        w_1 + w_2 = T \left( T^{-1}(w_1) + T^{-1}(w_2) \right) = T \left( T^{-1}(w_1) \right) + T \left( T^{-1}(w_2)  \right) = w_1 + w_2, 
    \] which is true.
\end{proof}