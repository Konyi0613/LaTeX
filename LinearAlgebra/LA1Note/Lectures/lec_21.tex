\lecture{21}{26 Nov. 10:20}{}
\begin{remark}
    From now on, the note is not the lecture note since I didn't go to the lecture.
\end{remark}

\section{Cyclic Subspaces and Annihilators}
\begin{definition}
    If \(\alpha \) is a any vector in \(V\), the \(T\)-cyclic subspace generated by \(\alpha \) is the subspace \(Z(\alpha ; T)\) of all vectors of the form \(g(T)\alpha \), \(g \in F[x]\). If \(Z(\alpha ; T) = V\), then \(\alpha \) is called a cyclic vector for \(T\).          
\end{definition}

\begin{remark}
    Another way to describe \(Z(\alpha ; T)\) is that 
    \[
        Z(\alpha ; T) = \mathrm{span} \left\{ T^k \alpha  \right\}_{k \ge 0},  
    \] 
    and thus \(\alpha \) is a cyclic vector for \(T\) if and only if these vectors span \(V\). However, the general operator \(T\) has no cyclic vector.    
\end{remark}

\begin{definition}
    If \(\alpha \) is any vector in \(V\), the \(T\)-annihilator of \(\alpha \) is the ideal \(M(\alpha ; T)\) in \(F[x]\) consisting of all polynomials \(g\) over \(F\) such that \(g(T)\alpha = 0\). The unique moic polynomial \(p_\alpha \) which generates this ideal will also be called the \(T\)-annihilator of \(\alpha \).            
\end{definition}

\begin{remark}
    \(\deg p_\alpha > 0\) unless \(\alpha \) is the zero vector.  
\end{remark}

\begin{theorem}
    Let \(\alpha \) be any non-zero vector in \(V\) and let \(p_\alpha \) be the \(T\)-annihilator of \(\alpha \), then 
    \begin{itemize}
        \item [(i)] The degree of \(p_\alpha \) is equal to the dimension of the cyclic subspace \(Z(\alpha ; T)\). 
        \item [(ii)] If the degree of \(p_\alpha \) is \(k\), then the vectors \(\alpha, T \alpha , \dots , T^{k-1}\alpha  \) form a basis for \(Z(\alpha ; T)\). 
        \item [(iii)] If \(U\) is the linear operator on \(Z(\alpha ; T)\) induced by \(T\), then the minimal polynomial for \(U\) is \(p_\alpha \).           
    \end{itemize}     
\end{theorem}
\begin{proof}
    Let \(g \in F[x]\), then \(g = p_\alpha q + r\), where either \(r = 0\) or \(\deg(r) < \deg p_\alpha = k\). The polynomial \(p_\alpha q \in \mathrm{Ann}_T (\alpha ) \), so 
    \[
        g(T)\alpha = r(T) \alpha .
    \]      
    Since \(r = 0\) or \(\deg(r) < k\), the vector \(r(T)\alpha \) is a linear combination of the vectors \(\alpha , T \alpha , \dots T^{k-1} \alpha \), and thus 
    \[
        g(T) \alpha = r(T) \alpha \in \mathrm{span}\left\{ \alpha , T \alpha , \dots , T^{k-1} \alpha  \right\}.  
    \]   
    Since \(g\) can be arbitrary polynomial of \(F[x]\), so 
    \[
        Z(\alpha ; T) \subseteq \mathrm{span} \left\{ \alpha , T \alpha , \dots , T^{k-1} \alpha  \right\},
    \]
    and
    \[
        \mathrm{span} \left\{ \alpha , T \alpha , \dots , T^{k-1} \alpha  \right\} \subseteq Z(\alpha ; T)
    \] is trivial, so
    \[
        Z(\alpha ; T) = \mathrm{span} \left\{ \alpha , T \alpha , \dots , T^{k-1} \alpha  \right\}.
    \]
    Note that the set \(\left\{ \alpha , T \alpha , \dots , T^{k-1} \alpha  \right\} \) is linearly independent, otherwise if 
    \[
        \sum_{i=0}^{k-1} \beta _i T^i \alpha = 0, 
    \] where \(\beta _i\) are not all zeros, then 
    \[
        d(x) = \sum_{i=0}^{k-1} \beta _i x^i \in \mathrm{Ann}_T(\alpha ),  
    \] but \(\deg d \le k - 1 < k = \deg p_\alpha \), so this is impossible. Hence, 
    \[
        \left\{ \alpha , T \alpha , \dots , T^{k-1} \alpha  \right\} 
    \] is a basis of \(Z(\alpha ; T)\), and thus we showed (i) and (ii). Now we show (c). Note that 
    \[
        p_\alpha (U) \left( g(T) \alpha  \right) = p_\alpha (T) g(T) \alpha  = g(T) p_\alpha (T) \alpha = g(T) 0 = 0. 
    \]
    Hence, \(p_\alpha (U) = 0\). Now if \(h \in F[x]\) with \(\deg h < k\) and \(h(U) = 0\), then we know \(h(U) \alpha = h(T) \alpha = 0\), so \(h \in \mathrm{Ann}_T(\alpha ) \) and thus \(p_\alpha \mid h\), so this is impossible since \(\deg p_\alpha > \deg h\). Hence, \(\deg m_U \ge k\) and thus \(m_U = p_\alpha \).         
\end{proof}

\begin{corollary}
    If \(V = Z(\alpha ; T)\) and \(T \in \mathcal{L} (V)\), then \(\deg m_T(x) = \dim V\). Also, since \(\dim V = \deg \mathrm{ch}_T(x) \), so we have \(m_T(x) = \mathrm{ch}_T(x) \) since \(m_T(x) \mid \mathrm{ch}_T(x) \) and they are both monic.      
\end{corollary}

Now if \(U \in \mathcal{L} (W)\) where \(\dim W = k\) and \(W = Z(\alpha ; U)\), then we know 
\[
    \alpha , U \alpha , \dots , U^{k-1} \alpha 
\]  
form a basis for \(W\), and the annihilator \(p_\alpha \) of \(\alpha \) is \(m_U(x) = \mathrm{ch}_U(x) \) by the above corollary. If we let \(\alpha _i = U^{i-1} \alpha \) for \(i = 1, 2, \dots , k\), then suppose \(\mathcal{B} = \left\{ \alpha _1, \dots , \alpha _k \right\} \), we know the action of \(U\) on \(\mathcal{B} \) is 
\begin{align*}
    U \alpha _i &= \alpha _{i+1} \text{ for } i = 1, \dots , k - 1 \\
    U \alpha _k &= -c_0 \alpha _1 - c_1 \alpha _2 - \dots - c_{k-1} \alpha _k
\end{align*} 
where \(p_\alpha  = c_0 + c_1 x + \dots + c_{k-1} x^{k-1} + x^k\). The expression for \(U \alpha _k\) is true since \(p_\alpha (U) \alpha = 0\). Hence, 
\[
    [U]_{\mathcal{B} } = \begin{pmatrix}
        0 & 0 & \cdots & 0 & -c_0  \\
        1 & 0 & \cdots & 0 & -c_1  \\
        0 & 1 & \cdots & 0 & -c_2  \\
        \vdots & \vdots & \ddots & \vdots &  \vdots \\
        0 & 0 & \cdots & 1 & -c_{k-1}  \\
    \end{pmatrix},
\]            
which is called the companion matrix of the monic polynomial \(p_\alpha \). 

\begin{theorem} \label{thm: U has cyclic vector iff there is some ordered basis can represent U to the companion matrix of mU(x)}
    If \(U \in \mathcal{L} (W)\) where \(\dim W < \infty \), then \(U\) has a cyclic vector if and only if there is some ordered basis for \(W\) in which \(U\) is represented by the companion matrix of \(m_U(x)\).      
\end{theorem}
\begin{proof}
    We have shown that if \(U\) has a cyclic vector, then there is some ordered basis for \(W\) in which \(U\) is represented by the companion matrix of \(m_U(x)\).
    
    Now if \([U]_B\) is the companion matrix of \(m_U(x)\), then we know 
    \[
        B = \left\{ \alpha , U \alpha , \dots , U^{k-1} \alpha  \right\} 
    \] where \(k = \dim W\). 
\end{proof}

\begin{corollary}
    If \(A\) is the companion matrix of a monic polynomial \(p\), then \(p\) is both \(m_A(x)\) and \(\mathrm{ch}_A(x) \).     
\end{corollary}
\begin{proof}
    If we regard \(A\) as the matrix representation of \(T \in \mathcal{L} (V)\) with respect to the standard basis, then we know \(V = Z(\alpha ; T)\) for some \(\alpha \), and thus \(m_T(x) = \mathrm{ch}_T(x) \). Also, we can check this matrix's characteristic polynomial is \(p\) by direct computing.      
\end{proof}

\section{Cyclic Decompositions and the Rational Form}
The primary purpose of this section is to prove that if \(T\) is any linear operator on a finite-dimensional vector space \(V\), then there exist vectors \(\alpha _1, \dots , \alpha _r\) in \(V\) s.t. 
\[
    V = Z(\alpha _1; T) \oplus Z(\alpha _2; T) \oplus \dots \oplus Z(\alpha_r; T).
\]    

This will show for any operator \(T:V \to V\),
\[
    T = T_1 \oplus T_2 \oplus \dots \oplus T_r,
\] 
where \(T_i\)'s all have cyclic vectors. 

The cyclic theorem is closely related to the following question. If \(W\) is a \(T\)-invariant subspace, then is there another subspace \(W^{\prime} \) s.t. \(V = W \oplus W^{\prime} \)? Usually, the answer is yes, and there are many such \(W^{\prime} \), each of them is called \textbf{complementary} to \(W\).

\begin{question}
    When a \(T\)-invariant subspace has a complementary subspace which is also invariant under \(T\)?  
\end{question}

Now if we find such \(T\)-invariant \(W^{\prime} \), then for \(\beta \in V\), we know \(\beta = \gamma + \gamma ^{\prime} \) where \(\gamma \in W\) and \(\gamma ^{\prime} \in W^{\prime} \). If \(f \in F[x]\), then 
\[
    f(T) \beta = f(T) \gamma + f(T) \gamma ^{\prime} .
\]      
Since \(W\) and \(W^{\prime} \) are both invariant under \(T\), so we can find that \(f(T) \beta \in W\) if and only if \(f(T) \gamma ^{\prime} = 0\). Thus, if \(f(T)\beta  \in W\), then \(f(T) \beta = f(T) \gamma \).    

\begin{definition}
    Let \(T \in \mathcal{L} (V)\) and let \(W\) be a subspace of \(V\). We say that \(W\) is \(T\)-admissible if 
    \begin{itemize}
        \item [(i)] \(W\) is invariant under \(T\). 
        \item [(ii)] if \(f(T) \beta \) is in \(W\), there exists a vector \(\gamma \) in \(W\) s.t. \(f(T) \beta = f(T) \gamma \).       
    \end{itemize}      
\end{definition}

\begin{corollary}
    If \(W\) is invariant and has a complementary invariant subspace, then \(W\) is admissible.   
\end{corollary}

Let us indicate how the admissibility property is involved in the attempt to obtain a decomposition 
\[
    V = Z(\alpha _1; T) \oplus \dots \oplus Z(\alpha _r ; T).
\]
Our basic method for arriving such a decomposition will be to inductively select the vectors \(\alpha _1, \dots , \alpha _r\). Suppose that by some process or another we have selected \(\alpha _1, \dots , \alpha _j\), and the subspace 
\[
    W_j = Z(\alpha _1; T) + \dots + Z(\alpha _j ; T)
\]  
is proper. We would like to find a non-zero vector \(\alpha _{j+1}\) such that 
\[
    W_j \cap Z(\alpha _{j+1}; T) = \left\{ 0 \right\} 
\] 
because the subspace \(W_{j+1} = W_j \oplus Z(\alpha _{j+1}; T)\) would then come at least one dimension nearer to exhausting \(V\). But, why should any such \(\alpha _{j+1}\) exist? If \(\alpha _1, \alpha _2, \dots , \alpha _j\) have been chosen so that \(W_j\) is a \(T\)-admissible subspace, then it is rather easy to see that we can find a suitable \(\alpha _{j+1}\). We choose some \(\beta \notin W\). Consider the \(T\)-conductor \(S(\beta ; W)\), where 
\[
    S(\beta ; W) = \left\{ g \in F[x] : g(T) \beta \in W \right\}, 
\]              
then \(S(\beta ;W)\) is an ideal and suppose \(S(\beta ;W) = (s(\beta ;W))\). Let \(f = s(\beta ;W)\), then \(f(T) \beta \in W\). Now since \(W\) is \(T\)-admissible, there is a \(\gamma \in W\) s.t. \(f(T) \beta = f(T) \gamma \). Let \(\alpha = \beta - \gamma \) and let \(g\) be any polynomial. Since \(\beta - \alpha \in W\), so \(g(T) \beta \in W\) if and only if \(g(T) \alpha \in W\). In other words, \(S(\alpha ; W) = S(\beta ; W)\). Thus, \(f\) is also the \(T\)-conductor of \(\alpha \) into \(W\). But \(f(T) \alpha = 0\), so \(g(T) \alpha \in W\) iff \(f \mid g\) iff \(g(T) \alpha = p(T) f(T) \alpha = 0\). Thus, 
\[
    W \cap Z(\alpha ; T) = \left\{ 0 \right\}. 
\]                     

\begin{theorem}[Cyclic Decomposition Theorem]
    Let \(T \in \mathcal{L} (V)\) where \(\dim V < \infty \) and let \(W_0\) be a proper \(T\)-admissible subspace of \(V\). There exists non-zero vectors \(\alpha _1, \alpha _2, \dots , \alpha _r \in V\) with respective \(T\)-annihilators \(p_1, p_2, \dots , p_r\) s.t. 
    \begin{itemize}
        \item [(i)] \(V = W_0 \oplus Z(\alpha _1; T) \oplus \dots \oplus Z(\alpha _r ; T)\); 
        \item [(ii)] \(p_k\) divides \(p_{k-1}\) for \(k = 2, 3, \dots , r\).    
    \end{itemize}      
    Furthermore, the integer \(r\) and the annihilators \(p_1, p_2, \dots , p_r\) are uniquely determined by (i), (ii), and the fact that no \(\alpha _k = 0\).  
\end{theorem}
\begin{proof}
    The proof is rather long; hence, we shall divide it into four steps. For the first reading it may seem easier to take \(W_0 = \left\{ 0 \right\} \), although it does not produce any substantial simplication, Throughout the proof, we shall abbreviate \(f(T) \beta \) to \(f \beta \). 
    \begin{itemize}
        \item Step 1: There exist non-zero vectors \(\beta _1, \beta _2, \dots , \beta _r \in V\) s.t. 
        \begin{enumerate}[(a)]
            \item \(V = W_0 + Z(\beta _1 ; T) + \dots + Z(\beta _r ; T)\) 
            \item If \(1 \le k \le r\) and 
            \[
                W_k = W_0 + Z(\beta _1; T) + \dots + Z(\beta _k ; T),
            \]
            then the conductor \(p_k = s(\beta _k ; W_{k-1})\) has maximum degree among all \(T\)-conductors into the subspace \(W_{k-1}\), i.e. for every \(k\) we have 
            \[
                \deg p_k = \max_{\alpha \in V} \deg s(\alpha ; W_{k-1}).
            \]    
        \end{enumerate}
        This step only depends upon the fact that \(W_0\) is an invariant subspace. If \(W\) is a proper \(T\)-invariant subspace, then 
        \[
            0 < \max _{\alpha } \deg s(\alpha ;W) \le \dim V,
        \]   
        and we can choose a vector \(\beta \) so that \(\deg s(\beta ; W)\) attains that maximum. Note that \(\beta \notin W\) since for any vector \(w\) in \(W\), we have \(\deg s(w ; W) = 0\). By this, we know \(W + Z(\beta ;T)\) is then \(T\)-invariant and has dimension larger than \(\dim W\). Apply this process to \(W = W_0\) to obtain \(\beta _1\). If \(W_1 = W_0 + Z(\beta _1 ; T)\) is still proper, then apply the process to \(W_1\) to obtain \(\beta _2\). Continue in that manner. Since \(\dim W_k > \dim W_{k-1}\) in every step, so we must reach \(W_r = V\) in no more than \(\dim V\) steps. 
        \item Step 2: Let \(\beta _1, \beta _2, \dots , \beta _r\) be non-zero vectors which satisfy conditions (a) and (b) of Step 1. Fix \(k\) for some \(1 \le k \le r\). Let \(\beta \) be any vector in \(V\) and let \(f = s(\beta ; W_{k-1})\). If 
        \[
            f \beta = \beta _0 + \sum_{1 \le i < k} g_i \beta _i \quad \text{for } \beta_i \in W_i,  
        \]
        then \(f\) divides each polynomial \(g_i\) and \(\beta _0 = f \gamma _0\), where \(\gamma _0 \in W_0\). 
        
        If \(k = 1\), then this is just the statement that \(W_0\) is \(T\)-admissible. In order to prove the assertion for \(k > 1\), apply the division algorithm: 
        \[
            g_i = f h_i + r_i, \quad r_i = 0 \text{ or } \deg r_i < \deg f. 
        \]    
        We wish to show that \(r_i = 0\) for each \(i\). Let 
        \[
            \gamma = \beta - \sum_{i=1}^{k-1} h_i \beta _i, 
        \]  
        then since \(\gamma - \beta \in W_{k-1}\), so we have 
        \[
            s(\gamma ; W_{k-1}) = s(\beta ; W_{k-1}) = f
        \] 
        since \(p \gamma \in W_{k-1}\) iff \(p \beta \in W_{k-1}\) for any polynomial \(p\). Furthermore, 
        \[
            f \gamma = f \beta - \sum_{i=1}^{k-1} f h_i \beta _i = \beta _0 + \sum_{i=1}^{k-1} g_i \beta _i - \sum_{i=1}^{k-1} f h_i \beta _i = \beta _0 + \sum_{i=1}^{k-1} \gamma _i \beta _i.    
        \]
        Suppose that some \(r_i \neq 0\), then we shall deduce a contradiction. Let \(j\) be the largest index \(i\) for which \(r_i \neq 0\). Then 
        \[
            f \gamma = \beta _0 + \sum_{i=1}^j r_i \beta _i \quad r_j \neq 0 \text{ and } \deg r_j < \deg f.  
        \]       
        Let \(p = s(\gamma ; W_{j-1})\). Since \(W_{j - 1} \subseteq W_{k-1}\), so the conductor \(f = s(\gamma ; W_{k-1})\) must divide \(p\), i.e. 
        \[
            p = fg.
        \]    
        Apply \(g(T)\) to the both side of above equation we have 
        \[
            p \gamma = gf \gamma = g \beta _0 + g r_j \beta _j + \sum_{1 \le i < j} g r_i \beta _i. 
        \] 
        By definition, \(p \gamma \in W_{j-1}\), and \(g \beta _0\) and \(\sum_{1 \le i < j} g r_i \beta _i \) are in \(W_{j-1}\). Therefore, \(g r_j \beta _j \in W_{j-1}\). Now use condition (b) of Step 1: 
        \begin{align*}
            \deg (g r_i) &\ge \deg s(\beta _j ; W_{j-1}) \ge \deg s(\gamma ; W_{j-1}) \ge = \deg p = \deg fg.
        \end{align*}  
        Thus, \(\deg r_j \ge \deg f\), and that contradicts the choice of \(j\). We now know that \(f\) divides each \(g_i\) and hence that \(\beta _0 = f \gamma \). (Recall that \(\gamma = \beta - \sum_{i=1}^{k-1} h_i \beta _i \)) Since \(W_0\) is \(T\)-admissible, so \(\beta _0 = f \gamma _0\) where \(\gamma _0 \in W_0\). 
        \begin{remark}
            Step 2 is a strengthened form of the assertion that each of the subspace \(W_1, W_2, \dots , W_r\) is \(T\)-admissible.  
        \end{remark}         
    \end{itemize}   
\end{proof}








\section{The Jordan Form}
Suppose \(N\) is a nilpotent linear operator on the finite-dimensional space \(V\). Then suppose 
\[
    V = Z(\alpha _1; N) \oplus Z(\alpha _2; N) \oplus \dots \oplus Z(\alpha_r; N)
\]  
where \(p_1, p_2, \dots , p_r\) are the \(N\)-annihilaotrs of \(\alpha _1, \dots , \alpha _r\) and \(p_{i+1} \mid p_i\) for \(i = 1,2,\dots ,r-1\) (by the theorem in previous section). Since \(N\) is nilpotent, the minimal polynomial is \(x^k\) for some \(k \le n\). Thus, each \(p_i\) is of the form \(p_i = x^{k_i}\) and the divisibility condition says
\[
    k_1 \ge k_2 \ge \dots \ge k_r.
\]       
Note that we must have \(k_1 = k\) since \(m_N(x) = x^k\) and \(m_N(N)u = 0\) for all \(u \in Z(\alpha _i ; N)\) for all \(i = 1,2,\dots ,r\) and \(k\) should be the smallest number satisfying this condition. Also, \(k_r \ge 1\). The companion matrix of \(x^{k_i}\) is the \(k_i \times k_i\) matrix 
\[
    A_i = \begin{pmatrix}
        0 & 0 & \cdots & 0 & 0  \\
        1 & 0 & \cdots & 0 & 0  \\
        0 & 1 & \cdots & 0 & 0  \\
        \vdots & \vdots & \ddots & \vdots & \vdots  \\
        0 & 0 & \cdots & 1 & 0  \\
    \end{pmatrix},
\]         
so we know there exists an ordered basis of \(V\) in which the matrix of \(N\) is the direct product of \(A_i\)s for \(i = 1,2,\dots ,r\), where the size of \(A_i\)s decrease as \(i\) increases. Hence, one sees from this that associated with a nilpotent \(n \times n\) matrix is a positive integer \(r\) and \(r\) positive integers \(k_1, k_2, \dots , k_r\) s.t.
\[
    k_1 + k_2 + \dots + k_r = n \text{ and } k_i \ge k_{i+1}, 
\]           
and these positive integers determine the rational form of the matrix, i.e. determine the matrix up to similarity. 

\begin{proposition}
    \(r = \nu (N)\).  
\end{proposition}
\begin{proof}
    Since \([N] \sim A_1 \oplus A_2 \oplus \dots \oplus A_r\) where each \(A_i\) contributes one to the nullity, so the nullity of \(N\) is \(r\).    
\end{proof}

\begin{remark}
    \[
        \left\{ N^{k_1 - 1} \alpha _1, N^{k_2 - 1} \alpha _2, \dots , N^{k_r - 1} \alpha _r \right\} 
    \]
    form a basis of \(\ker N\) since \(N^{k_i} \alpha _i = 0\) and this set is linearly independent (since \(V\) is the direct product of \(Z(\alpha _i ; N)\)).  
\end{remark}

Hence, suppose \(\alpha \in \ker N\), then we know 
\[
    \alpha = f_1(N) \alpha _1 + \dots + f_r(N) \alpha _r
\] where \(f_i\) is a polynomial, the degree of which we may assume is less than \(k_i\). Since \(N \alpha = 0\), so we have 
\[
    0 = N \alpha = N (f_1(N)) \alpha _1 + \dots + N(f_r(N)) \alpha _r,
\]   
and since \(N(f_i(N)) \alpha _i \in Z(\alpha _i; N)\), so we know 
\[
    0 = N(f_i(N))\alpha _i = ((xf_i)(N)) \alpha _i
\] for all \(i\). Thus, \(x^{k_i} \mid x f_i\), so \(\deg (f_i) \ge k_i - 1\), so \(f_i = c_i x^{k_i - 1} \), where \(c_i\) is some scalar. But then
\[
    \alpha = c_1 \left( x^{k_1 - 1} \alpha _1 \right) + \dots + c_r \left( x^{k_r - 1} \alpha _r \right). 
\]     
This gives an alternative way to show that 
\[
    \left\{ N^{k_1 - 1} \alpha _1, \dots , N^{k_r - 1} \alpha _r \right\} 
\] form a basis of \(\ker N\). 

Now what we want to do is to combine our findings about nilpotent operators or matrices with the primary decomposition theorem.  The situation is this: Suppose \(T \in \mathcal{L} (V)\) and 
\[
    \mathrm{ch}_T(x) = (x - c_1)^{d_1} \dots (x - c_k)^{d_k}, 
\] where \(c_i \neq c_j\) for all \(i \neq j\) and \(d_i \ge 1\) for all \(i\). Then
\[
    m_T(x) = (x - c_1)^{r_1} \dots (x - c_k)^{r_k},
\]    
where \(1 \le r_i \le d_i\). If \(W_i = \ker \left( T - c_i I \right)^{r_i} \), then the primary decomposition theorem tells us 
\[
    V = W_1 \oplus \dots \oplus W_k,
\]  
ans that the operaotr \(T_i\) induced on \(W_i\) by \(T\) has 
\[
    m_{T_i}(x) = (x - c_i)^{r_i}.
\]   
Let \(N_i = T_i - c_i I\) where \(N_i \in \mathcal{L} (W_i)\), then \(N_i \) is nilpotent since 
\[
    0 = m_{T_i}(T_i) = (T_i - c_i I)^{r_i} = N_i^{r_i}
\]   
and 
\[
    m_{N_i}(x) = x^{r_i}.
\]
Hence, we know \(T_i = N_i + c_i I \) and if we choose a basis for the subspace \(W_i\) corresponding to the cyclic decomposition for the nilpotent operator \(N_i\), then 
\[
    [T_i] \sim \bigoplus \begin{pmatrix}
        0 & 0 & \cdots & 0 & 0  \\
        1 & 0 & \cdots & 0 & 0  \\
        0 & 1 & \cdots & 0 & 0  \\
        \vdots & \vdots & \ddots & \vdots & \vdots  \\
        0 & 0 & \cdots & 1 & 0  \\
    \end{pmatrix} + c_i I = \bigoplus \begin{pmatrix}
        c_i & 0 & \cdots & 0 & 0  \\
        1 & c_i & \cdots & 0 & 0  \\
        0 & 1 & \cdots & 0 & 0  \\
        \vdots & \vdots & \ddots & \vdots & \vdots  \\
        0 & 0 & \cdots & 1 & c_i  \\
    \end{pmatrix}.
\]  
Hence, if we pick all the basis for the \(W_i\) together, we obtain an ordered basis for \(V\). Let us describe hte matrix \(A\) of \(T\) in this ordered basis, then 
\[
    A = \begin{pmatrix}
        A_1 & 0 & \cdots & 0  \\
        0 & A_2 & \cdots & 0  \\
        \vdots & \vdots & \ddots & \vdots  \\
        0 & 0 & \cdots & A_k  \\
    \end{pmatrix}.
\]  
Each
\[
    A_i = \begin{pmatrix}
        J_1(c_i) &  &  &   \\
         & J_2(c_i) &  &   \\
         &  & \ddots &   \\
         &  &  & J_{n_i}(c_i)  \\
    \end{pmatrix}
\] where each \(J_s(c_i)\) is a Jordan block with characteristic value \(c_i\). In this case, we call this \(A\) is in Jordan form. 

\begin{remark}
    We can decompose \(N_i\) into the direct product of many companion matrix of monic polynomial of the form \(x^z\) since we can decompose \(W_i\) into the direct product of many \(N_i\)-cyclic subspace and suppose
    \[
        W_i = Z(\beta _{i1}; N_i) \oplus \dots \oplus Z(\beta _{i n_i}; N_i),
    \] 
    then by \autoref{thm: U has cyclic vector iff there is some ordered basis can represent U to the companion matrix of mU(x)} we know there exists basis \(B_{il}\) s.t. 
    \[
        [N_i]_{\bigcup_{l=1}^{n_i} B_{il}} = \bigoplus \begin{pmatrix}
            0 &  &  &   \\
            1 & 0 &  &   \\
            \vdots & \ddots & \ddots &   \\
            0 & \cdots & 1 & 0  \\
        \end{pmatrix}
    \]    
\end{remark}

\begin{remark}
    We have just pointed out that if \(T\) is a linear operator for which \(\mathrm{ch}_T(x) \) splits over the field, then there is an ordered basis for \(V\) in which \(T\) is represented by a matrix which is in Jordan form.     
\end{remark}

We should like to show now that this matrix is something uniquely assosciated with \(T\), up to the order in which the characteristic values of \(T\) is written down. In other words, if two matrices both in Jordan form and they are similar, then they can differ only in that the order of scalar \(c_i\) is different. 

The uniqueness we see as follows. Suppose there is some ordered basis for \(V\) in which \(T\) is represented by some Jordan matrix \(A\) described in the previous paragraph. If \(A_i\) is a \(d_i \times d_i\) matrix, then we know 
\[
    \mathrm{ch}_A(x) = \mathrm{ch}_{A_1}(x) \dots \mathrm{ch}_{A_k}(x),   
\]     
and note that 
\[
    (A_i - c_i I )^{d_i} = 0
\] so \(x^{d_i} \mid \mathrm{m}_{A_i}(x) \mid \mathrm{ch}_{A_i}(x)  \), and since \(\deg \mathrm{ch}_{A_i}(x) = d_i \), so we know \(\mathrm{ch}_{A_i}(x) = (x - c_i)^{d_i} \), which gives 
\[
    \mathrm{ch}_A(x) = (x - c_1)^{d_1} \dots (x - c_k)^{d_k},  
\]  
so the characteristic polynomial is unique given a matrix in Jordan form. Hence, for a given linear operator \(T\) with \(\mathrm{ch}_T(x) = (x - c_1)^{d_1} \dots (x - c_k)^{d_k} \), we know its Jordan form must be the direct product of \(A_i\)'s where \(A_i\) has diagonal entries all \(c_i\) and the only place that may not be unique is the size of the Jordan blocks. Hence, if we can prove the following theorem, then we can show that the Jordan form of \(T\) is unique. 

\begin{theorem}
    If \(N\) is 
    \[
        J_{s_1}(0) \oplus J_{s_2}(0) \oplus \dots \oplus J_{s_r}(0),
    \] then \(\left\{ s_i \right\}_{i=1}^r \) is unique up to permutation. 
\end{theorem}
\begin{proof}
    Note that 
    \[
        \dim \ker N^k = \sum_{i=1}^r \dim \ker \left( J_i(0)^k \right)  
    \]
    by rank and nullity theorem. Also, since \(J_i(0)\) is nilpotent, so we can observe that 
    \[
        \dim \ker \left( J_i(0)^k \right) = \min \left\{ s_i, k \right\}  
    \] since \(J_i(0)\) is of the form 
    \[
        \begin{pmatrix}
        0 & 0 & \cdots & 0 & 0  \\
        1 & 0 & \cdots & 0 & 0  \\
        0 & 1 & \cdots & 0 & 0  \\
        \vdots & \vdots & \ddots & \vdots & \vdots  \\
        0 & 0 & \cdots & 1 & 0  \\
    \end{pmatrix}
    \] and 
    \[
        \begin{pmatrix}
        0 & 0 & \cdots & 0 & 0  \\
        1 & 0 & \cdots & 0 & 0  \\
        0 & 1 & \cdots & 0 & 0  \\
        \vdots & \vdots & \ddots & \vdots & \vdots  \\
        0 & 0 & \cdots & 1 & 0  \\
    \end{pmatrix} \cdot \begin{pmatrix}
        0 & 0 & \cdots & 0 & 0  \\
        1 & 0 & \cdots & 0 & 0  \\
        0 & 1 & \cdots & 0 & 0  \\
        \vdots & \vdots & \ddots & \vdots & \vdots  \\
        0 & 0 & \cdots & 1 & 0  \\
    \end{pmatrix} = \begin{pmatrix}
        0 & 0 & \cdots & 0 & 0  \\
        0 & 0 & \cdots & 0 & 0  \\
        1 & 0 & \cdots & 0 & 0  \\
        \vdots & \vdots & \ddots & \vdots & \vdots  \\
        0 & 0 & \cdots & 0 & 0  \\
    \end{pmatrix},
    \]
    which shift the \(1\)-line left by \(1\). Hence, 
    \[
        \dim \ker N^k = \sum_{i=1}^r \min \left\{ s_i, k \right\},  
    \] 
    and thus we have 
    \begin{align*}
        \dim \ker N^{k+1} - \dim \ker N^k &= \sum_{i=1}^r \min \left\{ s_i, k+1 \right\} - \min \left\{ s_i, k \right\} \\
        &= \# \text{ of Jordan blocks of size } \ge k+1 \\
        &= \ell _{k+1},    
    \end{align*} 
    so the number of Jordan blocks of size \(k\) is \(\ell _k - \ell _{k+1}\) and 
    \[
        \ell _k - \ell _{k+1} = \left( \dim \ker N^k - \dim \ker N^{k-1} \right) - \left( \dim \ker N^{k+1} - \dim \ker N^k \right),  
    \]  
    which is unique for any \(N\), so we know \(\left\{ s_i \right\}_{i=1}^k \) is unique.  
\end{proof}

\begin{remark}
    We say this theorem shows the uniqueness of Jordan form since
    \[
        A = A_1 \oplus \dots \oplus A_k,
    \]
    and \(A_i = N_i + c_i I\), so if we show \(N_i\) is unique, then \(A_i\) is unique.   
\end{remark}

\begin{corollary}
    Given any linear operator \(T\), then if \(\mathrm{ch}_T(x) \) splits, then \(T\) has a unique Jordan form.  
\end{corollary}

Now we with to make some further observations about the operator \(T\) and the Jordan matrix \(A\) which represents \(T\) in some ordered basis.

\begin{itemize}
    \item [(i)] Every characteristic value \(c_i\) of \(T\) is repeated \(d_i\) times on the diagonal line, where \(d_i\) is the multiplicity of \(c_i\) as a root of \(\mathrm{ch}_T(x) \), i.e. \(d_i = \dim W_i\). 
    \item [(ii)] For each \(i\), the matrix \(A_i\) is the direct sum of \(n_i\) Jordan blocks with characteristic value \(c_i\). For \(n_i\), the number of Jordan blocks in \(A_i\), we know it is equal to \(\dim \ker (T - c_i I)\). In particular, \(T\) is diagonalizable if and only if \(n_i = d_i\) for eeach \(i\). 
    \item [(iii)] For each \(i\), the size of the biggest Jordan block in \(A_i\) is \(r_i\), where \(r_i\) is the multiplicity of \(c_i\) as a root of \(m_T(x)\). This follows from the fact that the minimal polynomial for the nilpotent operator \((T_i - c_i I)\) is \(x^{r_i}\).                      
\end{itemize}