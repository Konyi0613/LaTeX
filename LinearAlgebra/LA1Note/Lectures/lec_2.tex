\lecture{2}{5 Sep. 10:20}{}
\section{Vector Space over general field}
Now we introduce the concept of field. 
\begin{definition}[Field]\label{def: field}
    A set \(F\) with \(+\) and \(\cdot\) is called a \textbf{field} if 
    \begin{itemize}
        \item \(\alpha + \beta = \beta + \alpha \) and \((\alpha + \beta ) + \gamma = \alpha + (\beta + \gamma )\). 
        \item There exists \(0 \in F\) such that \(\alpha + 0 = 0 + \alpha = \alpha \). 
        \item For \(\alpha \in F\), there exists \(-\alpha \) such that \(\alpha + (-\alpha ) = 0\).
        \item \(\alpha \beta = \beta \alpha \) and \((\alpha \beta ) \gamma = \alpha (\beta \gamma )\)
        \item \(\exists 1\) such that \(1 \neq 0\) and \(1 \cdot \alpha = \alpha \). 
        \item For \(\alpha \neq 0\), \(\exists \alpha ^{-1} \in F\) such that \(\alpha \alpha ^{-1} = 1\).  
        \item \(\alpha (\beta + \gamma ) = \alpha \beta + \alpha \gamma \)             
    \end{itemize}  
\end{definition}

\begin{eg}
     \(\mathbb{Q}  \subseteq \mathbb{R} \subseteq \mathbb{C} \) are all fields but \(\mathbb{Z} \) is not. 
\end{eg}

\begin{eg}
    \(\left\{ 0,1 \right\} \) is also a field. 
\end{eg}

Now we know the concept of filed, so we can make a vector space over a field.

\begin{theorem}[Cancellation law] \label{thm: Cancellation Law}
Suppose \(v_1, v_2, w \in V\), a vector space, then if \(v_1 + w = v_2 + w\), then \(v_1 = v_2\).  
\end{theorem}
\begin{proof}
    \[
        v_1 = v_1 + (w + (-w)) = (v_1 + w) + (-w) = (v_2 + w) + (-w) = v_2 + (w + (-w)) = v_2.
    \]
\end{proof}

\begin{theorem}\label{thm: zero unique}
    The zero vector \(0\) is unique. 
\end{theorem}
\begin{proof}
    Suppose we have \(0, 0^{\prime} \) both zero vector, then for some \(0 = 0 + 0^{\prime} = 0^{\prime} \).  
\end{proof}

\begin{theorem} \label{thm: 0u=0}
For any \(v \in V\), \(0 \cdot u = 0\). 
\end{theorem}
\begin{proof}
    \(0 \cdot u = (0 + 0) \cdot u = 0 \cdot u + 0 \cdot u\), so \(0 = 0 \cdot u\) by \hyperref[thm: Cancellation Law]{cancellation law}.   
\end{proof}

\begin{theorem} \label{thm: (-1)u=-u}
    \((-1) \cdot u = -u\). 
\end{theorem}

\begin{theorem} \label{thm: addition inverse unique}
    Given any \(u \in V\) is unique, \(-u\) is unique.  
\end{theorem}

\section{Subspaces}
\begin{definition}[subspace] \label{def: subspace}
    Let \(V\) be a vector space. A non-empty subset \(W \subseteq V\) is called a subspace of \(V\) if \(W\) is itself a vector space under \(+\) and \(\cdot\) on \(V\).       
\end{definition}

\begin{eg}
    \(M_n(F) = \left\{ n \times n \text{ matrix with entries in } F \right\} \) is a vector space, and 
    \[
        U_n(F) = \left\{ \begin{pmatrix}
            a_{11}  &  &  &   \\
            0 & a_{22}  &  &   \\
            \vdots &  & \ddots &   \\
            0 & \cdots & 0 & a_{nn}  \\
        \end{pmatrix} \right\} 
    \] is a subspace of \(M_n(F)\). 
\end{eg}

\begin{proposition}
    Suppose \(V\) is a vector space, and \(W \subseteq V\) is non-empty, then 
    \[
        W \text{ is a subspace } \iff \text{ For }u,v \in W, \alpha \in F, \text{ we have } u + v \in W \text{ and } \alpha \cdot u \in W.
    \]  
\end{proposition}
\begin{proof}[proof of \(\implies \)]
    Clear.
\end{proof}
\begin{proof}[proof of \(\impliedby \)]
    First, we would want to check \(0 \in W\), and we can pick any \(u \in W\), and pick \(\alpha = -1\), so we know \(-u \in W\), and thus \(0 = u + (-u) \in W\).     
\end{proof}

\begin{corollary}
    If we want to check \(W\) is a subspace, we just need to check for \(u,v \in W\), \(\alpha \in F\), \(u + \alpha v \in W\) or not.    
\end{corollary}

\section{Linear Combination}
\begin{definition}[Linear combinaiton] \label{dfn: linear combination}
    Given \(v_1, v_2, \dots , v_n \in V\), a linear combination of them is a vector of the form
    \[
        \alpha _1 v_1 + \alpha _2 v_2 + \dots + \alpha _n v_n.
    \] 
\end{definition}

\begin{proposition}
    Given \(v_1, v_2, \dots , v_n \in V\), 
    \begin{enumerate}
        \item \(W = \left\{ \text{all linear combinations of } v_, \dots , v_n \right\} \) is a subspace.
        \item  This subspace is the smallest subspace containing \(v_1, \dots , v_n\). That is, if \(W^{\prime} \subseteq V\) is a subspace containing \(v_1, \dots , v_n\), then \(W \subseteq W^{\prime} \).    
    \end{enumerate} 
    \begin{notation}
        \(\Span \left\{v_1, v_2, \dots , v_n \right\} = \left\{ \text{all linear combinations of } v_1, v_2, \dots , v_n \right\} \)
    \end{notation}
\end{proposition}

\section{Linearly independent}
\begin{definition*}
    Now we talk about the linear dependence and linear independence.
    \begin{definition}[Linearly dependent] \label{def: linearly dependent}
    \(v_1, v_2, \dots , v_n\) are linearly dependent if 
    \[
        \alpha _1 v_1 + \alpha _2 v_2 + \dots + \alpha _n v_n = 0
    \] for some \(\alpha _1, \alpha _2, \dots , \alpha _n\) not all zeros. 
\end{definition}

\begin{definition}[Linearly independent] \label{def: linearly independent}
    \(v_1, v_2, \dots , v_n\) are called linearly independent if they are not linearly dependent. 
\end{definition}
\end{definition*}

\begin{corollary}
    Say \(\alpha _i \neq 0\), then \(v_i \in \Span \left\{ \hat{v_1}, \hat{v_2}, \dots , \hat{v_k}   \right\} \) suppose the corresponding \(\alpha _i\) of \(\hat{v_1}, \dots , \hat{v_k}  \) are not zeros. 
\end{corollary}

\begin{corollary}
    Linearly independent means if \(\alpha _1 v_1 + \dots + \alpha _n v_n = 0\), then \(\alpha _1 = \alpha _2 = \dots = \alpha _n = 0\). 
\end{corollary}

\begin{corollary}
    Linearly independent meeans if \(\sum \alpha _i v_i = \sum \beta _i v_i  \), then \(\alpha _i = \beta _i\) for all \(i\).   
\end{corollary}

\begin{eg}
    \vphantom{text}
    \begin{itemize}
        \item \(v \in V\) is linearly independent iff \(v \neq 0\). 
        \item \(v, w \in V\) are linearly independent iff \(v\) is not a scaler of \(w\) and \(w\) is not a scalar of \(v\).       
    \end{itemize}
\end{eg}

\begin{lemma}
    \(v_1, \dots , v_n\) are linearly independent iff \(v_i \notin \Span \left\{ v_1, \dots , v_{i-1}, v_{i+1}, \dots , v_n \right\} \).  
\end{lemma}

\section{Basis}

\begin{definition*}
    We now talking about basis
    \begin{definition}[Basis] \label{def: basis}
        \(B = \left\{ v_1, v_2, \dots , v_n \right\} \) is called a basis of \(V\) if \(B\) spans \(V\) and \(B\) is linearly independent.     
    \end{definition}
    \begin{definition}[Dimension] \label{def: Dimeansion}
        In this case, \(n\) is called the dimension of \(V\), and denoted by \(\dim V\).   
    \end{definition}

    \begin{notation}
        \(\Span \left\{ v_1, v_2, \dots , v_n \right\} = \langle v_1, v_2, \dots , v_n \rangle  \) 
    \end{notation}

    \begin{notation}
        \(\Span(S) = \langle S \rangle \) 
    \end{notation}
\end{definition*}

\begin{theorem} \label{thm: unique coordinate}
    For any \(v \in V\), it has a unique expression \(v = \sum_{i=1}^n \alpha _i v_i \).  
\end{theorem}