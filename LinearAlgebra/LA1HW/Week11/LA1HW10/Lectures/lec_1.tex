\begin{problem}
Find a projection $E$ which projects $\mathbb{R}^2$ onto the subspace spanned by $(1, -1)$ along the subspace spanned by $(1, 2)$.
\end{problem}
\begin{proof}
    Since \((1, -1) \in \mathrm{span}\left\{ (1, -1) \right\}  \), so \(E(1, -1) = (1, -1)\), and \(E(1, 2) = (0, 0)\) since \(E\) is a projection along \(\mathrm{span}\left\{ (1, 2) \right\}  \). Hence, suppose \([E]_b = \begin{pmatrix}
        a & b  \\
        c & d  \\
    \end{pmatrix}\) where \(b\) is the standard basis of \(\mathbb{R} ^2\), then 
    \[
        \begin{dcases}
            a - b = 1 \\
            c - d = -1
        \end{dcases}, \quad 
        \begin{dcases}
            c - d = -1 \\
            c + 2d = 0
        \end{dcases},
    \]        
    so \((a, b) = \left( \frac{2}{3}, -\frac{1}{3} \right) \) and \((c, d) = \left( -\frac{2}{3}, \frac{1}{3} \right) \). Thus, 
    \[
        [E]_b = \begin{pmatrix}
            \frac{2}{3} & -\frac{1}{3}  \\
            -\frac{2}{3} & \frac{1}{3}  \\
        \end{pmatrix} \implies E(x,y) = \left( \frac{2}{3}x - \frac{1}{3}y, -\frac{2}{3}x + \frac{1}{3}y \right). 
    \] 
\end{proof}

\begin{problem}
Let $V$ be a real vector space and $E$ an idempotent linear operator on $V$, i.e., a projection. Prove that $(I + E)$ is invertible. Find $(I + E)^{-1}$.
\end{problem}
\begin{proof}
    Note that 
    \[
        \left( I - \frac{1}{2} E \right) (I + E) = I + E - \frac{1}{2} E - \frac{1}{2} E^2 = I + E - \frac{1}{2} E -\frac{1}{2} E = I + E, 
    \]
    so \(I + E\) is invertible and \((I + E)^{-1} = I - \frac{1}{2}E\).  
\end{proof}

\begin{problem}
Let $F$ be a subfield of the complex numbers (or, a field of characteristic zero). Let $V$ be a finite-dimensional vector space over $F$. Suppose that $E_1, \ldots, E_k$ are projections of $V$ and that $E_1 + \cdots + E_k = I$. Prove that $E_i E_j = 0$ for $i \neq j$ (Hint: Use the trace function and ask yourself what the trace of a projection is.)
\end{problem}
\begin{proof}
    We first show that if \(E\) is a projection, then \(\Tr (E) = \rank (E)\). If \(E\) is a projection, then \(E^2 = E\), which means \(E(E - I) = 0\), so if \(m_E(x)\) is the minimal polynomial of \(E\), then \(m_E(x) \mid x(x-1)\), which means \(E\) must be diagonalizable since \(m_E(x)\) has no repeated roots and \(E\)'s eigenvalues are \(0, 1\). Hence, there exists a basis \(b\) of \(V\) s.t. 
    \[
        [E]_b = \begin{pmatrix}
            1 &  &  &  &  &  &   \\
             & 1 &  &  &  &  &   \\
             &  & \ddots &  &  &  &   \\
             &  &  & 1 &  &  &   \\
             &  &  &  & 0 &  &   \\
             &  &  &  &  & \ddots &   \\
             &  &  &  &  &  & 0  \\
        \end{pmatrix},
    \] and since \(\rank (E)\) and \(\Tr (E)\) does not depend on the choice of matrix representation, so
    \[\rank (E) = \rank ([E]_b) = \Tr ([E]_b) = \Tr (E).\] 
    Now since \(E_1 + \dots + E_k = I\), so for all \(i = 1,2 ,\dots , k\) we have 
    \[
        E_i = E_i (E_1 + \dots + E_k) = E_i^2 + \sum_{j \neq i} E_i E_j = E_i + \sum_{j \neq i} E_i E_j, 
    \] so \(0 = \sum_{j \neq i} E_i E_j \), which gives 
    \[
        0 = \sum_{j \neq i} \Tr (E_i E_j) = \sum_{j \neq i} \rank (E_i E_j),  
    \] and since \(\rank (E_i E_j) \ge 0\) for all \(j \neq i\), so \(\rank (E_i E_j) = 0\) for all \(j \neq i\), which means \(E_i E_j = 0\) for all \(j \neq i\), and since \(i\) can be \(1,2, \dots , k\), so we're done.              
\end{proof}


\begin{problem}
Let $T$ be the linear operator on $\mathbb{R}^2$, the matrix of which in the standard ordered basis is
$$
\begin{pmatrix}
2 & 1 \\
0 & 2
\end{pmatrix}.
$$
Let $W_1$ be the subspace of $\mathbb{R}^2$ spanned by the vector $\epsilon_1 = (1, 0)$.
\begin{enumerate}[(a)]
    \item Prove that $W_1$ is invariant under $T$.
    \item Prove that there is no subspace $W_2$ which is invariant under $T$ and which is complementary to $W_1$:
    $$
    \mathbb{R}^2 = W_1 \oplus W_2.
    $$
\end{enumerate}
\end{problem}
\begin{proof}[proof of (a)]
    Note that 
    \[
        \begin{pmatrix}
            2 & 1  \\
            0 & 2  \\
        \end{pmatrix} \epsilon _1 = 2 \epsilon _1,
    \] so for all \(v \in W_1\), we can write \(v = c \epsilon _1\) for some \(c \in \mathbb{R} \), and thus \[T(v) = T(c \epsilon _1) = c T(\epsilon _1) = 2c \epsilon _1 \in W_1.\] This means \(W_1\) is \(T\)-invariant.      
\end{proof}
\begin{proof}[proof of (b)]
    If such \(W_2\) exists, then 
    \[
        2 = \dim \mathbb{R} ^2 = \dim W_1 + \dim W_2 = 1 + \dim W_2,
    \] so \(\dim W_2 = 1\), which means \(W_2 = \mathrm{span}\left\{ (a, b) \right\}  \) for some \((a, b) \in \mathbb{R} ^2\) and \(W_2\) is \(T\)-invariant. This means 
    \[
        T(a, b) = c(a, b) \text{ for some } c \in \mathbb{R}, 
    \] and since \((a, b) \neq (0, 0)\), so \(c\) is an eigenvalue of \(T\). However, note that 
    \[
        \det \begin{pmatrix}
            2 & 1  \\
            0 & 2  \\
        \end{pmatrix} = (x-2)^2,
    \] so \(c = 2\). Hence, 
    \[
        \begin{pmatrix}
            2 & 1  \\
            0 & 2  \\
        \end{pmatrix}\begin{pmatrix}
             a \\
             b \\
        \end{pmatrix} = \begin{pmatrix}
             2a \\
             2b \\
        \end{pmatrix},
    \] which gives \((a, b) \in \mathrm{span}\left\{ (1, 0) \right\} = W_1 \), so \(W_2 = W_1\) since \((a, b) \neq (0, 0)\), but \(\mathbb{R} ^2 = W_1 \oplus W_2\), so this is impossible. Hence, such \(W_2\) does not exist.   
\end{proof}

\begin{problem}
Let $T$ be a linear operator on $V$. Suppose $V = W_1 \oplus \cdots \oplus W_k$, where each $W_i$ is invariant under $T$. Let $T_i$ be the induced (restriction) operator on $W_i$.
\begin{enumerate}[(a)]
    \item Prove that $\det(T) = \det(T_1) \cdots \det(T_k)$.
    \item Prove that the characteristic polynomial for $T$ is the product of the characteristic polynomials for $T_1, \ldots, T_k$.
    \item Prove that the minimal polynomial for $T$ is the least common multiple of the minimal polynomials for $T_1, \ldots, T_k$. (Hint: Prove and then use the corresponding facts about direct sums of matrices.)
\end{enumerate}
\end{problem}
\begin{proof}[proof of (a)]
    Suppose \(b_i\) is a basis of \(W_i\) for \(i = 1,2, \dots , k\), then \(B = \bigcup_{i=1}^{k} b_i \) is a basis of \(V\), and note that 
    \[
        [T]_B = \begin{pmatrix}
            [T_1]_{b_1} & 0 & \cdots & 0  \\
            0 & [T_2]_{b_2} & \cdots & 0  \\
            \vdots & \vdots & \ddots & \vdots  \\
            0 & 0 & \cdots & [T_k]_{b_k}  \\
        \end{pmatrix},
    \]  
    so \(\det ([T]_B) = \det ([T_1]_{b_1}) \det ([T_2])_{b_2} \dots \det ([T_k])_{b_k}\), which gives 
    \[
        \det (T) = \det (T_1) \det (T_2) \dots \det (T_k).
    \]    
\end{proof}
\begin{proof}[proof of (b)]
    Note that 
    \[
        xI - [T]_B = \begin{pmatrix}
            xI - [T_1]_{b_1} & 0 & \cdots & 0  \\
            0 & xI - [T_2]_{b_2} & \cdots & 0  \\
            \vdots & \vdots & \ddots & \vdots  \\
            0 & 0 & \cdots & xI - [T_k]_{b_k}  \\
        \end{pmatrix},
    \]
    so
    \[
        \det (xI - [T]_B) = \det (xI - [T_1]_{b_1}) \det (xI - [T_2]_{b_2}) \dots \det (xI - [T_k]_{b_k}),
    \] which gives 
    \[
        \mathrm{ch}_T(x) = \mathrm{ch}_{T_1}(x) \mathrm{ch}_{T_2}(x) \dots \mathrm{ch}_{T_k}(x).    
    \]
\end{proof}
\begin{proof}[proof of (c)]
    Note that for all \(i = 1,2, \dots , k\) we have \(m_T(T_i) = 0\) since for any \(w_i \in W_i\) we know 
    \[
        0 = m_T(T)(w_i) = m_T(T_i)(w_i).
    \]   
    This means \(m_{T_i}(x) \mid m_T(x)\) for all \(i = 1,2, \dots , k\). Now let \(f(x) = \mathrm{lcm}(m_{T_1}(x), m_{T_2}(x), \dots , m_{T_k}(x))\), then note that \(f(x)\) is the polynomial with least degree s.t. \(m_{T_i}(x) \mid f(x)\) for all \(i = 1,2, \dots , k\) and \(f(x)\) is monic since \(m_{T_i}(x)\) is monic for all \(i = 1,2,\dots ,k\). Note that for all \(v \in V\), \(v = \sum_{i=1}^k w_i \) where \(w_i \in W_i\) for all \(i = 1,2,\dots ,k\) and 
    \[
        f(T)(v) = \sum_{i=1}^k f(T)(w_i) = \sum_{i=1}^k 0 = 0  
    \]            
    since for all \(i = 1,2, \dots , k\) we know \(m_{T_i}(x) \mid f(x)\), i.e. \(f(T) = q_i(T) m_{T_i}(T)\) for some \(q_i(x) \in F[x]\), and thus \(f(T)(w_i) = q_i(T)m_{T_i}(T)(w_i) = q_i(T)m_{T_i}(T_i)(w_i) = 0\). Hence, \(f(T)(v) = 0\) for all \(v \in V\) and thus \(f(x) = m_T(x)\).        
\end{proof}