\begin{problem}
Let $A$ be a $2 \times 2$ matrix over a field $F$. Then the set of all matrices of the form $f(A)$, where $f$ is a polynomial over $F$, is a commutative ring $K$ with identity. If $B$ is a $2 \times 2$ matrix over $K$, the determinant of $B$ is then a $2 \times 2$ matrix over $F$, of the form $f(A)$. Suppose $I$ is the $2 \times 2$ identity matrix over $F$ and that $B$ is the $2 \times 2$ matrix over $K$
\[
B = 
\begin{bmatrix}
A - A_{11}I & -A_{12}I \\
-A_{21}I & A - A_{22}I
\end{bmatrix}.
\]
Show that $\det B = f(A)$, where $f = x^2 - (A_{11} + A_{22})x + \det A$, and also that $f(A) = 0$.
\end{problem}
\begin{proof}
    Note that 
    \begin{align*}
        \det B &= (A - A_{11} I)(A - A_{22} I) - A_{12} A_{21} I = A^2 - (A_{11} + A_{22}  ) A + (A_{11} A_{22} - A_{12} A_{21}  )I \\
        &= A^2 - (A_{11} + A_{22})A + (\det A) \cdot I, 
    \end{align*}
    so we know \(\det B = f(A)\), where \(f(x) = x^2 - (A_{11} + A_{22}  )x + \det A\). Also, since we know 
    \begin{align*}
        A^2 &= \begin{pmatrix}
            A_{11}^2 + A_{12} A_{21} & A_{11} A_{12} + A_{12} A_{22}  \\
            A_{21} A_{11} + A_{22} A_{21} & A_{21} A_{12} + A_{22}^2  \\
        \end{pmatrix} \\
        (A_{11} + A_{22})A &= \begin{pmatrix}
            A_{11}^2 + A_{11}A_{22} & A_{11} A_{12} + A_{22} A_{12}  \\
            A_{11} A_{21} + A_{22} A_{21} & A_{11} A_{22} + A_{22}^2  \\
        \end{pmatrix} \\
        (\det A) \cdot I &= \begin{pmatrix}
            A_{11} A_{22} - A_{12} A_{21}   &  0 \\
            0 & A_{11} A_{22} - A_{12} A_{21}  \\
        \end{pmatrix},
    \end{align*}
    so we know \(f(A) = A^2 - (A_{11} + A_{22})A + (\det A) \cdot I = 0\).   
\end{proof}

\begin{problem}
If $\sigma$ is a permutation of degree $n$ and $A$ is an $n \times n$ matrix over the field $F$ with row vectors $\boldsymbol{\alpha}_1, \dots, \boldsymbol{\alpha}_n$, let $\sigma(A)$ denote the $n \times n$ matrix with row vectors
$$
\boldsymbol{\alpha}_{\sigma 1}, \dots, \boldsymbol{\alpha}_{\sigma n}.
$$
\begin{enumerate}
    \item[(a)] Prove that $\sigma(AB) = \sigma(A)B$, and in particular that $\sigma(A) = \sigma(I)A$.
    \item[(b)] If $T$ is the linear operator of Exercise 9, prove that the matrix of $T$ in the standard ordered basis is $\sigma(I)$.
    \item[(c)] Is $\sigma^{-1}(I)$ the inverse matrix of $\sigma(I)$?
    \item[(d)] Is it true that $\sigma(A)$ is similar to $A$?
\end{enumerate}

\begin{note}
    In Exercise 9, we define
    \[
        T:F^n \to F^n, \quad T(x_1, x_2, \dots , x_n) = \left( x_{\sigma (1)}, x_{\sigma (2)}, \dots , x_{\sigma (n)} \right) 
    \] for a permutation \(\sigma \in S_n\). 
\end{note}
\end{problem}
\begin{proof}
    \vphantom{text}
    \begin{itemize}
        \item [(a)] Suppose \(AB\)'s rows are \(r_1, r_2, \dots , r_n\) and \(A = (a_{ij})_{n \times n}\) and \(B = (b_{ij})_{n \times n}\), then we know 
        \[
            r_i = \left( \sum_{k=1}^n a_{ik} b_{k1}, \sum_{k=1}^n a_{ik} b_{k2}, \dots , \sum_{k=1}^n a_{ik} b_{kn} \right) \quad \forall 1 \le i \le n. 
        \]
        Thus, we know the \(p\)-th row of \(\sigma (AB)\) is 
        \[
            r_p^{\prime} = \left( \sum_{k=1}^n a_{\sigma (p) k} b_{k1}, \sum_{k=1}^n a_{\sigma (p) k} b_{k2}, \dots , \sum_{k=1}^n a_{\sigma(p) k} b_{kn} \right) 
        \]  for all \(1 \le p \le n\). Note that \(\sigma (A)\)'s rows are \(\alpha _{\sigma (1)}, \alpha _{\sigma (2)}, \dots , \alpha _{\sigma (n)}\), then if we suppose \(\sigma (A) B\)'s rows are \(r_1^{\prime\prime}  , r_2^{\prime\prime}  , \dots , r_n^{\prime\prime} \), then we know 
        \[
            r_i^{\prime\prime} = \left( \sum_{k=1}^n a_{\sigma (p) k} b_{k1}, \sum_{k=1}^n a_{\sigma (p) k} b_{k2}, \dots , \sum_{k=1}^n a_{\sigma(p) k} b_{kn} \right) = r_i^{\prime} \quad \forall 1 \le i \le n, 
        \] so \(\sigma (AB) = \sigma (A) B\). Thus, we have 
        \[
            \sigma (A) = \sigma (I A) = \sigma (I) A.
        \] 
        \item [(b)] Suppose \(b\) is the standard ordered basis, then if \(\sigma (j) = i\), we have \(T(e_i) = e_j\). Now if \([T]_b = A = (a_{ij})_{n \times n}\), then if \(a_{rc} = 1\), we must have \(T(e_c) = e_r\) since every row and every column of \(A\) has exactly one \(1\), while the other entries in the row/column are \(0\). Hence, we have \(c = \sigma (r)\), which means \([T]_b = \sigma (I)\).           
        \item [(c)] Suppose \(\sigma ^{-1}(I) \sigma (I) = (c_{ij})_{n \times n}\), then for \(c_{ij}\):
        \begin{itemize}
            \item Case 1: \(i = j\), we know 
            \[
                c_{ii} = \sum_{k=1}^n \sigma ^{-1}(I)_{ik} \sigma (I)_{ki} = \sigma ^{-1}(I)_{i, \sigma ^{-1}(i)} \sigma (I)_{\sigma ^{-1}(i), i} = \sigma (I)_{\sigma ^{-1}(i), i} = \sigma (I)_{w, \sigma (w)} = 1 
            \] if we suppose \(w = \sigma ^{-1}(i)\). Note that this is true since \(k = \sigma ^{-1}(i)\) is the only \(k\) s.t. \(\sigma ^{-1}(I)_{ik} = 1\), otherwise it is equal to \(0\). 
            \item Case 2: \(i \neq j\), then 
            \[
                c_{ij} = \sum_{k=1}^n \sigma ^{-1}(I)_{ik} \sigma (I)_{kj} = \sigma ^{-1}(I)_{i, \sigma ^{-1}(i)} \sigma (I)_{\sigma ^{-1}(i), j}. 
            \]
            Note that \(\sigma (\sigma^{-1} (i)) = i \neq j\), so we must have \(\sigma (I)_{\sigma ^{-1}(i), j} = 0\), and thus \(c_{ij} = 0\).   
        \end{itemize}
        Hence, we know \(\sigma^{-1} (I) \sigma (I) = I\), which means \(\sigma ^{-1}(I)\) is the inverse matrix of \(\sigma (I)\). 
        \item [(d)] The answer is: not necessarily true. 
        \begin{claim}
            If \(P \sim I\), then \(P = I\).  
        \end{claim}
        \begin{explanation}
            If \(P \sim I\), then \(Q^{-1} P Q = I\) for some \(Q\), so \(PQ = Q\), which means \(P = PQQ^{-1} = QQ^{-1} = I\).     
        \end{explanation}
        With this claim, if we pick some \(\sigma \in S_n\) s.t. \(\sigma \) is not identity permutation, then \(\sigma (I) \neq I\), and thus \(\sigma (I)\) is not similar to \(I\). 
    \end{itemize}
    
\end{proof}

\begin{problem}
Let $A$ be an $n \times n$ matrix over $K$, a commutative ring with identity. Suppose $A$ has the block form
$$
A = 
\begin{pmatrix}
A_1 & 0 & \cdots & 0 \\
0 & A_2 & \cdots & 0 \\
\vdots & \vdots & \ddots & \vdots \\
0 & 0 & \cdots & A_k
\end{pmatrix}
$$
where $A_i$ is an $r_i \times r_i$ matrix. Prove
$$
\det A = (\det A_1)(\det A_2) \cdots (\det A_k).
$$
\end{problem}
\begin{proof}
    We first do a easier case: If \(A = \begin{pmatrix}
        A_1 & 0  \\
        0 & B  \\
    \end{pmatrix}\), where \(A_1 \in M_{r_1}(K)\) and \(B\) is a square matrix, then we show that \(\det (A) = \det (A_1) \det (B)\). We do induction on \(r_1\). 
    \begin{itemize}
        \item For \(r_1 = 1\), we know \(A = \begin{pmatrix}
            a & 0  \\
            0 & B  \\
        \end{pmatrix}\), where \(A = (a)\), then we know
        \[
            \det (A) = a \det (B) = \det (A) \det (B)
        \]  by expanding along the first row. 
        \item Now suppose for all \(r_1 \le p - 1\) this is true. 
        \item Then for \(r_1 = p\), we know 
        \[
            \det (A) = \sum_{j=1}^p (-1)^{1+j} a_{ij} \det \left( A(1 \mid j) \right) = \sum_{j=1}^p (-1)^{1+j} a_{ij} \det \begin{pmatrix}
                A_1(1 \mid j) & 0  \\
                0 & B  \\
            \end{pmatrix},
        \] by expanding along the first row, and by induction hypothesis, we know 
        \[
            \det \begin{pmatrix}
                A_1(1 \mid j) & 0  \\
                0 & B  \\
            \end{pmatrix} = \det \left( A_1(1 \mid j) \right) \det (B),  
        \] so we know 
        \begin{align*}
            \det (A) &= \sum_{j=1}^p (-1)^{1+j} a_{1j} \det (A_1(1 \mid j))\det (B) = \det (B) \cdot \left( \sum_{j=1}^p (-1)^{1+j}a_{1j} \det (A_1 (1 \mid j))  \right) \\
            &= \det (B) \cdot \det (A),
        \end{align*}
        so we're done.
    \end{itemize}     
    By this case, we can first suppose
    \[
        B_1 = \begin{pmatrix}
            A_2 & 0 & \cdots & 0  \\
            0 & A_3 & \cdots & 0  \\
            \vdots & \vdots & \ddots & \vdots  \\
            0 & 0 & \cdots & A_k  \\
        \end{pmatrix},
    \] then we know \(\det (A) = \det \begin{pmatrix}
        A_1 & 0  \\
        0 & B_1  \\
    \end{pmatrix} = \det (A_1) \det (B_1)\), and similarly defines \(B_2, B_3, \dots , B_{k-1}\), then we know \(\det (B_i) = \det (A_{i+1}) \det (B_{i+1})\) for all \(1 \le i \le k-2\), and thus \[\det (A) = \det (A_1) \det (A_2) \dots \det (A_k).\]     
\end{proof}

\begin{problem}
Let $A$ be an $n \times n$ matrix over a field, $A \ne 0$. If $r$ is any positive integer between $1$ and $n$, an $r \times r$ \textbf{submatrix} of $A$ is any $r \times r$ matrix obtained by deleting $(n-r)$ rows and $(n-r)$ columns of $A$. The \textbf{determinant rank} of $A$ is the largest positive integer $r$ such that some $r \times r$ submatrix of $A$ has a \textbf{non-zero determinant}. Prove that the determinant rank of $A$ is equal to the \textbf{row rank} of $A$ ($=$ \textbf{column rank} $A$).
\end{problem}

\begin{problem}
Let $A$, $B$, $C$, $D$ be commuting $n \times n$ matrices over the field $F$. Show that the determinant of the $2n \times 2n$ matrix
$$
\begin{bmatrix}
A & B \\
C & D
\end{bmatrix}
$$
is $\det (AD - BC)$.
\end{problem}