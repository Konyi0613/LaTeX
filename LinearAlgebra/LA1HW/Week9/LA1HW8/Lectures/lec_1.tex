\begin{problem}
Let \( T \) be the linear operator on \( \mathbb{R}^4 \) which is represented in the standard ordered basis by the matrix
\[
\begin{pmatrix}
0 & 0 & 0 & 0 \\
a & 0 & 0 & 0 \\
0 & b & 0 & 0 \\
0 & 0 & c & 0
\end{pmatrix}.
\]
Under what conditions on \( a, b, \) and \( c \) is \( T \) diagonalizable?
\end{problem}
\begin{proof}
    Suppose 
    \[
        A = \begin{pmatrix}
        0 & 0 & 0 & 0 \\
        a & 0 & 0 & 0 \\
        0 & b & 0 & 0 \\
        0 & 0 & c & 0
        \end{pmatrix},
    \] then \(\det (xI - A) = x^4\), so if \(A\) is diagonalizable, then we must have \(\dim \ker (A - 0I) = \dim \ker A = 4\), which means \(\rank A = 0\) by rank and nullity theorem, so \(a = b = c = 0\).     
\end{proof}

\begin{problem}
Let \( A \) and \( B \) be \( n \times n \) matrices over the field \( F \). Prove that if \( (I - AB) \) is invertible, then \( I - BA \) is invertible and
\[
(I - BA)^{-1} = I + B (I - AB)^{-1} A.
\]
\end{problem}
\begin{proof}
    Since \(I-AB\) is not invertible, so \(\det (I-AB) \neq 0\), and thus we know \(1\) is not an eigenvalue of \(AB\), so there does not exists \(v \neq 0\) s.t. \(ABv=v\). Now suppose by contradiction, \(\det (I-BA) = 0\), then we know \(1\) is an eigenvalue of \(BA\), so there exists \(w \neq 0\) s.t. \(BAw=w\), so \(AB(Aw)=Aw\). Now note that \(Aw \neq 0\), otherwise \(w=BAw=B0=0\), which is impossible. Hence, if we suppose \(v = Aw\), then \(ABv=v\) and \(v \neq 0\), which is a contradiction, so \(\det (I-BA) \neq 0\), and thus \(I-BA\) is invertible. 
    
    Now suppose \(X = (I-AB)^{-1}\), so we have \((I-AB)X=X(I-AB)=I\), which gives 
    \begin{align*}
        &X - XAB = I \text{ and } X - ABX = I, \\
        \implies XAB=ABX=X-I.
    \end{align*}  
    Thus, we know 
    \begin{align*}
        (I-BA)(I+BXA) &= I + BXA - BA - BABXA \\
        &= I + BXA - BA - B(X - I)A \\
        &= I + BXA - BA - (BXA - BA) = I,
    \end{align*}
    so \((I - BA)^{-1} = I + BXA = I + B(I - AB)^{-1} A\), and we're done. 
\end{proof}

\begin{problem}[Exercise 9]
Use the result of Exercise 8 to prove that, if \( A \) and \( B \) are \( n \times n \) matrices over the field \( F \), then \( AB \) and \( BA \) have precisely the same characteristic values in \( F \).
\begin{remark}[Exercise 8]
Let \( A \) and \( B \) be \( n \times n \) matrices over the field \( F \). Prove that if \( (I - AB) \) is invertible, then \( I - BA \) is invertible and
\[
(I - BA)^{-1} = I + B (I - AB)^{-1} A.
\]
\end{remark}
\end{problem}
\begin{proof}
    If \(x\) is an eigenvalue of \(AB\), then 
    \begin{itemize}
        \item Case 1: \(x \neq 0\), then there exists \(v \neq 0\) s.t. \(ABv = xv\), and thus \(BA(Bv) = xBv\). Now we claim that \(Bv \neq 0\). If not, then \(xv = ABv = A0 = 0\), and since \(x \neq 0\), so \(v = 0\), which is a contradiction. Now since \(Bv \neq 0\), so \(Bv\) is an eigenvector for \(x\) of \(BA\), so \(x\) is an eigenvalue of \(BA\).               
        \item Case 2: \(x = 0\), then \(ABv = 0\) for some \(v \neq 0\) and we have two subcases:
        \begin{itemize}
            \item Subcase 1: \(A\) is invertible, then we know \(Bv = 0\) for \(v \neq 0\), and since \(A\) is surjective, so there exists \(p\) s.t. \(Ap = v \neq 0\), and thus \(BAp = Bv = 0\). Note that \(p \neq 0\) otherwise \(v=Ap=0\) and it is a contradiction, so we know \(0\) is an eigenvalue of \(BA\).             
            \item Subcase 2: \(A\) is not invertible, so there exists \(w \neq 0\) s.t. \(Aw = 0\), and thus \(BAw= B0=0\), which means \(0\) is an eigenvalue of \(BA\).      
        \end{itemize}
    \end{itemize}  
    Thus, we have shown that all eigenvalues of \(AB\) are eigenvalues of \(BA\). Similarly, we can use same arguments to show all eigenvalues of \(BA\) are eigenvalues of \(AB\), and thus \(AB\) and \(BA\) have precisely the same characteristic values in \(F\).       
\end{proof}

\begin{problem}[Exercise 12]
Use the result of Exercise 11 to prove the following: If \( A \) is a \( 2 \times 2 \) matrix with complex entries, then \( A \) is similar over \( \mathbb{C} \) to a matrix of one of the two types
\[
\begin{pmatrix}
a & 0 \\
0 & b
\end{pmatrix}
\quad \text{or} \quad
\begin{pmatrix}
a & 0 \\
1 & a
\end{pmatrix}.
\]
\begin{remark}[Exercise 11]
Let \( N \) be a \( 2 \times 2 \) complex matrix such that \( N^2 = 0 \). Prove that either \( N = 0 \) or \( N \) is similar over \( \mathbb{C} \) to
\[
\begin{pmatrix}
0 & 0 \\
1 & 0
\end{pmatrix}.
\]
\end{remark}
\end{problem}
\begin{proof}
    Since \(\mathbb{C} \) is algebraically closed, so we can always get \(2\) eigenvalues of \(A\). If two eigenvalues of \(A\) are distinct, then we know \(A\) is diagonalizable and thus it is similar to a matrix of type \(\begin{pmatrix}
        a & 0  \\
        0 & b  \\
    \end{pmatrix}\). If \(A\)'s two eigenvalues are same, say they are both \(\lambda \), then if \(A\) is diagonalizable, then \(A\) is also similar to a matrix of the type \(\begin{pmatrix}
        a  & 0  \\
        0 & b  \\
    \end{pmatrix}\). Now we consider the case that \(A\) is not diagonalizable. In this case, we know \(\dim E(\lambda ) = 1\), say \(v \neq 0\) and \(v \in E(\lambda )\), then we can extend \(\left\{ v \right\} \) to \(B = \left\{ w, v \right\} \), which is a basis of \(\mathbb{C} ^2\). Then, we know 
    \[
        A \sim \begin{pmatrix}
            x & 0  \\
            y &  \lambda  \\
        \end{pmatrix},
    \] for some \(x, y \in \mathbb{C} \). Then, we know \(A(w) = xw + yv\), so if we pick \(w^{\prime} = \frac{1}{y} w\), we know
    \[
        A\left( w^{\prime}  \right) = A \left( \frac{1}{y} w \right) = \frac{1}{y} \left( xw+yv \right) = xw^{\prime} + v,
    \] and note that \(B^{\prime} = \left\{ w^{\prime} , v \right\} \) is still a basis of \(\mathbb{C} ^2\), so we know 
    \[
        A \sim \begin{pmatrix}
            x & 0  \\
            1 & \lambda   \\
        \end{pmatrix}.
    \] Note that 
    \[
        \lambda + \lambda = \Tr (A) = \Tr \begin{pmatrix}
            x & 0  \\
            1 & \lambda   \\
        \end{pmatrix} = x + \lambda,
    \] so we know \(x = \lambda \), and we're done. 
\end{proof}


\begin{problem}
Let \( V \) be the space of \( n \times n \) matrices over \( F \). Let \( A \) be a fixed \( n \times n \) matrix over \( F \). Let \( T \) be the linear operator “left multiplication by \( A \)” on \( V \). Is it true that \( A \) and \( T \) have the same characteristic values?
\end{problem}
\begin{proof}
    The answer is true. If \(\lambda \) is an eigenvalue of \(A\), then \(\exists v \neq 0\) s.t. \(Av=\lambda v\), so if we construct a matrix \(M \in V\) by all the \(n\) the columns of \(M\) are \(v\), then we know 
    \[
        AM = A(v, v, \dots , v) = (Av, Av, \dots , Av) = (\lambda v, \lambda v, \dots , \lambda , v) = \lambda (v, v, \dots , v) = \lambda M,
    \] and \(M \neq 0\) is trivial since \(v \neq 0\). Hence, \(\lambda \) is an eigenvalue of \(T\).
    
    Now if \(\lambda \) is an eigenvalue of \(T\), then there exists \(M \neq 0\) s.t. \(AM = \lambda M \). Suppose the \(i\)-th column of \(M\) is not zero column, say this column is called \(c_i\), then we know \(Ac_i = \lambda c_i\) since
    \[
        AM = A(\dots , c_i, \dots ) = (\dots , Ac_i, \dots ) = \lambda M = \lambda (\dots ,c_i, \dots  ) = (\dots , \lambda c_i, \dots ).
    \] Hence, \(\lambda \) is an eigenvalue of \(A\). 
    
    By above arguments, we know \(A\) and \(T\) have same eigenvalues.  
\end{proof}