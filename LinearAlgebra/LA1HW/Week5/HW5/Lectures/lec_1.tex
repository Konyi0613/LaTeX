\section*{Secion 1.4}
\begin{problem}
Suppose $R$ and $R'$ are $2\times 3$ row-reduced echelon matrices and that the systems $R X = 0$ and $R' X = 0$ have exactly the same solutions. Prove that $R=R'$.
\end{problem}
\begin{proof}
    If \(RX = 0\) and \(R^{\prime} X = 0\) have exactly the same solutions, then \(\ker R = \ker R^{\prime} \), and by rank and nullity theorem we know \(\rank R = \rank R^{\prime} \). 
    \begin{itemize}
        \item Case 1: \(\rank R = \rank R^{\prime} = 0\), the only \(2 \times 3\) matrices with rank \(0\) is the zero matrix, so if \(\rank R = \rank R^{\prime} = 0\), then \(R = R^{\prime} = 0\). 
        \item Case 2: \(\rank R = \rank R^{\prime} = 1\), then since \(R\) and \(R^{\prime} \) are in row-reduced echelon form, so suppose 
        \[
            R = \begin{pmatrix}
                1 & a & b  \\
                0 & 0 & 0  \\
            \end{pmatrix} \quad R^{\prime} = \begin{pmatrix}
                1 & a^{\prime}  & b^{\prime}   \\
                0& 0 & 0  \\
            \end{pmatrix},
        \] and then we know 
        \[
            \begin{dcases}
                x_1 + ax_2 + bx_3 = 0 \\
                x_1 + a^{\prime} x_2 + b^{\prime} x_3 = 0
            \end{dcases}
        \]
        have same solutions \((x_1, x_2, x_3)\). Since \(x_1 + ax_2 + bx_3 = 0\) and \(x_1 + a^{\prime} x_2 + b^{\prime} x_3 = 0\) are both planes in \(\mathbb{R} ^3\), so we must have these two planes coincide, and thus 
        \[
            (1, a, b) \parallel \left( 1, a^{\prime} , b^{\prime}  \right), 
        \] which means \(a = a^{\prime} \) and \(b = b^{\prime} \), so \(R = R^{\prime} \). 
        \item Case 3: \(\rank R = \rank R^{\prime} = 2\), then since there are two types of row-reduced echelon form \(2 \times 3\) matrices with rank \(2\), which are 
        \[
            T_1 = \begin{pmatrix}
                1 & 0 & a  \\
                0 & 1 & b  \\
            \end{pmatrix} \quad 
            T_2 = \begin{pmatrix}
                1 & c & 0   \\
                0 & 0 & 1   \\
            \end{pmatrix},
        \] but we should note that it is impossible that \(R\) is in \(T_1\) form and \(R^{\prime} \) is in \(T_2\) form or the converse occurs, otherwise WLOG suppose \(R\) is in \(T_1\) form and \(R^{\prime} \) is in \(T_2\) form, then \(RX=0\) has some solutions \((x_1, x_2, x_3)\) with \(x_3 \neq 0\) but the solutions of \(R^{\prime} X = 0\) must be \((x_1, x_2, 0)\), so their solutions are not the same.
        
        Now if \(R, R^{\prime} \) are both in \(T_1\) form, so suppose 
        \[
            R = \begin{pmatrix}
                1 & 0 & a  \\
                0 & 1 & b  \\
            \end{pmatrix} \quad R^{\prime} = \begin{pmatrix}
                1 & 0 & a^{\prime}   \\
                0 & 1 & b^{\prime}   \\
            \end{pmatrix},
        \]  we know the solutions of \(RX=0\) are \(\left( -a x_3, -b x_3, x_3 \right) \) and the solutions of \(R^{\prime} X = 0\) are \(\left( -a^{\prime} x_3, -b^{\prime} x_3, x_3 \right) \), so we must have \(a = a^{\prime} \) and \(b = b^{\prime} \), and thus \(R = R^{\prime} \). 
        
        Now if \(R\) and \(R^{\prime} \) are both in \(T_2\) form, then suppose 
        \[
            R = \begin{pmatrix}
                1 & c & 0  \\
                0 & 0 & 1  \\
            \end{pmatrix} \quad 
            R^{\prime} = \begin{pmatrix}
                1 & c^{\prime}  & 0  \\
                0 & 0 & 1  \\
            \end{pmatrix},
        \] we know the solutions of \(RX = 0\) are \(\left( -c x_2, x_2, 0 \right) \) and the solutions of \(R^{\prime} X = 0\) are \(\left( -c^{\prime} x_2, x_2 , 0 \right) \), so we must have \(c = c^{\prime} \), and thus \(R = R^{\prime} \).         
    \end{itemize}    
\end{proof}
\section*{Section 3.2}

\begin{problem}
Let $V$ be a finite-dimensional vector space and let $T$ be a linear operator on $V$. Suppose that $\operatorname{rank}(T^{2})=\operatorname{rank}(T)$. Prove that the range and null space of $T$ are disjoint, i.e., have only the zero vector in common.
\end{problem}
\begin{proof}
    Since \(\rank T^2 = \rank T\), so by rank and nullity theorem we know \(\dim \ker T^2 = \dim \ker T\), and since \(\ker T \subseteq \ker T^2\), so \(\ker T = \ker T^2\). Now suppose \(s \in \ker T \cap \Im T\), then we know \(T(s) = 0\) and \(s = T(v)\) for some \(v \in V\), so \(T(s) = T\left( T(v) \right) = T^2(v) \), and since \(T(s) = 0\), so \(v \in \ker T^2 = \ker T\), so \(T(v) = 0\), and thus \(s = T(v) = 0\).             
\end{proof}

\begin{problem}
Let $p,m,$ and $n$ be positive integers and $F$ a field. Let $V$ be the space of $m\times n$ matrices over $F$ and let $W$ be the space of $p\times n$ matrices over $F$. Let $B$ be a fixed $p\times m$ matrix and let $T$ be the linear transformation from $V$ into $W$ defined by $T(A)=BA$. Prove that $T$ is invertible if and only if $p=m$ and $B$ is an invertible $m\times m$ matrix.
\end{problem}
\begin{proof}
    \vphantom{text}
    \begin{itemize}
        \item [\((\implies )\)] If \(T\) is invertible, then \(T\) is bijective, which means \(\dim V = \dim W\), and since \(\dim V = m \times n\) and \(\dim W = p \times n\), so \(m = p\). Now since \(T\) is bijective, so \(\ker T = \left\{ 0 \right\} \), so suppose \(A = (a_{ij})_{m \times n}\), then we know 
        \[
            B \begin{pmatrix}
                 a_{1i} \\
                 a_{2i} \\
                 \vdots \\
                 a_{mi} \\
            \end{pmatrix} = 0 \text{ has only trivial solution }\forall 1 \le i \le n,
        \] which means \(B\) is injective and thus invertible since \(B \in M_{m \times m}(F)\).  
        \item [\((\impliedby )\)] Now if \(p = m\) and \(B\) is invertible, then \(B^{-1} \) exists, so we can define \(T^{-1} W \to V \) as \(T^{-1}(X) = B^{-1}X\), then we have
        \[
            T T^{-1}(X) = T \left( B^{-1} X \right) = B B^{-1} X = X
        \]
        and 
        \[
            T^{-1} T (A) = T^{-1} \left( BA \right) = B^{-1} B A = A,
        \]
        so \(T^{-1} \) is the inverse function of \(T\), which means \(T\) is invertible.         
    \end{itemize}
\end{proof}


\section*{Section 3.5}
\begin{problem}
If $A$ and $B$ are $n\times n$ matrices over the field $F$, show that $\operatorname{trace}(AB)=\operatorname{trace}(BA)$. Now show that similar matrices have the same trace.
\end{problem}
\begin{proof}
    Suppose \(A = (a_{ij})_{n \times n}\) and \(B = \left( b_{ij} \right)_{n \times n} \), then 
    \[
        \Tr (AB) = \sum_{i=1}^{n} \sum_{k=1}^n  a_{ik} b_{ki}, 
    \] and we know 
    \[
        \Tr (BA) = \sum_{j=1}^n \sum_{s=1}^n b_{js} a_{sj} = \sum_{s=1}^n \sum_{j=1}^n b_{js} a_{sj} = \sum_{s=1}^n \sum_{j=1}^n a_{sj} b_{js} = \sum_{i=1}^n \sum_{k=1}^n a_{ik} b_{ki} = \Tr (AB).        
    \] Now suppose \(A\) and \(B\) are similar, then \(A = P^{-1} B P \) for some matrix \(P\), then we know 
    \[
        \Tr (A) = \Tr \left( P^{-1} (B P) \right) = \Tr \left( (B P) P^{-1} \right) = \Tr \left( B \left( P P^{-1} \right)  \right) = \Tr (B).
    \]    
\end{proof}

\begin{problem}
Let $V$ be the vector space of all polynomial functions $p$ from $\mathbb{R}$ into $\mathbb{R}$ which have degree $2$ or less:
\[
p(x)=c_0 + c_1 x + c_2 x^2.
\]
Define three linear functionals on $V$ by
\[
f_1(p)=\int_0^1 p(x)\,dx,\qquad
f_2(p)=\int_0^2 p(x)\,dx,\qquad
f_3(p)=\int_{0}^{-1} p(x)\,dx.
\]
Show that $\{f_1,f_2,f_3\}$ is a basis for $V^*$ by exhibiting the basis for $V$ of which it is the dual.
\end{problem}
\begin{proof}
    Suppose \(\left\{ p_1, p_2, p_3 \right\} \) is a basis of \(V\), and its dual basis is \(\left\{ f_1, f_2, f_3 \right\} \), then suppose 
    \begin{align*}
        p_1(x) &= a_1 x^2 + b_1 x + c_1 \\
        p_2(x) &= a_2 x^2 + b_2 x + c_2 \\
        p_3(x) &= a_3 x^2 + b_3 x + c_3,
    \end{align*} 
    we want to solve 
    \[
        \begin{dcases}
            \frac{1}{3} a_1 + \frac{1}{2} b_1 + c_1 = 1 \\
            \frac{8}{3} a_1 + 2 b_1 + 2 c_1 = 0 \\
            -\frac{1}{3}a_1 + \frac{1}{2} b_1 - c_1 = 0
        \end{dcases} \quad 
        \begin{dcases}
            \frac{1}{3} a_2 + \frac{1}{2} b_2 + c_2 = 0 \\
            \frac{8}{3} a_2 + 2 b_2 + 2 c_2 = 1 \\
            -\frac{1}{3}a_2 + \frac{1}{2} b_2 - c_2 = 0
        \end{dcases} \quad 
        \begin{dcases}
            \frac{1}{3} a_3 + \frac{1}{2} b_3 + c_3 = 0 \\
            \frac{8}{3} a_3 + 2 b_3 + 2 c_3 = 0 \\
            -\frac{1}{3}a_3 + \frac{1}{2} b_3 - c_3 = 1
        \end{dcases}
    \] since we know \(f_i \left( p_j \right) = \delta _{ij}\) by the definition of dual basis. By solving these system of equations, we know 
    \begin{align*}
        p_1(x) &= -\frac{3}{2}x^2 + x + 1 \\
        p_2(x) &= \frac{1}{2}x^2 - \frac{1}{6} \\
        p_3(x) &= -\frac{1}{2}x^2 + x - \frac{1}{2}.
    \end{align*} 
    Also, we can check that \(\left\{ p_1, p_2, p_3 \right\} \) is linearly independent since each of them cannot be represented as the linear combination of the other \(2\) elements, so \(\left\{ p_1, p_2, p_3 \right\} \) is linearly independent and thus a basis of \(V\). 
\end{proof}