\section*{Section 6.8}
\begin{problem*}
\textbf{1.} Let $T$ be a linear operator on $\mathbb{R}^3$ which is represented in the standard ordered basis by the matrix
$$
A = \begin{pmatrix}
6 & -3 & -2 \\
4 & -1 & -2 \\
10 & -5 & -3
\end{pmatrix}.
$$
Express the minimal polynomial $p$ for $T$ in the form $p = p_1 p_2$, where $p_1$ and $p_2$ are monic and irreducible over the field of real numbers. Let $W_i$ be the null space of $p_i(T)$. Find bases $\mathcal{B}_i$ for the spaces $W_1$ and $W_2$. If $T_i$ is the operator induced on $W_i$ by $T$, find the matrix of $T_i$ in the basis $\mathcal{B}_i$ (above).
\end{problem*}
\begin{proof}
    Note that 
    \[
        \mathrm{ch}_T(x) = \det \begin{pmatrix}
            x-6 & 3 & 2  \\
            -4 & x+1 & 2  \\
            -10 & 5 & x+3  \\
        \end{pmatrix} = (x-2)\left( x^2 + 1 \right). 
    \]
    If we regard this linear operator is over the field \(\mathbb{C} \), then we know \(\mathrm{ch}_T(x) \) splits and is \((x-2)(x-i)(x+i)\), and since \(m_T(x)\) share same roots as \(\mathrm{ch}_T(x) \) and \(m_T(x) \mid \mathrm{ch}_T(x) \), so \(m_T(x) = (x-2)(x-i)(x+i)\), but here the field is \(\mathbb{R} \), so \(m_T(x) = (x-2)\left( x^2 + 1 \right) \). Let \(p_1(x) = x - 2\) and \(p_2(x) = x^2 + 1\), then \(W_1 = \ker p_1(T) = \ker (T - 2I)\) and \(W_2 = \ker p_2(T) = \ker \left( T^2 + I \right) \). We first handle \(W_1\). Suppose \(b\) is the standard basis of \(\mathbb{R} ^3\). Note that 
    \[
        [T - 2I]_b = \begin{pmatrix}
            4 & -3 & -2  \\
            4 & -3 & -2  \\
            10 & -5 & -5  \\
        \end{pmatrix},
    \]             
    so if \([T - 2I]_b v = 0\), then we find that \(v \in \mathrm{span}\left\{ \begin{pmatrix}
         1 \\
         0 \\
         2 \\
    \end{pmatrix}  \right\}  \), so \(\ker (T - 2I) = \mathrm{span}\left\{ \begin{pmatrix}
         1 \\
         0 \\
         2 \\
    \end{pmatrix}  \right\}\) and thus we can let 
    \[
        \mathcal{B} _1 = \left\{ \begin{pmatrix}
             1 \\
             0 \\
             2 \\
        \end{pmatrix} \right\}. 
    \]   
    Now we handle \(W_2\). Note that 
    \[
        \left[ T^2 + I \right]_b = \begin{pmatrix}
            5 & -5 & 0  \\
            0 & 0 & 0  \\
            10 & -10 & 0  \\
        \end{pmatrix},
    \] 
    and we can find that \(\ker \left( T^2 + I \right) = \mathrm{span} \left\{ \begin{pmatrix}
         1 \\
         1 \\
         0 \\
    \end{pmatrix}, \begin{pmatrix}
         0 \\
         0 \\
         1 \\
    \end{pmatrix} \right\}   \), so we can choose 
    \[
        \mathcal{B} _2 = \left\{ \begin{pmatrix}
         1 \\
         1 \\
         0 \\
    \end{pmatrix}, \begin{pmatrix}
         0 \\
         0 \\
         1 \\
    \end{pmatrix} \right\}. 
    \] 
    Now since 
    \[
        T_1 \begin{pmatrix}
             1 \\
             0 \\
             2 \\
        \end{pmatrix} = \begin{pmatrix}
            6 & -3 & -2  \\
            4 & -1 & -2  \\
            10 & -5 & -3  \\
        \end{pmatrix}
        \begin{pmatrix}
             1 \\
             0 \\
             2 \\
        \end{pmatrix} = \begin{pmatrix}
             2 \\
             0 \\
             4 \\
        \end{pmatrix} = 2 \begin{pmatrix}
             1 \\
             0 \\
             2 \\
        \end{pmatrix},
    \]
    so \([T_1]_{\mathcal{B} _1} = (2)\). Also, 
    \begin{align*}
        T_2 \begin{pmatrix}
             1 \\
             1 \\
             0 \\
        \end{pmatrix} &= \begin{pmatrix}
            6 & -3 & -2  \\
            4 & -1 & -2  \\
            10 & -5 & -3  \\
        \end{pmatrix} \begin{pmatrix}
             1 \\
             1 \\
             0 \\
        \end{pmatrix} = \begin{pmatrix}
             3 \\
             3 \\
             5 \\
        \end{pmatrix} = 3 \begin{pmatrix}
             1 \\
             1 \\
             0 \\
        \end{pmatrix} + 5 \begin{pmatrix}
             0 \\
             0 \\
             1 \\
        \end{pmatrix} \\
        T_2 \begin{pmatrix}
             0 \\
             0 \\
             1 \\
        \end{pmatrix} &= \begin{pmatrix}
            6 & -3 & -2  \\
            4 & -1 & -2  \\
            10 & -5 & -3  \\
        \end{pmatrix} \begin{pmatrix}
             0 \\
             0 \\
             1 \\
        \end{pmatrix} = \begin{pmatrix}
             -2 \\
             -2 \\
             -3 \\
        \end{pmatrix} = -2 \begin{pmatrix}
             1 \\
             1 \\
             0 \\
        \end{pmatrix} - 3\begin{pmatrix}
             0 \\
             0 \\
             1 \\
        \end{pmatrix},
    \end{align*} 
    so we know 
    \[
        [T_2]_{\mathcal{B} _2} = \begin{pmatrix}
            3 & -2  \\
            5 & -3  \\
        \end{pmatrix}.
    \]
\end{proof}

\begin{problem*}
\textbf{4.} Let $T$ be a linear operator on the finite-dimensional space $V$ with characteristic polynomial
$$
f = (x - c_1)^{d_1} \cdots (x - c_k)^{d_k}
$$
and minimal polynomial
$$
p = (x - c_1)^{r_1} \cdots (x - c_k)^{r_k}.
$$
Let $W_i$ be the null space of $(T - c_i I)^{r_i}$.
\begin{enumerate}[(a)]
    \item Prove that $W_i$ is the set of all vectors $\alpha$ in $V$ such that $(T - c_i I)^m \alpha = 0$ for some positive integer $m$ (which may depend upon $\alpha$).
    \item Prove that the dimension of $W_i$ is $d_i$. (\textit{Hint}: If $T_i$ is the operator induced on $W_i$ by $T$, then $T_i - c_i I$ is nilpotent; thus the characteristic polynomial for $T_i - c_i I$ must be $x^{e_i}$, where $e_i$ is the dimension of $W_i$ (proof?); thus the characteristic polynomial of $T_i$ is $(x - c_i)^{e_i}$; now use the fact that the characteristic polynomial for $T$ is the product of the characteristic polynomials of the $T_i$ to show that $e_i = d_i$.)
\end{enumerate}
\end{problem*}
\begin{proof}
    \vphantom{text}
    \begin{itemize}
        \item [(a)] Suppose 
        \[
            K_{c_i}(T) = \left\{ \alpha \in V : (T - c_i I)^m \alpha = 0 \text{ for some } m \in \mathbb{N}   \right\}, 
        \]
        then \(W_i \subseteq K_{c_i}(T)\) since \(W_i = \ker (T - c_i I)^{r_i}\). Now we show that \(K_{c_i}(T) \subseteq W_i\). If we have \((T - c_i I)^m \alpha = 0\) for some \(m \in \mathbb{N} \), then we can consider
        \[
            \mathrm{Ann}_T(\alpha ) = \left\{ f(x) : f(T)\alpha = 0 \right\},  
        \]     
        and we know \((x - c_i)^m \in \mathrm{Ann}_T(\alpha ) \), so if \(\mathrm{Ann}_T(\alpha ) = (h(x)) \), then \(h(x) \mid (x - c_i)^m\). Suppose \(h(x) = (x-c_i)^p\) for some \(p \in \mathbb{N} \), then we know \(p \le m\). Also, since \(m_T(x) \in \mathrm{Ann}_T(\alpha ) \), so \(h(x) \mid m_T(x) = (x - c_i)^{r_i} q_i(x)\), and thus \(p \le r_i\). Note that this means 
        \[
            0 = h(T)\alpha = (T - c_i I)^p \alpha \text{ for some } p \le r_i,  
        \]         
        so \((T - c_i I)^{r_i}\alpha = 0\), and thus \(\alpha \in \ker (T - c_i I)^{r_i} = W_i \). Hence, \(W_i = K_{c_i}(T)\).  
        \item [(b)] If \(T_i = T\vert_{W_i}\), then 
        \[
            (T_i - c_i I)^{r_i}\alpha = 0 \text{ for all } \alpha \in W_i = \ker (T - c_i I)^{r_i}, 
        \]
        so \(T - c_i I\) is nilpotent. Suppose \(P_i = T_i - c_i I\), then \(P_i^{r_i} \alpha = 0\) for all \(\alpha \in W_i\), and thus 
        \[
            m_{P_i}(x) \mid x^{r_i},
        \]    
        so \(m_{P_i}(x)\) has only \(0\) as its root, and thus 
        \[
            \mathrm{ch}_{P_i}(x) = x^{e_i} \text{ for some } e_i \in \mathbb{Z}.  
        \]  
        Note that 
        \[
            e_i = \mathrm{deg} \mathrm{ch}_{P_i} (x) = \dim W_i  
        \]
        since \(P_i\) is a linear operator on \(W_i\). Note that 
        \begin{align*}
            \mathrm{ch}_{P_i}(x) &= \det \left( x[I]_b - [T_1 - c_i I]_b \right) = \det ((x + c_i)[I]_b - [T_i]_b) = x^{e_i} \\
            \mathrm{ch}_{T_i}(x) &= \det (x [I]_b - [T_i]_b) = \mathrm{ch}_{P_i }(x - c_i) = (x - c_i)^{e_i}.    
        \end{align*} 
        Now since \(V = \bigoplus_{i=1}^k W_i\) by primary decomposition theorem, so we know 
        \[
            \prod _{i=1}^k (x - c_i)^{d_i} = \mathrm{ch}_T(x) = \prod _{i=1}^k \mathrm{ch}_{T_i}(x) = \prod _{i=1}^k (x - c_i)^{e_i},  
        \]  
        so we must have \(d_i = e_i = \dim W_i\) for all \(1 \le i \le k\).  
    \end{itemize}
\end{proof}

\begin{problem*}
\textbf{8.} Let $V$ be the space of $n \times n$ matrices over a field $F$, and let $A$ be a fixed $n \times n$ matrix over $F$. Define a linear operator $T$ on $V$ by $T(B) = AB - BA$. Prove that if $A$ is a nilpotent matrix, then $T$ is a nilpotent operator.
\end{problem*}
\begin{proof}
    If \(A\) is nilpotent, then \(A^m = 0\) but \(A^{m - 1} \neq 0\) for some \(m \ge 1\). Define \(L_A(B) = AB\) and \(R_A(B) = BA\), then \(T(B) = L_A(B) - R_A(B)\). Then, we know 
    \[
        L_A^m(B) = A^m B = 0 \text{ and } R_A^m(B) = B A^m = 0 \text{ for all } B \in V.  
    \]       
    Hence, \(L_A\) and \(R_A\) are both nilpotent. Also, since 
    \[
        L_A \left( R_A (B) \right) = ABA = R_A \left( L_A (B) \right) \text{ for all } B \in V,   
    \]  
    so \(L_A\) and \(R_A\) commute. Hence, we know for all \(N \in \mathbb{N} \) we have 
    \[
        T^N = (L_A - R_A)^N = \sum_{j=0}^N \binom{N}{j} (-1)^{N - j} L_A^j R_A^{N-j},
    \]   
    so if we pick \(N = 2m - 1\), then for all \(0 \le j \le N\) we know either \(j \ge m\) or \(N - j \ge m\), i.e. 
    \[
        \binom{N}{j} (-1)^{N - j} L_A^j R_A^{N - j} \text{ for all } 0 \le j \le N.
    \]    
    Hence, \(T^{2m - 1} = 0\) and thus \(T\) is nilpotent.  
\end{proof}

\begin{problem*}
\textbf{10.} Let $T$ be a linear operator on the finite-dimensional space $V$, let $p = p_1^{r_1} \cdots p_k^{r_k}$ be the minimal polynomial for $T$, and let $V = W_1 \oplus \cdots \oplus W_k$ be the primary decomposition for $T$, i.e., $W_j$ is the null space of $p_j(T)^{r_j}$. Let $W$ be any subspace of $V$ which is invariant under $T$. Prove that
$$
W = (W \cap W_1) \oplus (W \cap W_2) \oplus \cdots \oplus (W \cap W_k).
$$
\end{problem*}
\begin{proof}
    For all \(1 \le j \le k\), we define 
    \[
        m_T(x) = p_j(x)^{r_j} q_j(x), 
    \] 
    and note that \(p_j(x)^{r_j}\) and \(q_j(x)\) are coprime. Thus, 
    \[
        \exists a_j(x), b_j(x) \text{ s.t. } a_j(x) q_j(x) + b_j(x) p_j(x)^{r_j} = 1. 
    \]  
    Let \(E_j = a_j(T) q_j(T)\), then we know 
    \[
        E_j + b_j(T) p_j(T)^{r_j} = I.
    \] 
    Hence, for all \(x \in W_j\), we have 
    \[
        x = E_j x + b_j(T) p_j(T)^{r_j} x = E_j x,
    \] 
    so \(E_j x = x\). Also, for \(i \neq j\) and \(y \in W_i\) we know 
    \[
        E_j y = a_j(T) q_j(T) y = a_j(T) \left( \frac{q_j(T)}{p_i(T)^{r_i}} \right) p_i(T)^{r_i} y = 0. 
    \] 
    Now since for all \(x \in V\) we have \(x = w_1 + w_2 + \dots + w_k\) where \(w_i \in W_i \), so 
    \[
        x = w_1 + w_2 + \dots + w_k = \sum_{i=1}^k E_i x, 
    \]     
    i.e. \(1 = \sum_{i=1}^k E_i \). Now since \(E_j = a_j(T) q_j(T)\) and \(W\) is \(T\)-invariant, so \(E_j(W) \subseteq W\) and we have shown that \(E_j(W) \subseteq W_j\), so 
    \[
        W \subseteq \sum_{i=1}^k E_i(W) \subseteq \sum_{i=1}^k W \cap W_i,  
    \]      
    and since 
    \[
        \sum_{i=1}^k W \cap W_i \subseteq W, 
    \]
    so 
    \[
        W = \sum_{i=1}^k W \cap W_i. 
    \]
    Now we show \(\sum_{i=1}^k W \cap W_i \) is in fact a direct product, i.e.
    \[
        W \cap (W_i \cap W_j) = (W \cap W_i) \cap (W \cap W_j) = \left\{ 0 \right\} \text{ for any } i \neq j. 
    \]
    Suppose \(w \in W \cap W_i \cap W_j\), then \(p_i(T)^{r_i}(w) = p_j(T)^{r_j}(w) = 0\). Since \(p_i(x)^{r_i}\) and \(p_j(x)^{r_j}\) are coprime, so there exists \(u(x), v(x)\) s.t. 
    \[
        u(x) p_i(x)^{r_i} + v(x) p_j(x)^{r_j} = 1,
    \]     
    but this means 
    \[
        0 = u(T) p_i(T)^{r_i} (w) + v(T) p_j(T)^{r_j} (w) = w,
    \]
    so \(w = 0\), and we're done. 
\end{proof}

\section*{Section 7.3}
\begin{problem*}
\textbf{11.} Let $N_1$ and $N_2$ be $6 \times 6$ nilpotent matrices over the field $F$. Suppose that $N_1$ and $N_2$ have the same minimal polynomial and the same nullity. Prove that $N_1$ and $N_2$ are similar. Show that this is not true for $7 \times 7$ nilpotent matrices.
\end{problem*}
\begin{proof}
    For the \(6 \times 6\) cases. Since \(N_1\) is nilpotent, we know 
    \begin{align*}
        N_1 \sim J_{\lambda _1}(0) \oplus J_{\lambda _2}(0) \oplus \dots \oplus J_{\lambda _r}(0)
    \end{align*} 
    where \(\lambda _1 \ge \lambda _2 \ge \dots \ge \lambda _r\) are the size of different Jordan blocks and \(\sum_{i=1}^r \lambda _i = 6 \). Note that 
    \[
        m_{N_1}(x) = x^{\text{size of largest Jordan block}} = x^{\lambda _1}
    \]  
    since \(N_1\) is nilpotent and \(J_{\lambda _i}(0)^{\lambda _i} = 0\) for all \(i\). Also, since 
    \[
        J_{\lambda _i}(0) = \begin{pmatrix}
            0 & 1 & \cdots & 0  \\
            0 & 0 & \ddots & \vdots  \\
            \vdots & \vdots & \ddots & 1  \\
            0 & 0 & \cdots & 0  \\
        \end{pmatrix},
    \]   
    so every Jordan block contributes exactly \(1\) to the nullity of \(N_1\) since only the last row is a zero row. Now since \(N_1\) and \(N_2\) has same minimal polynomial and same nullity, so \(N_1\) and \(N_2\)'s Jordan form has same number of Jordan blocks and same size of largest Jordan block. Consider all the cases:
    \begin{itemize}
        \item Case 1: \(\lambda _1 = 6\), then \( 6 = 6\). 
        \item Case 2: \(\lambda _1 = 5\), then \(6 = 5 + 1\). 
        \item Case 3: \(\lambda _1 = 4\), then \(6 = 4 + 2 = 4 + 1 + 1\). 
        \item Case 4: \(\lambda _1 = 3\), then \(6 = 3 + 3 = 3 + 2 + 1 = 3 + 1 + 1+ 1\). 
        \item Case 5: \(\lambda _1 = 2\), then \(6 = 2 + 2 + 2 = 2 + 2 + 1 + 1 = 2 + 1 + 1 + 1\). 
        \item Case 6: \(\lambda _1 = 1\), then \(6 = 1 + 1 + 1 + 1 + 1 + 1\).        
    \end{itemize}    
    Hence, if we fix \(\lambda _1\) and the number of Jordan blocks, i.e. the number of parts of partition of \(6\), then the partition is unique, i.e. \(N_1\) and \(N_2\) must have same Jordan form. Hence, \(N_1 \sim J \sim N_2\) where \(J\) is their mutual Jordan form matrix. 
    
    For the \(7 \times 7\) cases. Since \(7 = 3 + 3 + 1 = 3 + 2 + 2\), so \(N_1\) and \(N_2\) may have different Jordan forms. In fact, consider 
    \[
        N_1 = \left( \begin{array}{ccc|ccc|c}
             0 & 1 &  &  &  &  &   \\
             & 0 & 1 &  &  &  &   \\
             &  & 0&  &  &  &   \\
             \hline
             &  &  & 0 & 1 &  &   \\
             &  &  &  & 0 & 1 &   \\
             &  &  &  &  & 0 &   \\
             \hline
             &  &  &  &  &  & 1  \\
        \end{array} \right), \quad N_2 = \left( \begin{array}{ccc|cc|cc}
             0& 1 &  &  &  &  &   \\
             & 0 & 1 &  &  &  &   \\
             &  & 0 &  &  &  &   \\
             \hline
             &  &  & 0 & 1 &  &   \\
             &  &  &  & 0 &  &   \\
             \hline
             &  &  &  &  & 0 & 1  \\
             &  &  &  &  &  & 0  \\
        \end{array} \right), 
    \]    
    then if \(N_1 \sim N_2\), we will have \(N_1 = P^{-1} N_2 P\) for some \(P\) and thus 
    \[
        \rank N_1^2 = \rank \left( P^{-1} N_2^2 P \right) = \rank N_2^2  
    \]   
    since left or right multiplication of invertible matrices will not change the rank. However, \(\rank N_1^2 = 2\) and \(\rank N_2^2 = 1\), so \(N_1\) is not similar to \(N_2\).    
\end{proof}