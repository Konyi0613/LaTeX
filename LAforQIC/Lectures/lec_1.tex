\chapter{Inner Product Space}
\section{Inner Products and Norms}
\begin{definition}[Inner product] \label{def: inner product}
  In a vector space \(V\) over \(F\), an \textbf{inner product} of \(V\) is a binary operation \(\langle \cdot, \cdot \rangle : V \times V \to F \) that satisfies for \(c \in F\) and \(x, y \in V\),  
  \begin{itemize}
    \item [(a)] \(\langle x + y, z \rangle = \langle x, z \rangle + \langle y, z \rangle   \)
    \item [(b)] \(\langle cx, y \rangle = c \langle x, y \rangle \). 
    \item [(c)] \(\langle y, x \rangle = \overline{\langle x, y \rangle }  \). 
    \item [(d)] \(\langle x, x \rangle > 0 \) for all non-zero \(x \in V\).     
  \end{itemize}
\end{definition}

\begin{definition}[Inner product space]
  An \textbf{inner product space} is a vector space with inner product.
\end{definition}

\begin{eg}
  For \(x = (a_1, a_2, \dots , a_n)\) and \(y = (b_1, b_2, \dots , b_n)\) in \(F^n\), define 
  \[
    \langle x, y \rangle = \sum_{i=1}^n a_i \overline{b_i}.  
  \]   
  Then, we can verify that \(\langle \cdot, \cdot \rangle \) is an inner product.   
\end{eg}

\begin{eg} \label{eg: Fourier inner product}
  Let \(V = C([0, 1])\), the vector space of real-valued continuous functions on \([0, 1]\). For \(f, g \in V\), define \(\langle f, g \rangle = \int _0^1 f(t) g(t) \, \mathrm{d} t  \). Since the preceding integral is linear in \(f\), so it satisfies (a), (b) of \autoref{def: inner product}, and (c), (d) are also trivial.     
\end{eg}

\begin{definition}[Conjugate transpose] \label{def: conjugate transpose}
  Let \(A \in M_{m \times n}(F)\). We define the \textbf{conjugate transpose} or \textbf{adjoint} of \(A\) to be the \(n \times m\) matrix \(A^*\) such that \(\left( A^* \right)_{ij} = \overline{A_{ji}}  \) for all \(i, j\).      
\end{definition}

\begin{corollary}
  If \(x, y\) are column vectors in \(F^n\), then \(\langle x, y \rangle = y^* x \).
\end{corollary}

\begin{eg}
  Let \(V = M_{n \times m}(F)\), and define \(\langle A, B \rangle = \Tr \left( B^* A \right)  \) for \(A, B \in V\). Then this is also an inner product, and we called it the \textbf{Frobenius inner product}.   
\end{eg}

\begin{eg}
  On \(C([0, 1])\), the space of continuous complex-valued functions defined on interval \([0, 2 \pi ]\) has an inner product:
  \[
    \langle f, g \rangle = \frac{1}{2 \pi } \int _0^{2 \pi } f(t) \overline{g(t)} \, \mathrm{d} t.   
  \] 
  This inner product is often used in the physical situations.
\end{eg}

\begin{theorem}
  Let \(V\) be an inner product space. Then for \(x, y, z \in V\) and \(c \in F\), the following statements are true:
  \begin{itemize}
    \item [(a)] \(\langle x, y + z \rangle = \langle x, y \rangle + \langle x, z \rangle   \). 
    \item [(b)] \(\langle x, cy \rangle = \overline{c} \langle x, y \rangle   \). 
    \item [(c)] \(\langle x, 0 \rangle = \langle 0, x \rangle = 0 \). 
    \item [(d)] \(\langle x, x \rangle = 0\) if and only if \(x = 0\). 
    \item [(e)] If \(\langle x, y \rangle = \langle x, z \rangle  \) for all \(x \in V\), then \(y = z\).  
  \end{itemize}  
\end{theorem}
\begin{proof}
  \todo{DIY}
\end{proof}

\begin{definition}[Norm] \label{def: norm}
  Let \(V\) be an inner product space. For \(x \in V\), we define the \textbf{norm} or \textbf{length} of \(x\) by \(\lVert x \rVert = \sqrt{\langle x, x \rangle }  \).    
\end{definition}

\begin{eg}
  Let \(V = F^n\). If \(x = (a_1, a_2, \dots , a_n )\), then 
  \[
    \lVert x \rVert = \lVert (a_1, a_2, \dots , a_n) \rVert = \left[ \sum_{i=1}^n \vert a_i \vert^2   \right]^{\frac{1}{2}}   
  \]  
  is the Euclidean definition of length. Note that if \(n = 1\), then \(\left\lVert a \right\rVert  = \vert a \vert  \).  
\end{eg}

\begin{theorem} \label{thm: norm properties}
  Let \(V\) be an inner product space over \(F\). Then for all \(x, y \in V\) and \(c \in F\), the following statements are true:
  \begin{itemize}
    \item [(a)] \(\lVert cx \rVert = \vert c \vert \cdot \lVert x \rVert   \). 
    \item [(b)] \(\lVert x \rVert = 0 \) if and only if \(x = 0\). In any case, \(\lVert x \rVert \ge 0 \). 
    \item [(c)] (Cauchy-Schwarz inequality) \(\vert \langle x, y \rangle  \vert \le \lVert x \rVert \cdot \lVert y \rVert   \). 
    \item [(d)] (Triangle Inequality) \(\lVert x + y \rVert \le \lVert x \rVert + \lVert y \rVert   \).      
  \end{itemize}    
\end{theorem}
\begin{proof}
  We only show (c) and (d). 

  For (c), if \(y = 0\), then it is true. Now suppose \(y \neq 0\), then for any \(x \in F\), we have 
  \begin{align*}
    0 \le \lVert x - cy \rVert^2 &= \langle x - cy, x - cy \rangle = \langle x, x - cy \rangle - c \langle y, x - cy \rangle \\
    &= \langle x, x \rangle - \overline{c} \langle x, y \rangle - c\langle y, x \rangle + c\overline{c} \langle y, y \rangle.          
  \end{align*}   
  In particular, if we set 
  \[
    c = \frac{\langle x, y \rangle }{\langle y, y \rangle },
  \]
  the inequality becomes 
  \[
    0 \le \langle x, x \rangle - \frac{\vert \langle x, y \rangle  \vert^2 }{\langle y, y \rangle } = \lVert x \rVert^2 - \frac{\vert \langle x, y \rangle  \vert^2 }{\lVert y \rVert^2 },  
  \]
  so we know (c) is true. 

  As for (d), we have 
  \begin{align*}
    \lVert x + y \rVert^2 &= \langle x + y, x + y \rangle = \langle x, x \rangle + \langle y, x \rangle + \langle x, y \rangle + \langle y, y \rangle \\
    &= \lVert x \rVert^2 + 2\Re \langle x, y \rangle + \lVert y \rVert^2 \\
    &\le \lVert x \rVert^2 + 2 \vert \langle x, y \rangle  \vert + \lVert y \rVert^2 \\
    &\le \lVert x \rVert^2 + 2 \lVert x \rVert \cdot \lVert y \rVert + \lVert y \rVert^2 \\
    &= \left( \lVert x \rVert + \lVert y \rVert   \right)^2.                
  \end{align*}
  Note that we use (c) to prove (d).
\end{proof}

\begin{corollary}
  If \((a_1, a_2, \dots , a_n), (b_1, b_2, \dots , b_n) \in \mathbb{C} ^n\), then we have 
  \[
    \left\vert \sum_{i=1}^n  a_i \overline{b_i}  \right\vert \le \left[ \sum_{i=1}^n \vert a_i \vert^2   \right]^{\frac{1}{2}} \left[ \sum_{i=1}^n \vert b_i \vert^2   \right]^{\frac{1}{2}}   
  \]  
  and 
  \[
    \left[ \sum_{i=1}^n \vert a_i + b_i  \vert   \right]^{\frac{1}{2}} \le \left[ \sum_{i=1}^n \vert a_i \vert^2   \right]^{\frac{1}{2}} + \left[ \sum_{i=1}^n \vert b_i \vert^2   \right]^{\frac{1}{2}}.   
  \]
\end{corollary}
\begin{proof}
  Just use (c) and (d) of \autoref{thm: norm properties} with standard inner product. 
\end{proof}

\begin{remark}
  We have learnt that in \(\mathbb{R} ^3\) and \(\mathbb{R} ^2\) we have 
  \[
    \langle x,y \rangle = \lVert x \rVert \cdot \lVert y \rVert \cos \theta,
  \]  
  where \(0 \le \theta \le \pi \), and this can be observed by first equation. 
\end{remark}

In \(\mathbb{R} ^3\) and \(\mathbb{R} ^2\), we say two vectors \(x, y\)  are perpendicular if and only if \(\langle x, y \rangle = 0 \). Now we generalize the notion of perpendicularity to arbitrary inner product spaces. 

\begin{definition}[orthogonality] \label{def: orthogonality}
  Let \(V\) be an inner product space. Vectors \(x\) and \(y\) in \(V\) are \textbf{orthogonal(perpendicular)} if \(\langle x, y \rangle = 0 \). A subset of \(V\) is \textbf{orthogonal} if any two distinct vectors in \(S\) are orthogonal. A vector \(x \in V\) is a \textbf{unit vector} if \(\lVert x \rVert = 1 \). Finally, a subset \(S\) of \(V\) is \textbf{orthonormal} if \(S\) is orthogonal and consists entirely of unit vectors.             
\end{definition}

\begin{proposition}
  \(S = \left\{ v_1, v_2, \dots \right\} \) is orthonormal if and only if 
  \[
    \langle v_i, v_j \rangle = \delta _{ij}. 
  \]
\end{proposition}

\begin{eg} \label{eg: Fourier orthonormal basis}
  Recall the inner product introduced in \autoref{eg: Fourier inner product}. Now we introduce an orthonormal subset under this inner product. For any integer \(n\), let \(f_n(t) = e^{int }\), where \(0 \le t \le 2 \pi \). (Recall that \(e^{i n t} = \cos nt + i \sin nt\).) Now define \(S = \left\{ f_n : n \in \mathbb{Z}  \right\} \). Using the property that \(\overline{e^{it}} = e^{-it} \) for every real number \(t\), we have, for \(m \neq n\), 
  \begin{align*}
    \langle f_m, f_n \rangle &= \frac{1}{2\pi } \int _0^{2\pi } e^{i m t} \overline{e^{i n t}} = \frac{1}{2 \pi } \int _0^{2\pi } e^{i(m - n) t} \, \mathrm{d} t \\
    &= \left. \frac{1}{2 \pi (m - n)} e^{i(m - n) t}  \right\vert  _0^{2 \pi } = 0. 
  \end{align*}        
  Also, 
  \[
    \langle f_n, f_n  \rangle = \frac{1}{2 \pi } \int _0^{2\pi } e^{i(n - n) t} \, \mathrm{d} t = \frac{1}{2 \pi } \int _0^{2 \pi } 1 \, \mathrm{d} t = 1.   
  \]
  In other words, \(\langle f_m, f_n \rangle = \delta _{mn} \). 
\end{eg}

\begin{proposition}
  For any vector \(x\), \(\frac{x}{\lVert x \rVert }\) is a unit vector. The process of multiplying a nonzero vector by the reciprocal of its length is called \textbf{normalizing}.
\end{proposition}

\section{The Gram-Schmidt Orthogonalization Process and Orthogonal Complements}

\begin{definition}[orthonormal basis] \label{def: orthonormal basis}
  Let \(V\) be an inner product space. A subset of \(V\) is an \textbf{orthonormal basis} for \(V\) if it is an ordered basis that is orthonormal.    
\end{definition}

\begin{theorem} \label{thm: vectors in span of orthogonal set}
  Let \(V\) be an inner product space and \(S = \left\{ v_1, v_2, \dots , v_k \right\} \) be an orthogonal subset of \(V\) consisting of nonzero vectors. If \(y \in \mathrm{span}(S)\), then 
  \[
    y = \sum_{i=1}^k \frac{\langle y, v_i \rangle }{\lVert v_i \rVert^2 } v_i. 
  \]    
\end{theorem}
\begin{proof}
  Write \(y = \sum_{i=1}^k a_i v_i \), where \(a_1, a_2, \dots , a_k \in F\). Then, for \(1 \le j \le k\), we have 
  \[
    \langle y, v_j \rangle = \left\langle \sum_{i=1}^k a_i v_i, v_j  \right\rangle = \sum_{i=1}^k a_i \langle v_i, v_j \rangle = a_j \langle v_j, v_j \rangle = a_j \lVert v_j \rVert^2.    
  \]  
  Thus, 
  \[
    a_j = \frac{\langle y, v_j \rangle }{\lVert v_j \rVert^2 }.
  \]
\end{proof}

\begin{corollary} \label{cl: vectors in span of orthonormal set}
  If, in addition to the hypothesis of \autoref{thm: vectors in span of orthogonal set}, \(S\) is orthonormal and \(y \in \mathrm{span}(S) \), then 
  \[
    y = \sum_{i=1}^k \langle y, v_i \rangle v_i.  
  \]   
\end{corollary}

Thus, if \(V\) possesses a finite orthonormal basis, then \autoref{cl: vectors in span of orthonormal set} allows us to compute the coefficients in a linear combination very easily. 

\begin{corollary} \label{cl: orthogonal sets are linearly independent}
  Let \(V\) be an inner product space, and let \(S\) be an orthogonal subset of \(V\) consisting of nonzero vectors. Then \(S\) is linearly independent.   
\end{corollary}
\begin{proof}
  Suppose that \(v_1, v_2, \dots , v_k \in S \) and 
  \[
    \sum_{i=1}^k a_i v_i = 0. 
  \] 
  Then, as in proof of \autoref{thm: vectors in span of orthogonal set} with \(y = 0\), we have 
  \[
    a_j = \frac{\langle 0, v_j \rangle }{\lVert v_j \rVert^2 } = 0
  \] for all \(j\), so \(S\) is linearly independent.  
\end{proof}

Now we show a method to transform any basis of any vector space into an orthogonal basis. 

\begin{theorem}[Gram-Schmidt Process] \label{thm: Gram-Schmidt Process}
  Let \(V\) be an inner product space and \(S = \left\{ w_1, w_2, \dots , w_n \right\} \) be a linearly independent subset of \(V\). Define \(S^{\prime} = \left\{ v_1, v_2, \dots , v_n \right\} \), where \(v_1 = w_1\) and 
  \[
    v_k = w_k - \sum_{j=1}^{k-1} \frac{\langle w_k, v_j \rangle }{\lVert v_j \rVert^2 } v_j \quad \text{for } 2 \le k \le n.  
  \]     
  Then \(S^{\prime} \) is an orthogonal set of nonzero vectors such that \(\mathrm{span} \left( S^{\prime}  \right) = \mathrm{span} (S)   \).  
\end{theorem}

\begin{proof}
  The proof is by mathematical induction on \(n\), the number of vectors in \(S\). For \(k = 1, 2, \dots , n\), let \(S_k = \left\{ w_1, \dots , w_k \right\} \). If \(n = 1\), then the theorem is true. Now suppose this theorem is true for \(n = k - 1 \ge 1\), then we can first construct \(S_{k-1}\) into \(S_{k-1}^{\prime} = \left\{ v_1, v_2, \dots , v_{k-1}  \right\} \) and we know \(S_{k-1}^{\prime} \) is an orthogonal set of nonzero vectors and \(\mathrm{span} \left( S_{k-1}^{\prime}  \right) = \mathrm{span} (S_{k-1})   \), and now we construct \(S_k^{\prime} \) and show that it is also orthogonal and \(v_k \neq 0\) and \(\mathrm{span} (S_k^{\prime} ) = \mathrm{span} (S_k)  \). We first show that \(v_k \neq 0\). If \(v_k = 0\), then
  \[
    0 = w_k - \sum_{j=1}^{k-1} \frac{\langle v_k, v_j \rangle }{\lVert v_j \rVert^2 } v_j,
  \]             
  which shows \(w_k \in \mathrm{span} \left( S_{k-1}^{\prime}  \right) = \mathrm{span} (S_{k-1})   \), but this contradicts to the condition that \(S\) is linearly independent. Also, for \(1 \le i \le k - 1\), 
  \[
    \langle v_k, v_i \rangle = \langle w_k, v_i \rangle - \sum_{j=1}^{k-1} \frac{\langle w_k, v_j \rangle }{\lVert v_j \rVert^2 } \langle v_j, v_i \rangle = \langle w_k, v_i \rangle - \frac{\langle w_k, v_i \rangle }{\lVert v_i \rVert^2 } \lVert v_i \rVert^2 = 0,    
  \]  
  so \(S_{k}^{\prime} \) is orthogonal. Also, note that \(\mathrm{span}(S_k^{\prime} ) \subseteq \mathrm{span}(S_k) \) and 
  \[
    \dim \mathrm{span} (S_k^{\prime} ) = k = \dim \mathrm{span} (S_k)  
  \] since orthogonal sets are linearly independent, so we have \(\mathrm{span} (S_k^{\prime} ) = \mathrm{span} (S_k)  \). 
\end{proof}

\begin{theorem} \label{thm: vector representation by orthonormal basis}
  Let \(V\) be a nonzero finite-dimensional inner product space. Then \(V\) has an orthonormal basis \(\beta \). Furthermore, if \(\beta = \left\{ v_1, v_2, \dots , v_n \right\} \) and \(x \in V\), then 
  \[
    x = \sum_{i=1}^n \langle x, v_i \rangle v_i.
  \]     
\end{theorem}

\autoref{thm: vector representation by orthonormal basis} gives us a simple method for computing the entries of the matrix representation of a linear operator with respect to an orthonormal basis. 

\begin{corollary} \label{cl: matrix representation by orthonormal basis}
  Let \(V\) be a finite-dimensional inner product space with an orthonormal basis \(\beta = \left\{ v_1, v_2, \dots , v_n \right\} \). Let \(T\) be a linear operator on \(V\), and let \(A = [T]_{\beta }\). Then for any \(i\) and \(j\), we have 
  \[
    A_{ij} = \left\langle T(v_j), v_i \right\rangle.
  \]       
\end{corollary}
\begin{proof}
  From \autoref{thm: vector representation by orthonormal basis}, we have 
  \[
    T(v_j) = \sum_{i=1}^n \langle T(v_j), v_i \rangle v_i.  
  \] 
  Hence, \(A_{ij} = \langle T(v_j), v_i \rangle \). 
\end{proof}

\begin{definition}[Fourier coefficients] \label{def: Fourier coefficients}
  Let \(\beta \) be an orthonormal subset (possibly infinite) of an inner product space \(V\), and let \(x \in V\). We define the \textbf{Fourier coefficients} of \(x\) relative to \(\beta \) to be the scalars \(\langle x, y \rangle \), where \(y \in \beta \).        
\end{definition}

In the first half of the 19th century, the French mathematician Jean Baptiste Fourier was associated with the study of the scalars 
\[
  \int _0^{2\pi } f(t) \sin nt \, \mathrm{d} t \quad \text{and} \quad \int _0^{2\pi } f(t) \cos nt \, \mathrm{d} t,   
\]
or more generally, 
\[
  c_n = \frac{1}{2\pi } \int _0^{2 \pi } f(t) e^{- i n t} \, \mathrm{d} t, 
\]
for a function \(f\). In the context of \autoref{eg: Fourier orthonormal basis}, we see that \(c_n = \langle f, f_n \rangle \), where \(f_n(t) =e^{i n t}\); that is, \(c_n\) is the \(n\)-th Fourier coefficient for a continuous function \(f \in V\) relative to \(S\). These coefficients are the classical Fourier coefficients of a function, and the literature concerning the behaviour of these coefficients is extensive.

\begin{exercise}[Bessel's Inequality] \label{ex: Bessel's inequality}
  \vphantom{text}
  \begin{itemize}
    \item [(a)] Let \(V\) be an inner product space, and let \(S = \left\{ v_1, v_2, \dots , v_n \right\} \) be an orthonormal subset of \(V\). Prove that for any \(x \in V\) we have 
    \[
      \lVert x \rVert^2 \ge \sum_{i=1}^n \left\vert \langle x, v_i \rangle  \right\vert^2.   
    \]
    \textit{Hint:} Apply \autoref{thm: orthogonal projection is unique} to \(x \in V\) and \(W = \mathrm{span}(S) \). Then use Pythagoras theorem in inner product space. 
    \item [(b)] In the context of (a), prove that Bessel's inequality is an equality if and only if \(x \in \mathrm{span}(S) \).   
  \end{itemize}
\end{exercise}

\begin{eg}
  Let \(S = \left\{ e^{i n t} : n \text{ is an integer}  \right\} \). We have shown that \(S\) is an orthonormal set under the inner product in \autoref{eg: Fourier inner product}. Now we compute the Fourier coefficients of \(f(t) = t\) relative to \(S\). Using integration by parts, we have, for \(n \neq 0\), 
  \[
    \langle f, f_n \rangle = \frac{1}{2 \pi } \int _0^{2\pi } t \overline{e^{i n t}} = \frac{1}{2\pi } \int _0^{2\pi } t e^{-i nt} \, \mathrm{d} t = \frac{-1}{i n},  
  \]    
  and, for \(n = 0\), 
  \[
    \langle f, 1 \rangle = \frac{1}{2 \pi } \int _0^{2 \pi } \int _0^{2 \pi } t(1) \, \mathrm{d} t = \pi .  
  \] 
  Thus, by \hyperref[ex: Bessel's inequality]{Bessel's inequality}, we know 
  \begin{align*}
    \lVert f \rVert^2 &\ge \sum_{n = -k}^{-1} \left\vert \lVert f, f_n \rVert  \right\vert^2 + \left\vert \langle f, 1 \rangle  \right\vert^2 + \sum_{n=1}^k \left\vert \langle f, f_n \rangle  \right\vert^2 \\
    &= \sum_{n=-k}^{-1} \frac{1}{n^2} + \pi ^2 + \sum_{n=1}^k \frac{1}{n^2} \\
    &= 2 \sum_{n=1}^k \frac{1}{n^2} + \pi ^2         
  \end{align*}
  for every \(k\). Now, using the fact that \(\lVert f \rVert^2 = \frac{4}{3} \pi ^2 \), we obtain 
  \[
    \frac{4}{3} \pi ^2 \ge 2 \sum_{n=1}^k \frac{1}{n^2} + \pi ^2 
  \]  
  or 
  \[
    \frac{\pi ^2}{6} \ge \sum_{n=1}^k \frac{1}{n^2}. 
  \]
  Because this inequality holds for all \(k\), we may let \(k \to \infty \) to obtain 
  \[
    \frac{\pi ^2}{6} \ge \sum_{n=1}^{\infty} \frac{1}{n^2}. 
  \]  
\end{eg}

We are now ready to proceed with the concept of \textit{orthogonal complement}. 

\begin{definition}[orthogonal complement] \label{def: orthogonal complement}
  Let \(S\) be a non-empty subset of an inner product space \(V\). We define \(S^{\perp }\) to be the set of all vectors in \(V\) that are orthogonal to every vector in \(S\); that is, 
  \[
    S^\perp = \left\{ x \in V: \langle x, y \rangle = 0 \text{ for all } y \in S  \right\}. 
  \]     
  The set \(S^\perp \) is called the \textbf{orthogonal complement} of \(S\).   
\end{definition}

\begin{proposition}
  \(S^\perp \) is a subspace of \(V\) for any subset \(S\) of \(V\).    
\end{proposition}

\begin{eg}
  \(\left\{ 0 \right\}^\perp = V \) and \(V^\perp = \left\{ 0 \right\} \).  
\end{eg}

\begin{theorem} \label{thm: orthogonal projection is unique}
  Let \(W\) be a finite-dimensional subspace of an inner product space \(V\), and let \(y \in V\). Then there exist unique vectors \(u \in W\) and \(z \in W^{\perp }\) such that \(y = u + z\). Furthermore, if \(\left\{ v_1, v_2, \dots , v_k \right\} \) is an orthonormal basis for \(W\), then 
  \[
    u = \sum_{i=1}^k \langle y, v_i \rangle v_i.
  \]        
\end{theorem}
\begin{proof}
  We can check that for \(u = \sum_{i=1}^k \langle y, v_i \rangle v_i  \), we have \(u \in W\) and if we let \(z = y - u\), then for any \(j\) we have 
  \begin{align*}
    \langle z, v_j \rangle &= \left\langle \left( y - \sum_{i=1}^k \langle y, v_i \rangle v_i   \right), v_j  \right\rangle = \langle y, v_j \rangle - \sum_{i=1}^k \langle y, v_i \rangle \langle v_i, v_j \rangle \\
    &= \langle y, v_j \rangle - \langle y, v_j \rangle = 0.        
  \end{align*}   
  Thus, \(z \in W^{\perp }\). 
  
  To show the uniqueness of \(u\) and \(z\), suppose that \(y = u + z = u^{\prime} + z^{\prime} \), where \(u^{\prime} \in W\) and \(z^{\prime} \in W^\perp \). Then \(u - u^{\prime} = z^{\prime} - z \in W \cap W^\perp = \left\{ 0 \right\} \). Thus, \(u = u^{\prime} \) and \(z = z^{\prime} \).
  
  \begin{remark}
    We know \(W \cap W^\perp = \left\{ 0 \right\} \) because if \(x \in W \cap W^{\perp }\), then since \(W\) is an inner product space and 
    \[
      \langle x, v \rangle = 0 = \langle 0, v \rangle \quad \forall v \in W,
    \]   
    so \(x = 0\). 
  \end{remark}
\end{proof}

\begin{remark}
  Suppose \(P_E\) is the projection operator from \(W\) to \(E = \mathrm{span} \left\{ v_1, v_2, \dots , v_k \right\}  \), then 
  \[
    P_E = \sum_{i=1}^k v_i v_i^* 
  \] 
  since \(P_E v = u = \sum_{i=1}^k \langle y, v_i \rangle v_i  \). 
\end{remark}

\begin{corollary} \label{cl: orthogonal projection is the closest vector}
  In the notation of \autoref{thm: orthogonal projection is unique}, the vector \(u\) is the unique vector in \(W\) that is closest to \(y\); that is, for any \(x \in W\), \(\lVert y - x \rVert \ge \lVert y - u \rVert  \), and this inequality is an equality if and only if \(x = u\).       
\end{corollary}
\begin{proof}
  As in \autoref{thm: orthogonal projection is unique}, we have \(y = u + z\), where \(z \in W^\perp \). Let \(x \in W\). Then \(u - x\) is orthogonal to \(z\), so we have 
  \begin{align*}
    \lVert y - x \rVert^2 &= \lVert u + z - x \rVert^2 = \lVert (u - x) + z \rVert^2 = \lVert u - x \rVert^2 + \lVert z \rVert^2 \\
    \ge \lVert z \rVert^2 = \lVert y - u \rVert^2.       
  \end{align*}      
  Now suppose that \(\lVert y - x \rVert = \lVert y - u \rVert  \). Then \(\lVert u - x \rVert^2 = 0 \) must occur, so \(u = x\).   
\end{proof}

\begin{remark}
  The vector \(u\) in \autoref{cl: orthogonal projection is the closest vector} is called the \textbf{orthogonal projection} of \(y\) on \(W\). We will see the importance of orthogonal projections of vectors in the application to least squares in next section.     
\end{remark}

\begin{theorem}
  Suppose that \(S = \left\{ v_1, v_2, \dots , v_k \right\} \) is an orthonormal set in an \(n\)-dimensional inner product space \(V\). Then, 
  \begin{itemize}
    \item [(a)] \(S\) can be extended to an orthonormal basis \(\left\{ v_1, v_2, \dots , v_k, v_{k+1}, \dots , v_n \right\} \) for \(V\). 
    \item [(b)] If \(W = \mathrm{span}(S) \), then \(S_1 = \left\{ v_{k + 1}, v_{k + 2}, \dots , v_n \right\} \) is an orthonormal basis for \(W^\perp \) (using the preceding notation). 
    \item [(c)] If \(W\) is any subspace of \(V\), then \(\dim V = \dim W + \dim W^\perp \).         
  \end{itemize}   
\end{theorem}
\begin{proof}
  \vphantom{text}
  \begin{itemize}
    \item [(a)] Extend \(S\) to any basis of \(V\) then do Gram-Schmidt on it. 
    \item [(b)] Show that every vector in \(S_1\) is in \(W^{\perp }\) then show \(S_1\) spans \(W^\perp \). 
    \item [(c)] Use the fact that \(V = W \oplus W^\perp \).    
  \end{itemize}
\end{proof}

\section{The Adjoint of a Linear Operator}
\begin{theorem}[Riesz representation theorem] \label{thm: Riesz representation theorem}
  Let \(V\) be a finite-dimensional inner product space over \(F\), and let \(g : V \to F\) be a linear transformation. Then there exists a unique vector \(y \in V\) s.t. \(g(x) = \langle x, y \rangle \) for all \(x \in V\).      
\end{theorem}
\begin{proof}
  Let \(\beta = \left\{ v_1, v_2, \dots , v_n \right\} \) be an orthonormal basis for \(V\), and let 
  \[
    y = \sum_{i=1}^n \overline{g(v_i)} v_i.  
  \] 
  Define \(h: V \to F\) by \(h(x) = \langle x, y \rangle \), which is clearly linear. Furthermore, for \(1 \le j \le n\) we have 
  \begin{align*}
    h(v_j) = \langle v_j, y \rangle = \left\langle v_j, \sum_{i=1}^n \overline{g(v_i)} v_i   \right\rangle = \sum_{i=1}^n g(v_i) \langle v_j, v_i \rangle = g(v_j).    
  \end{align*}   
  Since \(g\) and \(h\) both agree on \(\beta \), we have that \(g = h\). To show that \(y\) is unique, suppose that \(g(x) = \langle x, y^{\prime}  \rangle \) for all \(x\). Then, \(\langle x, y \rangle = \langle x, y^{\prime}  \rangle  \) for all \(x\), so we have \(y = y^{\prime} \).          
\end{proof}

\begin{theorem} \label{thm: adjoint exists and unique}
  Let \(V\) be a finite-dimensional inner product space, and let \(T\) be a linear operator on \(V\). Then there exists a unique function \(T^* : V \to V\) such that \(\langle T(x), y \rangle = \langle x, T^*(y) \rangle  \) for all \(x, y \in V\). Furthermore, \(T^*\) is linear.       
\end{theorem}

\begin{proof}
  Let \(y \in V\). Define \(g: V \to F\) by \(g(x) = \langle T(x), y \rangle \) for all \(x \in V\). We can easily show that \(g\) is linear. Thus, by \autoref{thm: Riesz representation theorem}, we know there exists unique \(y^{\prime} \in V\) s.t. \(g(x) = \langle x, y^{\prime}  \rangle \); that is, \(\langle T(x), y \rangle = \langle x, y^{\prime}  \rangle  \) for all \(x \in V\). Defining \(T^* : V \to V\) by \(T^*(y) = y^{\prime} \), we have \(\langle T(x), y \rangle = \langle x, T^*(y) \rangle  \). To show that \(T^*\) is linear, let \(y_1, y_2 \in V\) and \(c \in F\). Then for any \(x \in V\), we have 
  \begin{align*}
    \langle x, T^*(c y_1 + y_2) \rangle &= \langle T(x), cy_1 + y_2 \rangle \\
    &= \overline{c} \langle T(x), y_1 \rangle + \langle T(x), y_2 \rangle \\
    &= \overline{c} \langle x, T^*(y_1) \rangle + \langle x, T^*(y_2) \rangle \\
    &= \langle x, c T^*(y_1) + T^*(y_2) \rangle,   
  \end{align*}                
  and since \(x\) is arbitrary, \(T^*(c y_1 + y_2) = c T^*(y_1) + T^*(y_2)\).

  Finally, we need to show that \(T^*\) is unique. Suppose that \(U : V \to V\) is linear and that it satisfies \(\langle T(x), y \rangle = \langle x, U(y) \rangle  \) for all \(x, y \in V\). Then \(\langle x, T^*(y) \rangle = \langle x, U(y) \rangle  \) for all \(x, y \in V\), so \(T^* = U\).       
\end{proof}

\begin{remark}
  The linear operator \(T^*\) described in \autoref{thm: adjoint exists and unique} is called the \textbf{adjoint} of the operator \(T\). The symbol \(T^*\) is read "\(T\) star". Note that we also have 
  \[
    \langle x, T(y) \rangle = \overline{\langle T(y), x \rangle } = \overline{\langle y, T^*(x) \rangle } = \langle T^*(x), y \rangle,    
  \]    
  so \(\langle x, T(y) \rangle = \langle T^*(x), y \rangle  \) for all \(x, y \in V\). 
\end{remark}

\begin{theorem}
  Let \(V\) be a finite-dimensional inner product space, and let \(\beta \) be an orthonormal basis for \(V\). If \(T\) is a linear operator on \(V\), then 
  \[
    \left[ T^* \right]_\beta = \left[ T \right]_{\beta }^*.  
  \]     
\end{theorem}
\begin{proof}
  Let \(A = [T]_\beta \), \(B = [T^*]_\beta \), and \(\beta = \left\{ v_1, v_2, \dots , v_n \right\} \). Then
  \[
    B_{ij} = \langle T^*(v_j), v_i \rangle = \overline{\langle v_i, T^*(v_j) \rangle } = \overline{\langle T(v_i), v_j \rangle } = \overline{A_{ji}} = \left( A^* \right)_{ij}.     
  \]   
  Hence, \(B = A^*\). 
\end{proof}

\begin{theorem}
  Let \(V\) be an inner product space, and let \(T\) and \(U\) be linear operators on \(V\). Then
  \begin{itemize}
    \item [(a)] \((T + U)^* = T^* + U^*\);
    \item [(b)] \((cT)^* = \overline{c} T^* \) for any \(c \in F\);
    \item [(c)] \((TU)^* = U^* T^*\); 
    \item [(d)] \(T^{* *} = T\); 
    \item [(e)] \(I^* = I\).    
  \end{itemize}    
\end{theorem}

\section{Isometries and unitary operators. Unitary and orthogonal matrices}

\begin{definition}[Isometries] \label{def: Isometries}
  An operator \(U: X \to Y\) is called an \textbf{isometry}, if it preserves the norm, 
  \[
    \lVert U \mathbf{x}  \rVert = \lVert \mathbf{x} \rVert \quad \forall \mathbf{x} \in X.
  \] 
\end{definition}

\begin{theorem} \label{thm: isometry preserves inner pd}
  An operator \(U: X \to Y\) is an isometry if and only if it preserves the inner product, i.e. if and only if 
  \[
    \langle x, y \rangle = \langle Ux, Uy \rangle \quad \forall x, y \in X.  
  \] 
\end{theorem}
\begin{proof}
  For complex space, 
  \begin{align*}
    \langle Ux, Uy \rangle &= \frac{1}{4} \sum_{\alpha = \pm 1, \pm i} \alpha \lVert Ux + \alpha U y \rVert^2 \\
    &= \frac{1}{4} \sum_{\alpha = \pm 1, \pm i} \alpha \lVert U ( x + \alpha y) \rVert^2 \\
    &= \frac{1}{4} \sum_{\alpha = \pm 1, \pm i} \alpha n\lVert x + \alpha y \rVert^2 = \langle x, y \rangle.     
  \end{align*}
  For real space, 
  \begin{align*}
    \langle Ux, Uy \rangle &= \frac{1}{4} \left( \lVert Ux + Uy \rVert^2 - \lVert Ux - Uy \rVert^2   \right) \\
    &= \frac{1}{4} \left( \lVert U(x + y) \rVert^2 - \lVert U(x - y) \rVert^2   \right) \\
    &= \frac{1}{4} \left( \lVert x + y \rVert^2 - \lVert x - y \rVert^2   \right) = \langle x, y \rangle .     
  \end{align*}
\end{proof}

\begin{lemma}
  An operator \(U: X \to Y\) is an isometry if and only if \(U^* U = I\).  
\end{lemma}
\begin{proof}
  If \(U^* U = I\), then by the definition of adjoint operator 
  \[
    \langle x, x \rangle = \langle U^* U x, x \rangle = \langle Ux, Ux \rangle \quad \forall x \in X.   
  \] 

  On the other hand, if \(U\) is an isometry, then we know for all \(x \in X\) 
  \[
    \langle U^* U x, y \rangle = \langle Ux, Uy \rangle = \langle x, y \rangle \quad \forall y \in X,   
  \] 
  and thus we know \(U^* U x = x\) for all \(x \in X\), i.e. \(U^* U = I\).   
\end{proof}

\begin{remark}
  The above lemma implies that an isometry is always left invertible(\(U^*\) being a left inverse).
\end{remark}

\begin{definition} \label{def: unitary operator}
  An isometry \(U : X \to Y\) is called a \textbf{unitary operator} if it is invertible.  
\end{definition}

\begin{proposition}
  An isometry \(U: X \to Y\) is a unitary operator if and only if \(\dim X = \dim Y\).  
\end{proposition}
\begin{proof}
  Since \(U\) is an isometry, it is left invertible, and since \(\dim X = \dim Y\) (a left invertible square matrix is invertible). 
  
  On the other hand, if \(U: X \to Y\) is invertible, then we must have \(\dim X = \dim Y\).  
\end{proof}


\begin{remark}
  A square matrix \(U\) is called unitary if \(U^* U = I\), i.e. a unitary matrix is a matrix of a unitary operator acting in \(\mathbb{F} ^n\). 
  
  A unitary matrix with real entries is called an orthogonal matrix. An orthogonal matrix can be interpreted a matrix of a unitary operator acting in the real space \(\mathbb{R} ^n\). 
  
  Few properties of unitary operators: 
  \begin{itemize}
    \item [1.] For a unitary transformation \(U\), \(U^{-1} = U^*\).  
    \item [2.] If \(U\) is unitary, \(U^* = U^{-1}\) is also unitary.  
    \item [3.] If \(U\) is a isometry, and \(v_1, v_2, \dots , v_n\) is an orthonormal basis, then \(U v_1, U v_2, \dots , U v_n\) is an orthonormal system. Moreover, if \(U\) is unitary, \(U v_1, U v_2, \dots , U v_n\) is an orthonormal basis.     
    \item [4.] A product of unitary operaotrs is a unitary operator as well.
  \end{itemize}
\end{remark}

\begin{eg}
  First of all, let us notice, that
  \begin{mdframed}
    A matrix \(U\) is an isometry if and only if its columns form an orthonormal system. 
  \end{mdframed}
  This statement can be checked directly by computing the product \(U^* U\).
  
  It is easy to check that the columns of the rotation matrix 
  \[
    \begin{pmatrix}
      \cos \alpha  & -\sin \alpha   \\
      \sin \alpha  & \cos \alpha   \\
    \end{pmatrix}
  \]
  are orthogonal to each other, and that each column has norm \(1\). Therefore, the rotation matrix is an isometry, and since it is square, it is unitary. Since all entries of the rotation matrix are real, it is an orthogonal matirx. 
\end{eg}