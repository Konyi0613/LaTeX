\documentclass[12pt,letterpaper]{report}

\usepackage{times}
\usepackage[T1]{fontenc}
\usepackage{mathrsfs}
\usepackage{latexsym}
\usepackage[dvips]{graphics}
\usepackage{comment}
\usepackage{epsfig}
%\usepackage{hyperref, amsmath, amsthm, amsfonts, amscd, flafter,epsf}
\usepackage{amsmath,amsfonts,amsthm,amssymb,amscd}
\input amssym.def
\input amssym.tex


\newcommand\st{\mbox{s.t.\ }}
\newcommand\be{\begin{equation}}
\newcommand\ee{\end{equation}}
\newcommand\bea{\begin{eqnarray}}
\newcommand\eea{\end{eqnarray}}
\newcommand\bi{\begin{itemize}}
\newcommand\ei{\end{itemize}}
\newcommand\ben{\begin{enumerate}}
\newcommand\een{\end{enumerate}}
\newcommand\bc{\begin{center}}
\newcommand\ec{\end{center}}
\newcommand\ba{\begin{array}}
\newcommand\ea{\end{array}}
%\newcommand\mod{\mbox{mod\ }}
\newcommand\ie{{\it i.e.\ }}

% General Symbols

\def\notdiv{\ \mathbin{\mkern-8mu|\!\!\!\smallsetminus}}
\newcommand{\done}{\Box} %use in linux
\newcommand{\tbf}[1]{\textbf{#1}}

%Blackboard Letters

\newcommand{\Pone}{\ensuremath{\mathbb{P}^1}}
\newcommand{\Pen}{\ensuremath{\mathbb{P}^n}}
\newcommand{\Aone}{\ensuremath{\mathbb{A}^1}}
\newcommand{\Aen}{\ensuremath{\mathbb{A}^n}}
\newcommand{\R}{\ensuremath{\mathbb{R}}}
\newcommand{\C}{\ensuremath{\mathbb{C}}}
\newcommand{\Z}{\ensuremath{\mathbb{Z}}}
\newcommand{\Q}{\mathbb{Q}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\F}{\mathbb{F}}
\newcommand{\W}{\mathbb{W}}
\newcommand{\Qoft}{\mathbb{Q}(t)}  %use in linux

% Finite Fields and Groups

\newcommand{\Fp}{ \F_p }
\newcommand{\Fpf}{ \Fp^{*} }
\newcommand{\ZnZ}{ \Z / n\Z}
\newcommand{\ZnZf}{ (\Z / n\Z)^{*}}
\newcommand{\ZqZ}{ \Z / q\Z}
\newcommand{\ZqZf}{ (\Z / q\Z)^{*}}

% Fractions

\newcommand{\fof}{\frac{1}{4}}  %oneforth
\newcommand{\foh}{\frac{1}{2}}  %onehalf
\newcommand{\fot}{\frac{1}{3}}  %onethird
\newcommand{\fop}{\frac{1}{\pi}}    %1/pi
\newcommand{\ftp}{\frac{2}{\pi}}    %2/pi
\newcommand{\fotp}{\frac{1}{2 \pi}} %1/2pi
\newcommand{\fotpi}{\frac{1}{2 \pi i}}

% Legendre Symbols

\newcommand{\jsi}[1]{ { \underline{#1} \choose p_i} }
\newcommand{\jso}[1]{ { \underline{#1} \choose p_1} }
\newcommand{\jst}[1]{ { \underline{#1} \choose p_2} }
\newcommand{\jsthree}[1]{ { \underline{#1} \choose 3} }
\newcommand\lag[2]{\ensuremath{\left(\frac{#1}{#2}\right)}}
\newcommand{\jsq}[1]{ { \underline{#1} \choose q} }            %(*/q)
\newcommand{\jsn}[1]{ { \underline{#1} \choose n} }            %(*/q)
\newcommand{\js}[1]{ { \underline{#1} \choose p} }


% Theorem / Lemmas et cetera

\newtheorem{thm}{Theorem}[section]
\newtheorem{conj}[thm]{Conjecture}
\newtheorem{cor}[thm]{Corollary}
\newtheorem{lem}[thm]{Lemma}
\newtheorem{prop}[thm]{Proposition}
\newtheorem{exa}[thm]{Example}
\newtheorem{defi}[thm]{Definition}
\newtheorem{exe}[thm]{Exercise}
\newtheorem{rek}[thm]{Remark}
\newtheorem{cla}[thm]{Claim}
\newtheorem{que}[thm]{Question}

\newcommand{\ncr}[2]{{#1 \choose #2}}
\newcommand{\twocase}[5]{#1 \begin{cases} #2 & \text{#3}\\ #4
&\text{#5} \end{cases}   }

\newcommand{\var}{\text{Var}}
%%% Dirac notation
\newcommand{\bra}[1]{\left\langle #1\right|}
\newcommand{\ket}[1]{\left|#1\right\rangle}
\newcommand{\braket}[2]{\left\langle #1 \left|#2\right.\right\rangle}
\newcommand{\braOket}[3]{\left\langle #1\left|#2\right|#3\right\rangle}

% matrices
% 2x2 symm
\newcommand{\sym}[3]{\ensuremath{\left(\begin{array}{ll}#1 &#2 \\ #2 &#3%
\end{array}\right)}}
\newcommand{\mat}[4]{\ensuremath{\left(\begin{array}{ll}#1 &#2 \\ #3 &#4%
\end{array}\right)}}
\newcommand{\half}{\mbox{\small $\frac{1}{2}$}}
\newcommand{\sfrac}[1]{\mbox{\small $\frac{1}{#1}$}}

\newcommand{\mbf}[1]{{\mathbf #1}}
\newcommand{\pd}[2]{\frac{\partial #1}{\partial #2}}

\newcommand{\infint}{\int_{-\infty}^{\infty} \!\!}  % infinite integral

\newcommand{\pnpo}{p_{n+1}}
\newcommand{\pmpo}{p_{m+1}}
%\newcommand{\pm}{p_m}
\newcommand{\pn}{p_n}
\newcommand{\pmmo}{p_{m-1}}
\newcommand{\pnmo}{p_{n-1}}
\newcommand{\pmmt}{p_{m-2}}
\newcommand{\pnmt}{p_{n-2}}

\newcommand{\qmpo}{q_{m+1}}
\newcommand{\qnpo}{q_{n+1}}
\newcommand{\qm}{q_m}
\newcommand{\qn}{q_n}
\newcommand{\qmmo}{q_{m-1}}
\newcommand{\qnmo}{q_{n-1}}
\newcommand{\qmmt}{q_{m-2}}
\newcommand{\qnmt}{q_{n-2}}


\newcommand{\mattwo}[4]
{\left(\begin{array}{cc}
                        #1  & #2   \\
                        #3 &  #4
                          \end{array}\right) }

\newcommand{\matthree}[9]
{\left(\begin{array}{ccc}
                        #1  & #2 & #3  \\
                        #4 &  #5 & #6 \\
                        #7 &  #8 & #9
                          \end{array}\right) }

\newcommand{\dettwo}[4]
{\left|\begin{array}{cc}
                        #1  & #2   \\
                        #3 &  #4
                          \end{array}\right| }

\newcommand{\detthree}[9]
{\left|\begin{array}{ccc}
                        #1  & #2 & #3  \\
                        #4 &  #5 & #6 \\
                        #7 &  #8 & #9
                          \end{array}\right| }
\newcommand{\gl}{\lambda}


\title{Honors $1$: Undergraduate Math Lab\thanks{Homepage: \texttt{http://www.math.nyu.edu/$\sim$millerj/}} }

\author{Peter Sarnak\thanks{E-mail: \texttt{sarnak@math.princeton.edu}}, Steven J. Miller\thanks{E-mail:
\texttt{millerj@cims.nyu.edu or sjmiller@math.princeton.edu}},
Alex Barnett\thanks{E-mail: \texttt{barnett@nmr.mgh.harvard.edu}}
\\
\\ $$ $$ Courant Institute of Mathematical Sciences \\
 New York University \\
 New York, NY\\  }


\date{\today}



\begin{document}

\maketitle

\begin{abstract}
The purpose of the Undergraduate Mathematics Laboratory is to form
a research team of undergraduates, graduate students and faculty
to investigate interesting unsolved conjectures theoretically and
experimentally. The UML is sponsored by a VIGRE grant of the
National Science Foundation.

In addition to the standard lecture-homework classes, we wanted a
class where the undergraduates would work on hot conjectures and
see what kinds of problems mathematicians study. In the sciences
and engineering, undergraduates are often exposed to state of the
art projects through experimental labs; we wanted to bring a
similar experience to the math majors.

The undergrads often have enough theory to understand the basic
framework and proofs of simple cases. Building on this, they then
numerically test the conjectures. The undergrads learn a good deal
of theory, they learn about coding, simulations and optimization
(very marketable skills), and they get to see what is out there.
The graduate students and the faculty get a potent calculating
force for numerical investigations. A similar course has been run
at Princeton ($2000-2002$). Many of the problems investigated by
the Princeton students arose from graduate dissertations or
current faculty research. It has been very easy finding graduate
students and faculty excited about working on this course; at the
end of a semester or year, instead of having a folder of solution
keys to calculus, a graduate student should be able to co-author
an experimental math paper with the undergrads.

Below are the notes from the NYU Fall $2002$ class.

\newpage

\begin{center} \tbf{Problem List} \end{center}

\ben

\item \tbf{Primality Testing:}

In a major theoretical breakthrough, Manindra Agarwal, Nitin
Saxena and Neeraj Kayal discovered a deterministic polynomial time
algorithm to determine if a number is prime or composite. Previous
algorithms are known to be polynomial only under well believed
conjectures (GRH), or are probabalistic. Some aspects of current
primality testing algorithms will be explored, possibly the
distribution of the least primitive root mod $p$.

\item \tbf{Ramanujan Graphs:}

The construction of graphs that are highly connected but have few
edges have many important applications, especially in building
networks. To each graph $G$ we can associate a matrix $A$, where
$A_{ij}$ is the number of edges from vertex $i$ to vertex $j$.
Many properties of $G$ are controlled by the size of the second
largest eigenvalue of $G$. One project will be to investigate the
distribution of the normalized second largest eigenvalues.

\item \tbf{Randomness of Arithmetic Maps:}

For a prime $p$, consider the map $Inv_p$ which sends $x$ to its
inverse mod $p$, $x \mapsto \overline{x}$. One project will be to
compare this map to random maps mod $p$. For example, let $L(p)$
be the length of the longest increasing subsequence. If $p$ is
congruent to $3$ mod $4$, the inverse map is a fixed-point-free
signed involution, and the length of $L(p)$ can be compared to
that from random fixed-point-free signed involutions (studied by
Rains and others). Let $S(m,n;c)$ be the Kloosterman sum,

\be S(m,n;c) \ = \ \sum_{x mod p} e^{ 2\pi i \frac{m x  +  n
\overline{x}}{c} } \ee

An additional project will be to investigate $\sum_c
\frac{|S(1,1;c)|^2}{ c^2}$, which is related to number variance of
the Upper Half Plane mod $\text{SL}(2,\Z)$. Finally, let $\sqrt{p}
\le x \le 2\sqrt{p}$. Arrange in increasing order the $\sqrt{p}$
numbers $\overline{x}$, and compare their spacings to Poissonian
behavior.

\een

\end{abstract}



\tableofcontents

\chapter{Introduction, Primality Testing, Algebra Review}

We introduce basic number theory concepts and primality
algorithms. Lecture by Peter Sarnak; notes by Steven J. Miller.

\section{Primality Testing}

Given $n$, is $n$ prime or composite? How difficult is this? How
long does it take? Brute force: try factors up to $\sqrt{n}$, so
can do in $\sqrt{n}$ steps.

$P = NP$ problem. Deep central problem in theoretical Computer
Science. $P$ is problems solvable in polynomial number of steps
(in terms of input); if equals $NP$, a lot of problems are
solvable quickly.

Telling when $n$ is prime: isn't supposed to be hard, but until
two weeks ago, wasn't known to be a $P$ problem.

\textbf{Notation:} $A(x) = O\Big( B(x) \Big)$ if there exists a $C
> 0$ (which can be computed; if it cannot be computed, we say so)
such that $|A(x)| \le C B(x)$.

Example: One could show every sufficiently large odd number is the
sum of three primes. However, we didn't know how large
sufficiently large was! IE, we couldn't go through the calculation
and make explicit a number $N_0$ such that if $n$ is odd and
greater than $N_0$, then $n$ is the sum of three primes. (Note:
this has been removed, and we know have another proof giving an
explicit $N_0$).

\begin{thm}[Agrawal, Kayal, Saxena]There is a procedure which runs
in at most $O\Big(\log^{12} n \Big)$ steps determines whether $n$
is prime. (Might be a little more than $\log^{12} n$, ie, might be
something like $\log^{12} n (\log \log n)^A$).
\end{thm}

There were algorithms that were known and faster, but only known
to work all the time assuming certain well believed hypotheses
(Riemann Hypothesis, RH).

(Go to
http://www.math.nyu.edu/$\sim$millerj/problemlist/problems.htm for
a copy of their paper).

Need a feel for numbers. When $n$ is big, $\log n$ (to any power)
is much less than $n$. For practical applications, the size of the
constant is important, as a constant of size $10^{100}$ would make
an algorithm useless for our real world applications (ie, for the
ranges we can reach). In AKS, the constants are tractable.

\textbf{Technical Point:} In AKS, they quote a theorem from number
theory (they treat this as a black box: someone from this class
will hopefully investigate this result further). \emph{There are
many primes $p$ for which $p-1$ has a large prime factor $q$, $q >
p^{\frac{2}{3}}$}. Related to Sophie Germain primes: primes $p$
where $p-1 = 2q$, $q$ prime. (She showed that for primes like
this, $x^p + y^p = z^p$, you can solve Fermat's Last Theorem for
such primes. It is not known if there are infinitely many primes
like this). AKS does not need to know there are infinitely many
Sophie Germain primes; fortunately all they need is that there are
sufficiently many primes $p$ with $p-1$ with large prime factors.

Similar to Twin Primes: primes $p_1$, $p_2$ with $p_1 - p_2 = 2$.
We don't know if there are infinitely many twin primes, but we do
have heuristics (Hardy-Littlewood) predicting how many twin primes
there are (and we observe exactly that many). Sophie Germain
primes are more subtle, but should be able to get heuristics. For
twin primes and related quantities, see David Schmidt's report on
Prime Investigations (Princeton Undergraduate Math Lab,
$2000-2001$). Anyway, this would be a good project.

\section{Arithmetic Modulo $p$}

Number Theory: the study of whole numbers. $\Z$ is the integers,
look at $\Z / n\Z$ $= \{0,1,2,\dots, n-1\}$. This is a finite
group (under addition); in fact, it is a finite ring (can also
multiply, have inverses for the non-zero elements only if $n$ is
prime).

Notation: $x \equiv y$ mod $n$ means $x-y$ is a multiple of $n$.

Try and solve in $\Z$ the equation $2x + 1 = 2y$. The left hand
side is odd, the right hand side is even. Thus, there are no
solutions. Really, just did arithmetic mod $2$ or in $\Z/ 2\Z$.
Harder: $x^2 + y^2 + z^2 = 8n + 7$. This never has a solution.
Look modulo $8$. The RHS is $7$ modulo $8$. What are the squares
mod $8$? $1^1 = 1, 2^2 = 4, 3^2 = 1$, $4^2 = 0$, repeats. See
there is no way to add three squares and get $7$.

Idea: First, try and solve the equation modulo different primes.
If you cannot solve it for some prime, then you cannot solve it
over the integers.

\subsection{Algebra Review}

$\Z / n\Z$: do arithmetic over this ring. $\Big(\Z / n\Z
\Big)^{*}$ are the invertible (multiplicatively) elements in the
ring $\Z / n\Z$, ie, $x$ is in $\Big(\Z / n\Z \Big)^{*}$ if there
is a $y$ such that $xy \equiv 1$ mod $n$. Note: if $\gcd(x,n) > 1$
(ie, $x$ and $n$ have a common prime divisor $p$), then you cannot
invert $x$ (there is no $y$ with $xy \equiv 1$ mod $n$). Why? $xy
\equiv 1$ mod $n$ means $xy = 1 + \lambda n$ for some integer
$\lambda$. But if $p|x$ and $p|n$, then $p|1$ which is absurd.
Exercise: if $\gcd(x,n) = 1$, there is an inverse (Euclidean
Algorithm).

The cardinality (number of elements in the set) of $\Big(\Z / n\Z
\Big)^{*}$ is the number of $x \in \{0,1,2,\dots, n-1\}$ such that
$\gcd(x,n) = 1$. We denote the number of such $x$ by $\phi(x)$,
the Euler totient function. (Good Reference: H. Davenport: The
Higher Arithmetic). Note that if $p$ is prime, $\phi(p) = p-1$.
This implies that $\Big| \Big(\Z / p\Z \Big)^{*}\Big| = p-1$ $=
\Z/ p\Z - \{0\}$. IE, we have a field if $n$ is a prime, as every
non-zero element is invertible.

$\Big(\Z / n\Z \Big)^{*}$, for any $n$, is a finite Abelian group
(we have inverses under multiplication, and order of
multiplication doesn't matter). Finite Abelian Groups is a trivial
subject: Structure Theorem for Finite Abelian Groups: product of
cyclic groups.

For $n = p$ a prime, $\Big(\Z / n\Z \Big)^{*}$ is a cyclic group
of order $p-1$.

If $G$ is a group (have identity, closed under some binary
operation, have inverses with respect to the binary operation,
operation is associative), we say the order of $x \in G$,
ord($x$), is the least positive power $m$ such that $x^m = e$,
where $e \in G$ is the identity of the group. In a finite group,
every element has finite order (proof: use the pidgeonhole
principle).

\begin{thm}[Lagrange]ord($x$) $|$ ord($G$).
\end{thm}

\begin{cor}[Fermat's Little Theorem]For any prime $p$, if
$\gcd(a,p) = 1$, then $a^{p-1} \equiv 1$ mod $p$.
\end{cor}

\subsection{Using Fermat's Little Theorem}

To check and see if a number $n$ is prime, why not check if
$a^{n-1} \equiv 1$ mod $n$ if $\gcd(a,n) = 1$. It is very easy to
quickly compute $\gcd(a,n)$ (use Euclid's Algorithm). Sketch:
without loss of generality, let $a < n$. Write $n = b_0 a + b_1$,
and now the gcd of $a$ and $n$ is the same as that of $a$ and
$b_1$ (note $0 \le b_1 < a$).

How long does it take to raise $a$ to the $n-1$ power? Use
repeated squares and base $2$ expansion. Example $100 = 64 + 32 +
4$, or $1 \cdot 2^6 + 1\cdot 2^5 + 1\cdot 2^2$. Thus, do $a^2$,
$a^2 \cdot a^2$, $a^4 \cdot a^4$, $a^8 \cdot a^8$, $a^{16} \cdot
a^{16}$, $a^{32} \cdot a^{32}$. Then $a^{100} = a^{64} \cdot
a^{32} \cdot a^{4}$.

If, by choosing an $a$, you find $a^{n-1} \not\equiv 1$ mod $n$,
you have a certificate for compositeness, but you have no idea
what the factors of $n$ are!

When a machine uses Fermat's Little Theorem, it randomly chooses a
fixed number of $a$'s between $1$ and $n-1$. Problems: how many
times should you run these tests to be very confident of the
result; are there any numbers which always pass this test, yet are
composite?

About eight years ago it was proved there are infinitely many
Carmichael numbers (numbers $n$ such that $a^{n-1} \equiv 1$ mod
$n$ for all $a$, but $n$ is composite).

\subsection{Quadratic Reciprocity}

$p, q$ odd primes. We define $\js{a}$ to be $1$ if $a$ is a
non-zero square mod $p$, $0$ if $a = 0$, and $-1$ otherwise (ie,
if $a$ is not a square mod $p$). Note $a$ is a square mod $p$ if
there exists an $x \in \{0,1, \dots, p-1\}$ such that $a \equiv
x^2$ mod $p$. For $p$ an odd prime, half the non-zero numbers are
squares, half are not.

\textbf{Exercise:} $\js{a} = a^{\frac{p-1}{2}}$ mod $p$ for odd
$p$. Note the above squared is $a^{p-1} \equiv 1$.

\begin{thm}[Quadratic Reciprocity]$\js{q} = \jsq{p} \cdot (-1)^{\frac{p-1}{2}
\frac{q-1}{2}}$, $p$, $q$ odd primes.
\end{thm}

Gauss gave at least four proofs of this deep result. If either $p$
or $q$ is equivalent to $1$ mod $4$, then one has $\js{q} =
\jsq{p}$, ie, I'm a square root modulo you if you are a square
root modulo me.

Carmichael numbers were behaving like primes. We want to get rid
of them. Instead of testing $a^{n-1} \equiv 1$ mod $n$, test and
see if $a^{\frac{n-1}{2}} \equiv \jsn{a}$ mod $n$. Similar to the
Euclidean Algorithm, can computer $\jsn{a}$ in $log n$ steps by
constant applications of Quadratic Reciprocity.

\textbf{Key Test:} Will test that $a^{\frac{n-1}{2}} \equiv
\jsn{a}$ mod $n$ for $1 \le a \le C \log^2 n$. If fails for some
$a$, the number is composite. If it passes, by the Riemann
Hypothesis (RH), then it is true for all $a$. Will then show this
is a valid test for primality (ie, unlike Carmichael numbers, if
this is satisfied for all $a$ up to $C \log^2 n$, then $n$ is
prime. This will take about $\log^4 n$ steps).

Algebra Books: Herstein (Topics in Algebra); Birkhoff-Mclean
(Algebra), Lang (Undergraduate Algebra).

\section{Lecture Next Week}

Steve will talk about reciprocity, finite fields.

\section{WWW Resources}

http://mathworld.wolfram.com/ is a good place to look up
unfamiliar terms.



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\chapter{Notation, Euclid's Algorithm, Lagrange's Theorem, Riemann Zeta Function}

We will review notation, Euclid's Algorithm, Lagrange's Theorem,
and prove there are infinitely many primes (three ways: following
Euclid, by studying the Riemann Zeta Function $\zeta(s)$ as $s
\rightarrow 1$, and by analyzing $\zeta(2)$. Lecture by Steven J.
Miller; notes by Alex Barnett and Steven J. Miller.

\section{Notation}

\bi
\item $a|b$ : $a$ divides $b$, \ie the remainder after integer division $\frac{b}{a}$
is 0.

\item $(a,b)$ : Greatest Common Divisor (GCD) of $a$ and $b$.
Sometimes written $\gcd(a,b)$.

\item $x \equiv y (\mod n)$ : An equality once
both sides of the equation have been taken modulo $n$.
Equivalently, there exists an integer $a$ such that $x = y + an$.


\item wlog : `without loss of generality'. For example, if we have
two numbers $x$ and $y$, it is often convenient to know which is
larger and which is smaller. Without loss of generality, we can
say $x \le y$, as the case $x \ge y$ is handled identically (after
permuting the variables).


\item \st : such that.

\item $\forall$ : for all.

\item $\exists$ : there exists.

\item big O notation :
$A(x) = O(B(x))$, read ``$A(x)$ is of order $B(x)$'', is shorthand
for, there is a $C>0$ (which we can explicitly calculate), \st
$|A(x)| \le C\, B(x)$, $\forall x$.

%tilde : $x ~ y$ is another notation for $x = O(y)$.

\item $|S|$ or $\# S$ : number of elements in the set $S$.

\item $\#\{\mbox{\em condition}\}$ :
number of objects satisfying the {\em condition}.

\item $p$ : unless otherwise stated, a prime number.

\item $\Z$ : the set of integers.

\item $\Z / n\Z$ : the additive group of integers mod $n$.

\item $(\Z / n\Z)^{*}$ : the multiplicative group of invertible
elements mod $n$.

\item $\Q$: the set of rational numbers. $\Q = \{x: x =
\frac{p}{q}, p,q \in \Z, q \neq 0\}$.

\item $\R$: the set of real numbers.

\item $\C$: the set of complex numbers.

\item \lag{a}{p} : Legendre symbol of $a$ and $p$, defined as
\be \lag{a}{p} = \left\{ \ba{ll}
0, \hspace{1in} & \mbox{if } p|a, \mbox{that is, } a = 0 (\mod p) \\
1, & \mbox{if } \exists x \;\st x^2 = a (\mod p) \\
-1, & \mbox{if the above does not exist}. \ea\right. \ee The
symbol tests the question, ``Does $a$ have a square root in the
{\em field} of arithmetic modulo $p$?''

\item `weak bound' :  an inequality constraining some quantity which
does a very poor job of getting close to the true size of the
quantity. That is, a not very useful bound.

\ei

\section{Euclid's algorithm for GCD}

Tells you if two positive integers $x$ and $y$ have a GCD greater
than 1 by finding it. Therefore it's a `constructive proof'. It is
also `deterministic' (involves no random choices). A fast
procedure, \ie takes only O(log $y$) steps. Remember that the
number of digits $\propto \log y$. wlog we take $y>x$.

Each step is the `black box' integer division routine we'll call
$D$, which given the pair $x,y$ returns the pair of integers $b,r$
\st $r<x$ and satisfying \be y = bx + r . \ee Note that this step
is polynomial in the number of digits (probably $\sim (\log y)^2$
--- anyone?).

\vspace{0.05in}

\noindent {\small ALGORITHM:}

\bc \framebox{\parbox{5in}{\parindent=0in Start with the pair
$y,x$ and perform $D$ to get $b_1, r_1$.

Perform $D$ on $x,r_1$ to get $b_2,r_2$.

Perform $D$ on $r_1,r_2$ to get $b_3,r_3$.

\hspace{2in}\vdots

Perform $D$ on $r_{n-2},r_{n-1}$ to get $b_n, r_n$.

\vspace{0.1in}

Stop when $r_n$ is either \bi
\item 0, in which case $r_{n-1}$ is the GCD, or
\item 1, in which case the GCD is 1, that is, $x$ and $y$ are
relatively prime. \ei }} \ec

The procedure works because the $D$ step gives an $r$ which
inherits {\em all} common divisors of $y$ and $x$. This is easy to
see by writing $D$ as $r = y - bx$. Therefore all the adjacent
pairs in the sequence $y,x,r_1,r_2\cdots r_{n-1}$ share the same
GCDs. The sequence is also descending $y>x>r_1>r_2>\cdots >
r_{n-1}$, so must reach the case that $r_{n-1}|r_{n-2}$, in which
case $r_{n-1}$ is the GCD and $r_n = 0$, or that a remainder of 1
is reached, which implies no common divisors. We have a worst-case
scenario that each remainder is smaller than the previous by a
constant factor $c<1$ (I believe this is the inverse of the Golden
Ratio $(\sqrt{5}-1)/2 \approx 0.618\cdots$ --- anyone?), giving
geometric (exponential) shrinkage of the $r$'s. Therefore the
worst case is that the answer is reached in $n = O(\log \; y)$
steps, and the whole algorithm is therefore polynomial in $\log
y$.


\section{Lagrange's Theorem}

\subsection{Basic group theory}

Group $G$ is a set of elements $g_i$ satisfying the four
conditions below, relative to some binary operation. We often use
multiplicative notation ($g_1 g_2$) or additive notation ($g_1 +
g_2$) to represent the binary operation. For definiteness, we use
multiplicative notation below; however, one could replace $xy$
with $b(x,y)$ below.

If the elements of $G$ satisfy the following four properties, then
$G$ is a group.

\ben
\item $\exists e \in G \;\st \forall g \in G : eg = ge = g$. (Identity.)
We often write $e=1$ for multiplicative groups, and $e = 0$ for
additive groups.

\item $\forall x,y,z \in G$ : $(xy)z = x(yz)$. (Associativity.)

\item $\forall x \in G, \exists y \in G \;\st xy = yx = e$. (Inverse.)
We write $y = x^{-1}$ for multiplication, $y = -x$ for addition.

\item $\forall x,y \in G : xy \in G$. (Closure.)
\een

If commutation holds ($\forall x, y \in G$, $xy = yx$), we say the
group is Abelian. Non-abelian groups exist and are important. For
example, consider the group of $N \times N$ matrices with real
entries and non-zero determinant. Prove this is a group under
matrix multiplication, and show this group is not commutative.

$H$ is a {\em subgroup} of $G$ if it is a group and its elements
form a subset of those of $G$. The identity of $H$ is the same as
the identity of $G$. Once you've shown the elements of $H$ are
closed (ie, under the binary operation, $b(x,y) \in H$ if $x, y
\in H$), then associativity in $H$ follows from closure in $H$ and
associativity in $G$.

For the application to Fermat's Little Theorem you will need to
know that the set $\{1,x,x^2,\cdots\,x^{n-1}\}$ where $n$ is the
lowest positive integer \st $x^n = 1$, called the {\em cyclic
group}, is indeed a subgroup of any group $G$ containing $x$, as
well as $n$ divides the order of $G$.

For a nice introduction to group theory see: M. Tinkham, {\em
Group Theory and Quantum Mechanics}, (McGraw-Hill, 1964) or S.
Lang, {\em Undergraduate Algebra}.


\subsection{Lagrange's Theorem}

The theorem states that if $H$ is a subgroup of $G$ then $|H|$
divides $|G|$.

First show that the set $hH$, \ie all the elements of $H$
premultiplied by one element, is just $H$ rearranged (Cayley's
theorem). By closure $hH$ falls within $H$. We only need to show
that $h h_i$ can never equal $h h_j$ for two different elements
$i\ne j$. If it were true, since a unique $h^{-1}$ exists we could
premultiply the equation $h h_i = h h_j$ by $h^{-1}$ to give $h_i
= h_j$, which is false. Therefore $h h_i \ne h h_j$, and we have
guaranteed a 1-to-1 mapping from $H$ to $hH$, so $hH = H$.

Next we show that the sets $g_i H$ and $g_j H$ must either be
completely disjoint, or identical. Assume there is some element in
both. Then $g_i h_1 = g_j h_2$. Multiplying on the right by
$h_i^{-1} \in H$ (since $H$ is a subgroup) gives $g_i = g_j h_2
h_1^{-1}$. As $H$ is a subgroup, $\exists h_3 \in H$ such that $h
= h_2 h_1^{-1}$. Thus $g_i = g_j h_3$. Therefore, as $h_3 H = H$,
$g_i H = g_j h_3 H = g_j H$, and we see if the two sets have one
element in common, they are identical. We call a set $gH$ a
\emph{coset} (actually, a left coset) of $H$.

Clearly

\be G = \bigcup_{g \in G} g H \ee

Why do we have an equality? As $g \in G$ and $H \subset G$, every
set on the right is contained in $G$. Further, as $e \in H$, given
$g \in G$, $g \in gH$. Thus, $G$ is a subset of the right side,
proving equality.

There are only finitely many elements in $G$. As we go through all
$g$ in $G$, we see if the set $gH$ equals one of the sets already
in our list (recall we've shown two cosets are either identical or
disjoint). If the set equals something already on our list, we do
not include it; if it is new, we do. Continuing this process, we
obtain

\be G = \bigcup_{i = 1}^k g_i H \ee

for some finite $k$. If $H = \{e\}$, $k$ is the number of elements
of $G$; in general, however, $k$ will be smaller.

Each set $g_i H$ has $|H|$ elements. Thus, $|G| = k|H|$, proving
$|H|$ divides $|G|$.


\section{Introduction to Riemann zeta function}


\subsection{Prelude: Euclid's proof of infinity of primes}

Given the set of primes $p_1\cdots p_n$ you can always construct
the number $\prod_{i=1}^n p_i + 1$ which is indivisible by any of
the given $p_1\cdots p_n$. Therefore this number must be divisible
only by primes greater than $p_n$, or must be prime itself.
Therefore there exists a prime greater than $p_n$. An analysis of
this proof gives a very weak lower bound on the number of primes
less than $x$. The worst case scenario is that $\prod_{i=1}^n p_i
+ 1$ is the next prime. Thus, if we had $n-1$ primes up to $x =
\prod_{i=1}^{n-1} p_i + 1$, we would have $n$ primes up to

\be \prod_{i=1}^{n-1} p_i \cdot \Big(\prod_{j=1}^n p_j + 1\Big) +
1 \ee

Thus, having at least $n-1$ primes less than $x$, we have at least
$n$ primes less than (basically) $x^2$. One can quantify this
further. One should get something like there are at least $n$
primes less than $2^n$ or $4^n$.

The zeta function will give us other ways to prove this, and to
get a better estimate on the {\em prime counting function}, \be
    \pi (x) \; \equiv \; \#\{p<x\} ,
\ee giving the number of primes below any number $x$.


\subsection{Definition, two forms}

The Riemann zeta function $\zeta(s)$ is defined, for Re($s) > 1$,
by

\be \zeta(s) = \sum_{n=1}^\infty \frac{1}{n^s}. \ee

We prove the useful fact that, for Re($s) > 1$,

\be \sum_{n=1}^\infty\frac{1}{n^s} \; = \; \zeta(s) \; = \;
\prod_{\stackrel{\mbox{\tiny primes}}{p}} \left( 1 - \frac{1}{p^s}
\right)^{-1}, \ee which we call LHS and RHS. We call the product
over primes an Euler Product.

To show equivalence, we use the Fundamental Theorem of Algebra
(FTA) that all positive integers can be expressed as a single,
unique, product of prime factors. Expanding all reciprocals in the
RHS using the geometric series sum formula $(1-x)^{-1} = 1 + x +
x^2 + x^3 + \cdots$, gives for the RHS, \bea (1 + 2^{-s} + 2^{-2s}
+ 2^{-3s} + \cdots) (1 + 3^{-s} + \cdots )(1 + 5^{-s} + \cdots)
\cdots. \nonumber \eea Remarkably, due to the FTA, we can
associate 1-to-1 each term (choice of prime factors) on the RHS
with each $n$ on the LHS. For instance, $n = 12$ from LHS is
accounted for by the RHS term, \bea
2^{-2s}\cdot3^{-s}\cdot1\cdot1\cdot1\cdots = \frac{1}{(2^2 \cdot
3)^s}
 = \frac{1}{12}.
\nonumber \eea Each combination of RHS terms corresponds uniquely
to a single $n$.

\subsection{$\zeta(s)$'s Behaviour and the Infinitude of Primes}

We take the limit of $s$ going to 1 and compare sides. LHS gives
\be \lim_{s\rightarrow1} \zeta(s) \; = \; \lim_{s\rightarrow1}
\sum_{n=1}^\infty \frac{1}{n^s} = \sum_{n=1}^\infty \frac{1}{n}.
\ee

This sum diverges. Why? Crudely, $\sum_{n=1}^N n^{-1}$ is close to
$\int_1^N \frac{dy}{y}$ which equals $\log N$. The definition of
`close' can be tightened up. For instance, you can create upper
and lower bounds by approximating the integral by rectangular
strips, getting

\be \sum_{n=2}^N \frac{1}{n} \le \int_1^N \frac{dy}{y} \le
\sum_{n=1}^{N-1} \frac{1}{n}. \ee

As the two sums differ by a bounded amount, we see the sum grows
like $\log N$.

As $s$ goes to $1$, if there are only finitely many primes than
the product over primes is well behaved (ie, finite). Therefore,
there must be infinitely many primes!

Further study of the zeta function will lead us to a good estimate
for $\pi(x)$.

A second proof follows from the fact that $\zeta(2m) = $ rational
$ \cdot \pi^{2m}$, for integer $m$. This is known as a
\emph{Special Values} proof, as we are using the value of
$\zeta(s)$ at a special value. We need the fact that $\pi^2$ is
irrational. $\zeta(2) = \sum_{n=1}^\infty \frac{1}{n^2}$ $=
\frac{\pi^2}{6}$, which is irrational. Thus, the right hand side
(the product over primes) must also be irrational; however, if
there are only finitely many primes, when $s = 2$ the right hand
side is rational! Thus, there must be infinitely many primes.


Please see Steve's notes, and URLs for more information on all of
the above.



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\chapter{Legendre Symbols, Gary Miller's Primality Test and GRH}

We review the Legendre Symbol. We discuss Gary Miller's primality
test, and show that if the General Riemann Hypothesis (for
Dirichlet Characters) is true, then Miller's test correctly
determines if a number $n$ is prime or composite, and runs in time
$O(\log^4 n$). Lecture by Peter Sarnak; notes by Steven J. Miller.

\section{Review of the Legendre Symbol}

Recall Fermat's Little Theorem: $a^{p-1} \equiv 1$ mod $p$ if $p$
is prime.

Given $n$, check $a^{n-1} \equiv 1$ mod $n$ for many $a$'s
relatively prime to $n$. \textbf{Exercise:} If ever this is not
satisfied, then $n$ \emph{must} be composite.

There are composite numbers (called Carmichael numbers) which
satisfy $a^{n-1} \equiv 1$ mod $n$ for all $a$, yet are not prime.
The first Carmichael number is $561 = 3 \cdot 11 \cdot 17$; the
third is $1729 = 7 \cdot 13 \cdot 19$. \textbf{Exercise:} Prove
all Carmichael numbers \emph{must} be square-free.

Aside: $1729$ has an interesting history (Ramanujan, Hardy and
taxicabs). Hardy visited Ramanujan in the hospital, Hardy remarked
that his taxicab's number was particularly uninteresting;
Ramanujan remarks it ($1729$) is the smallest number which can be
written in two different ways as the sum of two cubes. $1729 = 1^3
+ 12^3 = 9^3 + 10^3$.

Recall the \textbf{Legendre symbol} $\js{a}$ is $0$ if $p|a$, $1$
if there is an $x \neq 0$ with $x^2 \equiv a$ mod $p$, and $-1$ if
there is no solution to $x^2 \equiv a$ mod $p$. \textbf{Euler's
condition} is $\js{a} \equiv a^{\frac{p-1}{2}}$ mod $p$.

Really, the Legendre symbol is a function on $\Fp = \Z / p\Z$. We
can extend the Legendre symbol to all integers. We only need to
know $a$ mod $p$, and we define $\js{a} = \js{a \ \mbox{mod} \
p}$.

Initially the Legendre symbol is define only when the bottom is
prime. We now extend the definition to all $n$ as follows: let $n
= p_1 \cdot p_2 \cdots p_t$ be the product of $t$ distinct primes.
Then $\lag{a}{n} = \lag{a}{p_1} \lag{a}{p_2} \cdots \lag{a}{p_t}$.
Note this is \emph{not} the same as saying that if $a$ is a square
(a quadratic residue) mod $n$, then $a$ is a square mod $p_i$ for
each prime divisor.

The main result (which allows us to calculate the Legendre symbol
quickly and efficiently) is the celebrated

\begin{thm}[Quadratic Reciprocity] For $m$, $n$ odd and relatively
prime, $\lag{m}{n} \lag{n}{m} =
(-1)^{\frac{m-1}{2}\frac{n-1}{2}}$.
\end{thm}

\section{Gary Miller's Primality Test, $1976$}

\textbf{Miller Test:} \emph{Given $n$ as input ($n$ must be odd),
test whether for $2 \le a \le 70 \log^2 n$, $\lag{a}{n} \equiv
a^{\frac{n-1}{2}}$ mod $n$ (where, of course, $a$ and $n$ are
relatively prime). If this test fails for some $a$ in this range,
output \emph{composite}; if it passes the test for all such $a$,
output \emph{Prime}.}

Note that we can very quickly determine if two number are
relatively prime (use the Euclidean Algorithm, which takes $O(\log
n)$ steps).

\begin{thm}[Miller Test Results]The Miller Test runs in $O(\log^4
n)$ steps. If the output is \emph{composite}, then the number $n$
\emph{is} composite (ie, the algorithm's result is correct). If we
assume GRH (the \emph{General Riemann Hypothesis}, the most
important unsolved problem in mathematics), then the output
\emph{prime} is also correct.
\end{thm}

Running time: we can compute $\lag{a}{n}$ in $O(\log n)$ steps. By
Quadratic Reciprocity, to compute $\lag{a}{n}$ (may assume
$-\frac{n}{2} \le a \le \frac{n}{2}$), it is enough to compute
$\lag{n}{a}$ (with a factor of $-1$).

Why? We only need to know $a$ mod $n$, so we may reduce $a$ until
$-\frac{n}{2} \le a \le \frac{n}{2}$. Note the top (in absolute
value) is at most half the size of the bottom ($n$). We then use
quadratic reciprocity to evaluate $\lag{a}{n}$. Up to a factor of
$-1$, $\lag{a}{n} = \lag{n}{a}$. Thus, we may reduce $n$ mod $a$,
so that $n$ mod $a$ lies between $-\frac{a}{2}$ and $\frac{a}{2}$.
Again, the top is half the bottom, and the bottom is at most
one-quarter of what we started with ($n$).

We continue this process; we need to do such flippings at most
$\log n$ times. Why? Each time the size of the denominator is at
most half what it was before. If $2^r = n$, then $r = \log_2 n <
\log n$. Thus, after at most $\log n$ passes, the denominator
would be about $1$. So, a stage or two before would give a
denominator around $2$ or $4$. The point is, in $\log n$ steps, we
can reduce to evaluating the Legendre symbol of something where
the bottom is of bounded size. Thus, we can evaluate $\lag{a}{n}$
in $O(\log n)$ steps. (Have a lookup table for $n < 10$, et
cetera).

We need to evaluate $\lag{a}{n}$ for $C \log^2 n$ choices of $a$,
and for each choice we need to evaluate $a^{\frac{n-1}{2}}$ mod
$n$ (so we can compare it to $\lag{a}{n}$), which takes $O(\log
n)$ steps. Thus, the number of steps is $O(\log^4 n)$.

The Riemann Hypothesis plays a very important catalystic role. It
leads us to statements we feel should be true, statements which
can often be proved without the full force of Riemann.

Suppose we pass the test for all $a$ with $2 \le a \le 70 \log^2
n$ and $a$ relatively prime to $n$. Gary Miller proved that, if
GRH is true, then knowing that $n$ passes the test for all $a$ in
this little segment allows us to conclude that $n$ will pass the
test \emph{for all} $a$. Clearly, we know if $n$ passes the test
for all $a$ up to $n-1$, then we know $n$ will pass the test for
all $a$ relatively prime to $n$. The power of GRH is that we need
only check $\log^2 n$ values of $a$.

\subsection{Aside: Finite Abelian Groups}

Let $A$ be a finite Abelian Group, then $A$ is  (ie, is isomorphic
to) a product of cyclic groups.

Look at $(\Z / n\Z, +)$, the group of integers mod $n$ under clock
addition. This is a cyclic group.

The general statement is:

\begin{thm}[Structure Theorem for Finite Abelian Groups] Let $A$
be a finite Abelian Group. Then there are integers $n_1$ through
$n_t$ such that $A \approxeq \Z / n_1\Z \times \cdots \times \Z/
n_r \Z$, ie, $A$ is isomorphic to the Cartesian product of groups
of the form $\Z/m\Z$.
\end{thm}

Note $\approxeq$ means is isomorphic to; we say two groups are
\textbf{isomorphic} if there is a group homomorphism between them
which is one-to-one and onto. A \textbf{group homomorphism} $\phi$
is a map which preserves the group structure. Consider two groups
$G_1$ and $G_2$ and a map $\phi: G_1 \rightarrow G_2$. If $\phi$
is a group homomorphism, then for $x, y \in G_1$, $\phi(x+y) =
\phi(x) \bigoplus \phi(y)$, where $+$ is addition in the first
group and $\bigoplus$ is addition in the second group.

We now define the \textbf{Cartesian Product} of two groups. $X
\times Y$ is the set of pairs $(x,y)$ where $x \in X$ and $y \in
Y$. If we are writing the group action of $X$ and $Y$ additively
(say by $\bigoplus_X$ for addition in $X$ and $\bigoplus_Y$ for
addition in $Y$), then  $(x_1,y_1) \bigoplus_{X+Y} (x_2,y_2) =
(x_1 \bigoplus_X x_2, y_1 \bigoplus_Y y_2)$.

Let us consider a group written in additive notation. Recall the
\textbf{order of an element} is the number of times you must add
the element to itself to get the identity. We define the
\textbf{exponent} of a group as the least common multiple of the
orders of the elements of the group.

Consider $\Z / 2\Z \times \Z / 2\Z$, the product of two groups. In
$\Z / 2\Z \times \Z / 2\Z$, we have the pairs $(0,0)$, $(0,1)$,
$(1,0)$ and $(1,1)$. $(0,0)$ is the identity under addition; all
other elements (check) have order $2$. Thus, the exponent of $\Z /
2\Z \times \Z / 2\Z$ is $2$.

Now consider $\Z / 4Z = \{0,1,2,3\}$. $0$ is the identity, and
under addition $1$ and $3$ have order $4$, and $2$ has order $2$.
Thus, the exponent of this group is $4$. \textbf{Exercise:} using
the exponent, observe that $\Z / 2\Z \times \Z / 2\Z$ and $\Z /
4Z$ cannot be isomorphic.

\textbf{Fact:} if $p$ is prime, then $\Z / p\Z = \Fp$ is a field.
The non-zero elements have multiplicative inverses. $(\Z /
p\Z)^{*} = \Fp^{*} = \Fp - \{0\}$ (the non-zero elements under
multiplication) is a cyclic abelian group with $p-1$ elements! IE,
there is an element $g$ whose powers generate the group.
\textbf{Exercise:} Prove that if $n$ is composite, then $\ZnZ$ is
not a field. Hint: show some non-zero element has no
multiplicative inverse.

\textbf{Sketch of Proof of Fact:} Suppose $\Fpf$ is not cyclic.
Let $d$ be the exponent (the least common multiple of the orders
of the elements) of $\Fpf$. Then $d < p-1$. As the order of every
element divides $p-1$ (Lagrange's Theorem), we have $d|p-1$.

For each $x \in Fpf$, $x^d = 1$ (in the field $\Fpf$). This is
because $d$ is the least common multiple of the orders of the
elements of the group. Thus, if $\mbox{ord}(x)$ is the order of
$x$, $x^{\mbox{ord}(x)} = 1$. As $\mbox{ord}(x)|d$, we have
$\mbox{ord}(x) = kd$ for some $k \in \Z$. Thus, $x^d = x^{k
\mbox{ord}(x)}$ $= (x^{\mbox{ord}(x)})^k = 1^k = 1$.

Now use the fact that $\Fpf$ is a field. Consider $x^d - 1 = 0$
over $\Fp$ (clearly $0$ is not a root). \textbf{Over $\Fp$} means
look for solutions to this equation with $x \in \Fp$). A
polynomial of degree $d$ has at most $d$ roots (another theorem of
Lagrange). But every $x \in \Fpf = \Fp - \{0\}$ is a root, because
$d$ (the exponent of the group) is the least common multiple of
the orders of the elements. This is a contradiction: we have $p-1$
roots (every $x \in \Fpf$ solves $x^d - 1 \equiv 0$ mod $p$), but
by Lagrange there are at most $d$ roots. As $d < p-1$,
contradiction.

Steve Miller will discuss this needed theorem of Lagrange. See
also the handout from Davenport's \emph{The Higher Arithmetic}.

\subsection{Lehmer's Proposition}

\begin{prop}[Lehmer] If $\lag{a}{n} \equiv a^{\frac{n-1}{2}}$ mod
$n$ for all $a$ up to $C \log^2 n$, then $n$ is prime.
\end{prop}

We assume $n = p_1 \cdots p_t$, $t > 1$, and all primes are
distinct and odd. (When there are repeated primes, the proof is
easier, and is left as an exercise to the reader). Then $a^{n-1}
\equiv 1$ mod $n$ for $(a,n) = 1$ (ie, $a$ and $n$ relatively
prime), implies that $a^{n-1} \equiv 1$ mod $p_j$ ($p_j$ one of
the prime factors of $n$).

Thus, $n-1 \equiv 0$ mod $p_j - 1$. If there were a remainder (ie,
if $n-1$ wasn't equivalent to $0$ mod $p_j - 1$ but was equivalent
to $r_j \not\equiv 0$ mod $p_j - 1$), if we raised $a$ to this
power ($r_j$), we wouldn't get $1$ for all $a$. Just take $a$ a
generator of $\F_{p_j}^{*}$, ie, an element of maximal order $p_j
- 1$. Then $a^{n-1} \equiv a^{r_j} \neq 1$ mod $p_j$. Note that
$a^{p_j - 1} \equiv 1$ mod $p_j$, so $a^{n-1} \equiv a^{r_j}$ mod
$p_j$.

We say $p_j$ (a factor of $n$) is \textbf{of type $1$} if
$\frac{n-1}{2} \equiv 0$ mod $p_j-1$; we say $p_j$ is \textbf{of
type $2$} if $\frac{n-1}{2} \equiv \frac{p_j-1}{2}$ mod $p_j-1$.

If at least one of the $p_j$'s (without loss of generality, say
$p_1$) is of type $1$, take $a$ a quadratic non-residue mod $p_1$
and a quadratic residue for $p_2$, $p_3, \dots, p_t$. One can find
such an $a$ by the \textbf{Chinese Remainder Theorem} as the
primes are distinct. For a statement of the Chinese Remainder
Theorem, see the Appendix at the end of the notes.

We have $\lag{a}{p_1} = -1$ and $\lag{a}{p_j} = 1$ for $j > 1$. As
we are assuming $n$ is composite, there are at least two primes.

As $\lag{a}{n} = \lag{a}{p_1} \lag{a}{p_2} \cdots \lag{a}{p_t}$,
we obtain $\lag{a}{n} = -1 \cdot 1 \cdots 1$.

As we are assuming $p_1$ is of type $1$, we have $\frac{n-1}{2}
\equiv 0$ mod $p_1 - 1$. Mod $p_1 - 1$, each non-zero element in
$(\F/p_1\F)^{*}$ has order dividing $p_1 - 1$. Thus,
$a^{\frac{n-1}{2}} \equiv 1$ mod $p_1$.

We are assuming that the Miller Test is satisfied for all $a$.
Thus, $\lag{a}{n} = a^{\frac{n-1}{2}}$ mod $n$. Mod $p_1$, we have
shown the left hand side is $-1$ and the right hand side is $1$,
contradiction!

We are left with the case where all the primes are of type $2$. We
leave this as an exercise for the reader.

\section{GRH Implies Just Need To Check Up To $O(\log^2 n)$ in
Miller}

Why does GRH imply that $\lag{a}{n} \equiv a^{\frac{n-1}{2}}$ mod
$n$ for all $a$ up to $70\log^2 n$ (and relatively prime to $n$)
implies that $\lag{a}{n} \equiv a^{\frac{n-1}{2}}$ mod $n$ for all
$a$ relatively prime to $n$?

\subsection{Fourier Analysis}

Representation Theory (especially characters of Abelian Groups) is
the most important things in Mathematics. Let $A$ be a finite
Abelian Group. To each such group, we associate a dual group,
denoted $\hat{A}$, where $\hat{A}$ is the set of all homomorphisms
from $A$ into $\C^{*}$, $\C^{*}$ are the complex numbers
invertible under multiplication.

Recall $\psi$ is a \emph{group homomorphism} if $\psi(a+b) =
\psi(a)\psi(b)$. Note here $\psi:G_1 \rightarrow G_2$, and $G_1$
has been written additively and $G_2$ has been written
multiplicatively. We call such a $\psi$ a \emph{character}.


\subsection{Examples}

Let $A = \Z/ n\Z$, and $\nu \in \ZnZ$. Let $e(z) = e^{2\pi i z}$
for $z \in \C$. Define $\psi_\nu(x) = e(\frac{\nu x}{n})$.
\textbf{Exercise:} Show by direct calculation that $\psi_\nu(a+b)
= \psi_\nu(a) \psi_\nu(b)$. In fact, for each $\nu \in \ZnZ$ we
get a character, and the characters are distinct (exercise). We
only need to know how $\psi_\nu$ acts on $1$, as $\psi_\nu(k) =
\psi_\nu(1 + \cdots + 1) = \Big(\psi_\nu(1) \Big)^k$. $\hat{A}$ is
canonically isomorphic to $A$. What this means is, to each $\nu
\in A$ there corresponds a character $\psi_\nu$ in $\hat{A}$, and
to each character $\psi \in \hat{A}$ there corresponds a number
$\nu_\psi \in A = \ZnZ$.

We can multiply two characters: $(\psi_1 \psi_2)(x) = \psi_1(x)
\psi_2(x)$. It is easy to see that this is a character. The
trivial character (which sends everything to $1$) is the identity
of the group $\hat{A}$. \\

Consider two groups $A$ and $B$. We can use the characters of $A$
and $B$ to get the characters of the cartesian product $A \times
B$. If we want the characters of $A \times B$, take a character
$\psi_a$ of $A$ and $\phi_b$ of $B$ and form the character $\psi_a
\phi_b$, defined by $(\psi_a \phi_b)(x,y) = \psi_a(x)\phi_b(y)$.

\subsection{Characters of $\Fpf$: Dirichlet Characters}

We denote characters of $\Fpf$ by $\chi$. Dirichlet proved the
best theorem of mathematics, introducing a lot of new math to
solve the following:

\begin{thm}[Primes in Arithmetic Progressions, $1836$, $1839$]
Let $a$ and $n$ be relatively prime. There are infinitely many
primes which give $a$ when divided by $n$; moreover, to first
order all residue classes $a$ mod $n$ ($a$, $n$ relatively prime)
have the same number of primes!
\end{thm}

In other words, there are infinitely many $x$ such that $xn + a$
is prime if $a$ and $n$ are relatively prime.

\textbf{Hard Question:} without using Dirichlet's Theorem, can you
prove that there must be \emph{one} prime congruent to $a$ mod $n$
if $a$ and $n$ are relatively prime?

Clearly, if $a$ and $n$ are not relatively prime, there cannot be
infinitely many primes congruent to $a$ mod $n$. Dirichlet shows
this is also a sufficient condition.

Dirichlet introduced characters \emph{without} introducing the
concept of a group! They didn't have group notation until later.

Look at $\ZnZf = \{x: (x,n) = 1, 0 < x \le n-1\}$. This is a
finite Abelian group. $\# \ZnZf = \phi(n)$, where $\phi(n)$ is the
Euler totient function, and $\phi(n)$ is the number of integers
(between $0$ and $n$) which are relatively prime to $n$.

Dirichlet used $q$ instead of $n$, so we change notation and look
at $\ZqZf$.

A \textbf{Dirichlet Character} is a character $\chi$ of $\ZqZf$;
ie, $\chi: \ZqZf \rightarrow \C^{*}$ and $\chi(ab) =
\chi(a)\chi(b)$. Thus, $\chi$ lies in the dual group of $\ZqZf$.
Recall that $\C^{*}$ is the set of complex numbers with
multiplicative inverses, ie, $\C^{*} = \C - \{0\}$.

The \textbf{principal or trivial character} takes every $x \in
\ZqZf$ to $1$; we denote the trivial character by $\chi_0$.

If $\chi$ is a Dirichlet Character of $\ZqZf$, we say $\chi$ has
\textbf{modulus} (also called \textbf{conductor}) $q$. We extend
$\chi$ to be defined on all integers (all of $\Z$) by $\chi(m) =
0$ if $m$ and $q$ are not relatively prime, and $\chi(m) = \chi(m
\ \mbox{mod} \ q)$ otherwise. Clearly $\chi$ is periodic, as
$\chi(x + \lambda q) = \chi(x)$ for any $\lambda \in \Z$. Thus, we
have the map $\chi: \Z \rightarrow \C^{*}$.

If $q$ is prime, we have previously seen the character $\chi(a) =
\lag{a}{q}$. This is an extremely important character.

Another example: Let $q = 4$. We define $\chi(n)$ to be $0$ if $n$
is even, $1$ if $n \equiv 1$ mod $4$, and $-1$ if $n \equiv -1$
mod $4$. \textbf{Exercise:} Show this is a character.

\subsection{General Riemann Hypothesis (GRH)}

Below, $p$ will always denote a prime. $\chi$ will be a Dirichlet
character modulo $q$ ($q$ need not be prime).

\textbf{Conjecture: GRH:} \emph{For any $q \ge 1$, $x \ge 3$, if
$\chi \neq \chi_0$ (ie, if $\chi$ is not the principal character)
then}

\be \Big| \sum_{p \le x} \Big(1 - \frac{p}{x} \Big) \cdot \log p
\cdot \chi(p) \Big| \le C \log q \cdot \sqrt{x}. \ee

\emph{If $\chi = \chi_0$, then}

\be \sum_{p \le x} \Big(1 - \frac{p}{x} \Big) \log p \le
\frac{x}{2} + O(\sqrt{x}). \ee

\emph{$C$ is a universal constant, independent of $q$.}

We are summing primes up to $x$. The $1 - \frac{p}{x}$ factors are
just weight factors. If $q = 4$ and $\chi$ is the character above,
$\chi(p)$ gives a positive sign for primes congruent to $1$ mod
$4$ and a minus sign for primes congruent to $-1$ mod $4$.

\textbf{Analysis} means cancellation; you want to see cancellation
in a sum. The numbers in these sums are flipping (plus one, minus
one); we have about $\frac{x}{\log x}$ primes less than x (this is
the Prime Number Theorem, first proved in $1896$).

Random Drunk: each moment he flips a coin. If it is heads he
staggers one unit left, tails he staggers one unit right. After
$n$ steps, where do you think he'll be? Turns out he'll be about
$\sqrt{n}$ units from the origin (with high probability). Steve
Miller will prove this in a later lecture or handout.

\textbf{Motto:} random numbers (of size around $1$) cancel like
the square-root of the number of terms.

What GRH is telling us is that the remainder term behaves like
random noise.

\subsection{Proof of the Miller Test}

\begin{thm}If GRH is true, then if $\lag{a}{n} \equiv a^{\frac{n-1}{2}}$ mod
$n$ for all $a \le C \log^2 n$ for some fixed constant $C$, then
it is true for all $a$ (relatively prime to $n$, of course).
\end{thm}

Remarkable observation: $S = \{a \in \ZnZf: \lag{a}{n} \equiv
a^{\frac{n-1}{2}} \ \mbox{mod} \ n\}$ is a subgroup of $\ZnZf$.
Why? Exercise (Closure: show if $a, b$ in this set, so is $ab$.
Identity: show $1$ is in this set. Inverses: show if $a \in S$,
$a^{-1}$ mod $n$ is in $S$. Note associativity is inherited from
$\ZnZf$).

By assumption, $S$ contains (at least) the first $C \log^2 n$
elements. We claim this implies $S$ contains all $a$ relatively
prime to $n$.

Suppose $S$ is not all of $\ZnZf$. We talked about cosets (of
Abelian Groups) last time. (For more information about cosets and
quotient groups, see the following lecture by Steven J. Miller,
notes by Alex Barnett). As everything is abelian, we can form a
quotient group by dividing our group $\ZnZf$ by the abelian
subgroup $S$.

Thus, $A = \frac{\ZnZf}{S}$ is a group (a quotient group) and $A$
is a non-trivial group (ie, there is more than one element in this
group). Why must $A$ have more than one element? $A$ is the group
of representative cosets of $\ZnZf$ by the abelian subgroup $S$.
As $S$ is not all of $\ZnZf$, there must be at least two cosets.
Hence $A$ is not just the identity coset (which is $1 \cdot S$ or
just $S$).

But this quotient ($A$) is a finite abelian group. We know for a
finite abelian group that its dual is isomorphic to itself. This
means to each element in $A$ we have a character in $\hat{A}$, and
vice-versa. Further, the identity of $A$ is mapped to the trivial
character of $\hat{A}$.

Thus, there is a non-trivial $\chi: A \rightarrow \C^{*}$. Now
$\ZnZf \rightarrow \frac{\ZnZf}{S} \rightarrow \C^{*}$, and thus
we have a non-trivial Dirichlet character of $\ZnZf$ which is
trivial on $S$ (ie, $\chi(s) = 1$ for all $s \in S$).

By construction, $\chi$ is a Dirichlet character of $\ZnZf$, $\chi
\neq \chi_0$, and $\chi|_S = 1$ (the last means that $\chi$,
\textbf{restricted to} $s \in S$ is the identity map). We know $S$
is all elements up to at least $C \log^2 n$. So, $\chi(a) = 1$ for
all $a \le C \log^2 n$ (from the givens).

So, look at

\be\label{eqsumpqx} \mbox{Sum}(p,q,x) = \sum_{p \le x} \Big(1 -
\frac{p}{x} \Big) \log p \cdot \chi(p). \ee

By GRH, $\mbox{Sum}(p,q,x)$ is less than a constant multiple of
$\log q \cdot \sqrt{x}$. Calling the constant $C_2$, we have
$\mbox{Sum}(p,q,x) \le C_2 \log q \cdot \sqrt{x}$.

If $x \le C \log^2 n$, then there is a contradiction. Why? In
Equation \ref{eqsumpqx}, $\mbox{Sum}(p,q,x)$ is going to be
$\sum_{p \le x} \Big(1 - \frac{p}{x} \Big) \log p$, because all
the $\chi(p) = 1$ in this range. The \textbf{Prime Number Theorem}
states that the number of primes less than $x$ is $\frac{x}{\log
x}$ plus an error term which is smaller than $\frac{x}{\log x}$
(ie, in the limit, the size of the error term divided by the
number of primes less than $x$ tends to $0$ as $x$ goes to
infinity).

Thus,$\mbox{Sum}(p,q,x)$ is going to look like a multiple of $x$.
Why a multiple of $x$ and not a multiple of $\frac{x}{\log x}$?
Remember we have the factor $\log p$ in the sum; this follows from
\textbf{Partial Summation}. Partial Summation is a discrete
version of Integration by Parts; see the Appendices at the end for
a further statement.

By GRH, $\mbox{Sum}(p,q,x)$ is bounded by $C_2 \log q \cdot
\sqrt{x}$. Therefore, $x \le C_2 \log q \cdot \sqrt{x}$. This
implies $\sqrt{x} \le C_2 \log q$, or $x \le C_2^2 \log^2 q$.

\textbf{Remember, we've changed notation from $n$ to $q$. We are
assuming that $\lag{a}{q} \equiv a^{\frac{q-1}{2}}$ mod $q$ for
all $a \le C \log^2 q$ for some fixed constant $C$.} Take $C$
greater than $C_2^2$. Then we have a contradiction! If GRH is
true, $\mbox{Sum}(p,q,x) \le C_2 \log q \cdot \sqrt{x}$ only for
$x \le C_2^2 \log^2 q$. But we are assuming that $q$ passes the
Miller Test for all $a$ up to $C \log^2 q$. Thus, we can take $x$
larger than $C_2^2 \log^2 q$.


\subsection{Review of Proof}

Intuition: For random sequences, expect square-root cancellation
in sums.

If we have a non-trivial character, if we look at these weighted
sums of $\chi(p)$, there is no extra structure; we expect
cancellation like $\sqrt{x}$. There has to be \emph{some}
$q$-dependence, but GRH says it is like a universal constant times
$\log q$.

In the Miller Test, we test something $C \log^2 n$ times. If the
condition is always true for these $C \log^2 n$ elements, we have
a subgroup $S$ of $\ZnZf$. If $S$ isn't all of $\ZnZf$, we can
find a character, and we have sums with this character (any sum
with characters is called \textbf{Harmonic Analysis}). Further,
this is a non-trivial character which is the identity on the
original group. The GRH cannot accomodate a character of this
type.

Why does the GRH lead to a contradiction? Basically, GRH says a
certain weighted sum of $\chi(p)$ over the primes less than $x$
(where $\chi$ is a Dirichlet character with modulus $q$) cannot be
too large. Specifically, it is at most $C_2 \log q \cdot
\sqrt{x}$.

This implies that there is a lot of noise in the $\chi(p)$;
basically, we need to have a good mixing of primes which give
$\chi(p) = +1$ with primes giving $\chi(p) = -1$.

However, if $n$ satisfies the Miller Test for $a$ up to $C \log^2
q$ (with $C > C_2^2$), then we can find a modulus $q$ and a
Dirichlet character $\chi$ where we have a very long string of
primes giving $\chi(p) = +1$. This forces the weighted sum of
$\chi(p)$ over primes less than $x$ (taking $x = C \log^2 q$) to
be larger than $C_2 \log q \sqrt{x}$, a contradiction.

\section{Appendices}

\subsection{Aside: For $p$ Odd, Half the Non-Zero Numbers are
Quadratic Residues}

Note, for an odd prime $p_j$, half of the non-zero numbers are
quadratic residues, and half are quadratic non-residues. The
\textbf{Legendre symbol} takes each element $a \in \Fpf$ to an
element in the group $\{-1, 1\}$. This is a homomorphism; not
every element has a square. The image is $\{-1,1\}$; the kernel is
all the elements of $\Fpf$ which are sent to $1$. Thus, half the
numbers are residues, half are non-residues.

$a \rightarrow \lag{a}{p} = a^{\frac{p-1}{2}}$ mod $p$. Thus, we
have a homomorphism (given by the Legendre symbol) from $\Fpf
\rightarrow \{-1,1\}$. We claim the map is onto. The kernel is all
elements in $\Fpf$ which are mapped by the Legendre symbol to $1$,
ie, the quadratic residues. (One needs to show the Legendre symbol
is a group homomorphism: $\lag{xy}{p} = \lag{x}{p} \cdot
\lag{y}{p}$).

Standard Group Theory Arguments: $\frac{ \# \Fpf }{\mbox{kernel} }
\approxeq \{-1,1\}$. Thus, half the numbers in $\Fpf$ are
quadratic residues.

\subsection{Chinese Remainder Theorem}

\begin{thm}[Chinese Remainder Theorem]Let $m = m_1m_2$, $m_1$ and $m_2$
relatively prime. Then $\Z / m\Z \approxeq \Z/ m_1\Z \times \Z /
m_2 \Z$.
\end{thm}

This allows us to, given $a_1$ mod $m_1$ and $a_2$ mod $m_2$, find
an $a$ mod $m$ such that $a \equiv a_1$ mod $m_1$ and $a \equiv
a_2$ mod $m_2$. For example, try and solve $x \equiv 3$ mod $5$
and $x \equiv 4$ mod $7$.

See any book on Algebra.

\subsection{Partial Summation}

\begin{lem}[Partial Summation: Discrete
Version]\label{lempartialsummationdiscretefirst}
\begin{eqnarray}
\sum_M^N a_n b_n = A_N b_N - A_{M-1} b_M + \sum_M^{N-1} A_n (b_n -
b_{n+1})
\end{eqnarray}
\end{lem}

\begin{lem}[Abel's Summation Formula - Integral
Version]\label{partialsummationintegralfirst} Let $h(x)$ be a
continuously differentiable function. Let $A(x) = \sum_{n \leq x}
a_n$. Then
\begin{eqnarray}
\sum_{n \leq x} a_n h(n) = A(x) h(x) - \int_1^x A(u) h'(u) du
\end{eqnarray}
\end{lem}

See, for example, Walter Rudin, \emph{Principles of Mathematical
Analysis} (also known as \emph{The Blue Book}), page $70$.




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\chapter{Cosets, Quotient Groups, and an Introduction to Probability}

Quotient groups. Basic probability theory for random walk. Lecture
by Steven J. Miller; notes by Alex Barnett and Steven J. Miller.

\section{Quotient groups}

Say we have a finite Abelian group $G$ (this means for all $x, y
\in G$, $xy = yx$) of order $m$ which has a subgroup $H$ of order
$r$. We will use multiplication as our group operation. Recall the
{\em coset} of an element $g\in G$ is defined as the set of
elements $gH = g\{h_1,h_2,\cdots,h_r\}$. Since $G$ is Abelian
(commutative) then $gH = Hg$ and we will make no distinction
between left and right cosets here.

The {\em quotient group} (or {\em factor group}), symbolized by
$G/H$, is the group formed from the cosets of all elements $g\in
G$. We treat each coset $g_i H$ as an element, and define the
multiplication operation as usual as $g_i H g_j H$. Why do we need
$G$ to be Abelian? The reason is we can then analyze $g_i H g_j
H$, seeing that it equals $g_i g_j H H$. We will analyze this
further when we prove that the set of cosets is a group.

There are several important facts to note. First, if $G$ is not
Abelian, then the set of cosets might not be a group. Second,
recall we proved the coset decomposition rule: given a finite
group $G$ (with $n$ elements) and a subgroup $H$ (with $r$
elements) then there exist elements $g_1$ through $g_k$ such that

\be G = \bigcup_{i=1}^k g_i H. \ee

The choices for the $g_i$'s is clearly not unique. If $g_1$
through $g_k$ work, so do $g_1 h_1$ through $g_k h_k$, where $h_i$
is any element of $H$. Recall this was proved by showing any two
cosets are either distinct or identical.

We will show below that, for $G$ Abelian, the set of cosets is a
group. Note, however, that while it might at first appear that
there are many different ways to write the coset group, they
really are the same. For example, the cosets $gH$ and $g h_1 h_2^4
h_3 H$ are equal. This is similar to looking at integers mod $n$;
mod $12$, the integers $5$, $-7$ and $19$ are all equal, even
though they look different.

We now prove that the set of cosets is a group (for $G$ Abelian).


{\bf Closure.} By commutivity $g_i H g_j H = g_i g_j H H$. What is
``$H H$''? Just the set of all $r^2$ possible combinations of
elements of $H$. By closure, and the existence of the identity,
this just gives $H$ again (recall no element in a group can appear
more than once---duplicates are removed). Therefore $g_i H g_j H =
g_i g_j H$. Now, as $G$ is a group and is closed, $g_i g_j \in G$.
Thus, there is a $\alpha$ such that $g_i g_j \in g_\alpha H$ (as
$G = \bigcup_{\beta=1}^k g_\beta H$. Therefore, there is an $h \in
H$ such that $g_i g_j = g_\alpha h$, which implies $g_i g_j H =
g_\alpha h H = g_\alpha H$. Thus, the set of cosets is closed
under coset multiplication. Note, however, that while the coset
$g_i g_j H$ is in our set of cosets, it may be written
differently.

{\bf Identity.} If $e$ is identity of $G$, then $eH g_i H = g_i H$
and $g_i H eH = g_i H$, so $eH$ is the identity of this quotient
group.

{\bf Associativity.} Since as you may have noticed, the quotient
group elements behave just like those of $G$, associativity
follows from that of $G$.

{\bf Inverse.} It is easy to guess $g^{-1} H$ is the inverse of $g
H$. Check it: $g^{-1} H g H = g^{-1} g H = eH = $ identity, also
true the other way round of course by commutativity.
Unfortunately, $g^{-1} H$ might not be listed as one of our
cosets! Thus, we must be a little more careful. Fortunately, as
$g^{-1} \in G = \bigcup_{\beta=1}^k g_\beta H$, there is an
$\alpha$ such that $g^{-1} \in g_\alpha H$. Then, there is an $h
\in H$ with $g^{-1} = g_\alpha h$. Thus, $g^{-1} = g_\alpha h H =
g_\alpha H$, and direct calculation will show that the coset
$g_\alpha H$ is the inverse (under coset multiplication) of $g H$.





\section{Random walk and discrete probability}

Each step in a random walk is a random event. We first study a
single random event, then the combination of two random events,
then multiply repeated events.

For other introductory probability theory see\newline
\verb+http://engineering.uow.edu.au/Courses/Stats/File24.html+

\subsection{Probability distribution for a single event, mean, variance}

Our single event consists of one of a set of choices happening.
The choices are labelled by $i = 1\cdots N$, which are exclusive
(no more than one can happen), and complete (no less than one can
happen). For instance, for a single coin toss, \bea
i& =& 1 \hspace{1in} H , \mbox{ heads}, \nonumber \\
i&=&2 \hspace{1in} T, \mbox{ tails}. \nonumber \eea We take the
choice $i$ as a {\em random variable}, meaning all we know is a
probability $p_i \ge 0$ that each choice can happen. $p=0$ means
it never happens, $p=1$ means it always happens, and most things
are somewhere in between (the unbiased coin has $p_i=\frac{1}{2}$,
$\forall i$, ignoring the small probabilities of the coin landing
on its edge or quantum tunneling through the table.)

The set of $\{p_i\}$ we call the {\em probability distribution}.
Completeness implies \be
    \sum_i p_i  \; = \; 1.
\ee Note the abbreviation $\sum_i$ for $\sum_{i=1}^N$.

We have some quantity $f$ which has a value $f_i$ for each choice
$i$. For instance, $f$ could be the number of dots on each face
$i$ of a die, in which case $f_i = i$. {\em Mean} and {\em
variance} are ways to characterize (summarize) the distribution
over $f$.

{\bf Mean.} The mean or {\em expectation value} (expected value)
of $f$ over the distribution is \be \label{eq:m}
    \bar{f} \equiv E[f]  \; = \; \sum_i p_i f_i .
\ee This is just a weighted average of $f$. Check it for the
(unweighted) dice. $p_i=\frac{1}{6}$, $\forall i$. You should get
$\bar{f} = \frac{7}{2}$.

{\bf Variance.} The variance is the square of the {\em standard
deviation} $\sigma_f$:

\be \label{eq:v}
    \sigma_f^2 \equiv \var[f] \; = \;
\sum_i p_i (f_i - \bar{f})^2. \ee

Crudely $\sigma_f$ gives the width of the distribution in $f$.
From the definition you can see $\sigma_f$ is the {\em root mean
square} (rms) deviation from the mean. Expanding out the square,
and using earlier results, \bea \label{eq:var}
    \var[f] & \; = \; & \sum_i p_i f_i^2 - 2\bar{f} \cdot \sum_i p_i f_i +
\bar{f}^2 \cdot \sum_i p_i \nonumber \\
& = & \sum_i p_i f_i^2 - \bar{f}^2 \nonumber \\
& = & E[f^2] - E[f]^2. \eea This is a very useful formula. The
first term is often known as the {\em second moment} of $f$. The
{\em first moment} is just the mean. The $m^{th}$ {\em moment} is
defined as $\sum_i p_i f^m_i$.

A quick comment about units. If $f_i$ and $f$ are measured in feet
(for example), the variance (which gives information on how spread
out $f_i$ is) has units feet-squared. If someone asks how tall the
people in our class are, one would answer about $5 \frac{1}{2}$
feet. If one is further pressed to give a range for the heights of
our class, one might say $5 \frac{1}{2}$ feet, plus or minus
$\frac{1}{4}$ of a foot. One would not give the error range in
feet-squared! Thus, in measuring error it is the square-root of
the variance that comes into play. Note that if $f_i$ is in feet,
the variance is in feet-squared, and the square-root of the
variance is in feet.

\subsection{Multiple events}

Consider two random events. Let the first have choices $i=1\cdots
N$ (with probabilities $p_1$ through $p_N$); let the second event
have choices $j=1\cdots M$ (with probabilities $q_1$ through
$q_M$). Then there are $NM$ choices (possibilities) for the
combined event, which we could label by $k\equiv ij$. Since $k$ is
also a random variable, it has probability $r_k = r_{ij}$ (you
could think of this as a matrix in $i,j$). If the two events are
{\em independent} (also called {\em uncorrelated}), then \be
r_{ij} \; = \; p_i q_j, \;\; \forall ij \hspace{1in}
\mbox{independence}. \ee (In other words, the matrix is separable
in the $i$ and $j$ directions). Many events we study will be
independent. For example, if you flip a fair coin twice, the
result of the first flip has no effect on the result of the second
flip. Or if you roll a fair die, et cetera.

As before we have a quantity $f_i$ associated with each choice $i$
for the first event. For the second event we have a (in general
different) quantity $g_j$ for its choice $j$. We want to learn
about the sum of these two quantities, \be s \equiv f + g. \ee
Note that for each combined choice $ij$, this quantity $s$ has
value $s_{ij} = f_i + g_j$. How is $s$ is distributed? We will
show that {\em independence} of the two events implies a simple
law giving the mean and variance of $s$ in term of those of $f$
and $g$.


We compute the mean of $s$, following Eq.~\ref{eq:m}, except now
we are summing over all combined possibilities $ij$, \bea E[s] & =
& \sum_{ij} r_{ij} s_{ij} = \sum_{ij} p_i q_j (f_i + g_j) = \sum_i
p_i f_i \cdot \sum_j q_j + \sum_i p_i \cdot \sum_j q_j g_j
\nonumber \\ & = &\bar{f}\cdot 1 + 1\cdot \bar{g} =
\bar{f}+\bar{g}. \eea So, the means {\em add}.

We isolate this important fact:

\begin{lem}\label{lemmeansum}For independent events, the mean of a sum
is the sum of the means. Equivalently, the sum of the expected
values is the expected value of the sums. Thus, for any
independent events $A$ and $B$, $E[A+B] = E[A] + E[B]$.
\end{lem}


What if we multiply $A$ by a constant $c$? For example, consider
outcomes $A_i$ with probabilities $p_i$. The mean $\bar{A} =
E[A]$. What is the mean of the new event with outcomes $cA_i$
occurring with probabilities $p_i$?

Well,

\bea E[cA] &=& \sum_i p_i cA_i \nonumber\\ &=& c \sum_i p_i A_i
\nonumber\\ &=& c E[A]. \eea

We have therefore shown

\begin{lem}\label{meanmultiple}The mean of a multiple is the multiple of the mean.
Equivalently, the expected value of a multiple is the multiple of
the expected value. Thus, $E[cA] = cE[A]$.
\end{lem}

We now calculate the variance of a sum, using the above results.
For the variance of $s = f + g$ we can use Eq.~\ref{eq:var} to get
\bea \var[s] &=& E[s^2] - E[s]^2 \nonumber \\
    &=& E[(f+g)^2] - (E[f+g])^2 \nonumber\\
    &=& E[(f+g)^2] - (E[f] + E[g])^2 \ \mbox{by Lemma
    \ref{lemmeansum}} \nonumber\\
    &=& E[(f + g)^2] - (\bar{f}+\bar{g})^2 \nonumber \\
    &=& E[f^2] + 2E[fg] + E[g^2] -(\bar{f}+\bar{g})^2.
\eea

We justify the last step as follows:

\bea E[(f+g)^2] &=& E[f^2 + 2fg + g^2] \nonumber\\ &=& E[(f^2 +
2fg) + g^2] \nonumber\\ &=& E[(f^2 + 2fg)] + E[g^2] \ \mbox{by
Lemma \ref{lemmeansum}} \nonumber\\ &=& E[f^2] + E[2fg] + E[g^2] \
\mbox{by Lemma \ref{lemmeansum}} \nonumber\\ &=& E[f^2] + 2E[fg] +
E[g^2] \ \mbox{by Lemma \ref{meanmultiple}}. \eea

Using independence again we can factorize $E[fg] = \sum_{ij}
r_{ij} f_i g_j = \sum_i p_i f_i \cdot \sum_j q_j g_j = E[f] E[g] =
\bar{f}\bar{g}$. Elegantly, this term is responsible for
cancelling the cross-term in $(\bar{f}+\bar{g})^2$, and collecting
the remaining terms leaves \be \var[s] = \var[f] + \var[g] \ee So,
the variances {\em also} add.

We now take the special case when the second event is identical to
the first. That is, $M=N$, $q_i = p_i$, $g_i = f_i$, $\forall i$.
In this case the above shows that $\bar{s} = 2\bar{f}$ and
$\var[s] = 2\var[f]$.

We can repeat the above combination law for successively repeated
events. Such events are called {\em identical independently
distributed} (iid) events. Suppose we have $K$ repetitions of iid
events, with total $s \equiv f + g + \cdots + z$,
% better notation here? But don't change the whole of above.
we can just repeatedly apply the above rules to get \bea
\bar{s} & = & K \;\bar{f} \\
\var[s] & = & K \; \var[f]. \eea

The mean and variance are not the only characteristics of the
distribution that add like in this way. Amazingly, there is an
infinite sequence of special combinations of the higher moments,
called {\em cumulants}, which add just like this. The mean and
variance are just the first two cumulants.



\subsection{Simplest random walk}

We now have the tools to characterize a random walk. We choose
$N=2$ and $p_1 = p_2 = \frac{1}{2}$, just as with the coin toss,
and define the ``step'' displacements $f_1 = +1$ and $f_2 = -1$.
This corresponds to a drunkard taking (uncorrelated) steps of unit
length along the integer line.

We use Eqs.~\ref{eq:m} and \ref{eq:var} to evaluate the mean and
variance of a single step event. \bea \bar{f} &=& +\frac{1}{2} -
\frac{1}{2} = 0 \\ \var[f] &=& \frac{1}{2}. (1)^2 + \frac{1}{2}.
(-1)^2  - (0)^2 = 1. \eea For $K$ steps, starting from the origin,
we have the final displacement of $s$, the sum of all the steps,
using the formulae above,

\bea \bar{s} & = & K \cdot 0 = 0 \\ \var[s] & = & K \cdot 1  = K.
\eea

So the standard deviation, \ie the width of the distribution of
$s$, has value \be \sigma_s \equiv \sqrt{\var[s]} = \sqrt{K}. \ee
This is our first vital fact about all but the most pathological
\footnote{Random walks which do {\em not} exhibit this power law
have infinite variance and exhibit {\em anomalous diffusion}. For
more on this, see M. Bazant's excellent course at {\tt
http://www-math.mit.edu/$\sim$bazant/teach/18.325}} random walks:
the distribution has width which scales like $K^{1/2}$. This means
that a {\em typical} distance from the origin is $\sqrt{K}$. This
is called a {\em diffusion process} and is very common in the real
world.

Again, remember that if the person walks in feet, the variance
(which is a measure of how much the distribution spreads out) will
be in feet-squared. By taking the square-root we again have units
of feet.

\subsection{Central Limit Theorem}

The Central Limit Theorem (CLT) states that the distribution on
$s$ tends to a {\em Gaussian distribution}, \be p(s) \approx
\mathcal{N}(\mu,\sigma^2) \; \equiv \; \frac{1}{\sqrt{2\pi}\sigma}
e^{-\frac{(s-\mu)^2}{2\sigma^2}} \ee with mean $\mu = \bar{s}$ and
variance $\sigma^2 = \var[s]$ as given above, as $K\rightarrow
\infty$. This is a very common ``bell curve'' with width $\sigma$
centered about $\mu$. We have not defined $p(s)$ very
rigorously---it is simply the probability of being displaced $s$
from the origin at the end of the $K$-step random walk. An exact
formula for $p(s)$ involves counting all the ways that $\pm 1$ can
be added $K$ times to get exactly $s$. We will postpone this, and
the proof of CLT, for next time.

Remarkably the CLT applies to {\em any} $N$ with any discrete step
distribution $\{p_i\}$ and any step displacements $\{f_i\}$. It
also applies to the case of continous-valued steps with
distribution $p(f)$ along the real line. However a criterion for
validity is always the {\em finiteness of the second moment} of
$p(f)$.





%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\chapter{Quadratic Reciprocity, Central Limit Theorem and Graph Problems}

We give Eisenstein's proof of Quadratic Reciprocity, and then
introduce the Graph Theory problems. Lecture by Steven J. Miller
and Peter Sarnak; notes by Steven J. Miller.

\section{Eisenstein's Proof of Quadratic Reciprocity}

\subsection{Preliminaries}

\begin{thm}[Quadratic Reciprocity] Let $p$ and $q$ be distinct odd
primes. Then

\be \js{q} \lag{p}{q} = (-1)^{\frac{p-1}{2}\frac{q-1}{2}}. \ee
\end{thm}

As $p$ and $q$ are distinct, odd primes, both $\js{q}$ and
$\lag{p}{q}$ are $\pm 1$. The difficulty is figuring out which
signs are correct, and how the two signs are related.

We use Euler's Criterion, proved in a previous lecture:

\begin{lem}[Euler's Criterion]
\be \js{q} \equiv q^{\frac{p-1}{2}} \ \mbox{mod} \ p. \ee
\end{lem}

The idea behind Eisenstein's proof is as follows: $\js{q}
\lag{p}{q}$ is $-1$ to a power. Further, we only need to determine
the power mod $2$. Eisenstein shows many expressions are
equivalent, mod $2$, to this power. Eventually, we arrive at an
expression which is trivial to calculate (mod $2$).


\subsection{First Stage}

Consider all even multiples of $q$ by an $a \le p-1$: $\{2q, 4q,
6q, \dots, (p-1)q\}$. Denote a generic multiple by $aq$. Recall
$[x]$ is the greatest integer less than or equal to $x$. By
integer division,

\be aq = \Bigg[\frac{aq}{p}\Bigg]p + r_a, \ 0 \le r_a < p-1. \ee

Thus, $r_a$ is the least non-negative number equivalent to $qa$
mod $p$.

The numbers $(-1)^{r_a} r_a$ are equivalent to even numbers in
$\{0,\dots,p-1\}$. If $r_a$ is even this is clear; if $r_a$ is
odd, then $(-1)^{r_a} r_a \equiv p - r_a$ mod $p$, and as $p$ and
$r_a$ are odd, this is even.

\begin{lem} If $(-1)^{r_a} r_a \equiv (-1)^{r_b} r_b$, then $a =
b$. \end{lem}

Proof: We quickly get $\pm r_a \equiv r_b$ mod $p$. If the plus
sign holds, then $r_a \equiv r_b$ mod $p$ implies $qa \equiv qb$
mod $p$. As $q$ is invertible mod $p$, we get $a \equiv b$ mod
$p$, which yields $a = b$ (as $a$ and $b$ are even integers
between $0$ and $p-1$).

If the minus sign holds, then $r_a + r_b \equiv 0$ mod $p$, or $qa
+ qb \equiv 0$ mod $p$. Multiplying by $q^{-1}$ mod $p$ now gives
$a + b \equiv 0$ mod $p$. As $a$ and $b$ are even integers between
$0$ and $p-1$, $0 < a + b \le 2(p-1)$. The only integer strictly
between $0$ and $2p$ which is equivalent to $0$ mod $p$ is $p$;
however, $p$ is odd and $a+b$ is even. Thus, the minus sign cannot
hold, and the elements are all distinct. $\Box$.

\begin{lem} \be \js{q} = (-1)^{\sum_{a \mbox{\tiny even}} r_a }. \ee
\end{lem}

Proof: For each even $a$, $qa \equiv r_a$ mod $p$. Thus, mod $p$:

\bea \prod_{a \ \mbox{\tiny even}} qa & \equiv & \prod_{a \
\mbox{\tiny even}} r_a \nonumber\\ q^{\frac{p-1}{2}} \prod_{a \
\mbox{\tiny even}} a & \equiv & \prod_{a \ \mbox{\tiny even}} r_a
\nonumber\\ \js{q} \prod_{a \ \mbox{\tiny even}} a & \equiv &
\prod_{a \ \mbox{\tiny even}} r_a, \eea

where the above follows from the fact that we have $\frac{p-1}{2}$
choices for an even $a$ (to get $q^{\frac{p-1}{2}}$ and Euler's
Criterion (to replace $q^{\frac{p-1}{2}}$ with $\js{q}$).

As $a$ ranges over all even number from $0$ to $p-1$, so too do
the numbers $(-1)^{r_a} r_a$ mod $p$. Thus, mod $p$,

\bea \prod_{a \ \mbox{\tiny even}} a & \equiv & \prod_{a \
\mbox{\tiny even}} (-1)^{r_a} r_a \nonumber\\ \prod_{a \
\mbox{\tiny even}} a & = & (-1)^{\sum_{a \ \mbox{\tiny even}} r_a}
\prod_{a \ \mbox{\tiny even}} r_a. \eea

Combining gives

\be \js{q} (-1)^{\sum_{a \ \mbox{\tiny even}} r_a} \prod_{a \
\mbox{\tiny even}} r_a \equiv \prod_{a \ \mbox{\tiny even}} r_a.
\ee

As each $r_a$ is invertible mod $p$, so is the product. Thus,

\be \js{q} (-1)^{\sum_{a \ \mbox{\tiny even}} r_a} \equiv 1 \
\mbox{mod} \ p. \ee

As $\js{q}$ is its own inverse, the Lemma now follows by
multiplying both sides by $\js{q}$. $\Box$.


Therefore, it is sufficient to determine $\sum_{a \ \mbox{\tiny
even}} r_a$ mod $2$.

We make one last simplification. By integer division, we have

\bea \sum_{a \ \mbox{\tiny even}} qa &=& \sum_{a \ \mbox{\tiny
even}} \Bigg( \Bigg[ \frac{qa}{p} \Bigg] p + r_a \Bigg)
\nonumber\\ &=& \sum_{a \ \mbox{\tiny even}} \Bigg[ \frac{qa}{p}
\Bigg] p + \sum_{a \ \mbox{\tiny even}} r_a. \eea

As we are summing over even $a$, the Left Hand Side above is even.
Thus, the Right Hand Side is even, so

\bea \sum_{a \ \mbox{\tiny even}} \Bigg[ \frac{qa}{p} \Bigg] p &
\equiv & \sum_{a \ \mbox{\tiny even}} r_a \ \mbox{mod} \ 2
\nonumber\\ p \sum_{a \ \mbox{\tiny even}} \Bigg[ \frac{qa}{p}
\Bigg] & \equiv & \sum_{a \ \mbox{\tiny even}} r_a \ \mbox{mod} \
2 \nonumber\\ \sum_{a \ \mbox{\tiny even}} \Bigg[ \frac{qa}{p}
\Bigg] & \equiv & \sum_{a \ \mbox{\tiny even}} r_a \ \mbox{mod} \
2, \eea

where the last line follows from the fact that $p$ is odd, so mod
$2$, dropping the factor of $p$ from the Left Hand Side doesn't
change the parity.

We have shown

\begin{lem} It is sufficient to calculate $\sum_{a \ \mbox{\tiny even}} \Big[ \frac{qa}{p}
\Big]$ \end{lem}



\subsection{Second Stage}

Consider the rectangle with vertices at $A = (0,0)$, $B = (p,0)$,
$C = (p,q)$ and $D = (0,q)$. The upward slopping vertical is given
by the equation $y = \frac{q}{p}x$. As $p$ and $q$ are distinct
odd primes, there are no pairs of integers $(x,y)$ on the line
$AC$.

We now interpret $\sum_{a \ \mbox{\tiny even}} \Big[ \frac{qa}{p}
\Big]$. Consider the vertical line with $x$ coordinate $a$. Then
$\Big[ \frac{qa}{p} \Big]$ gives the number of pairs $(x,y)$ with
$x$-coordinate equal to $a$ and $y$-coordinate an integer at most
$\Big[ \frac{qa}{p} \Big]$. Thus, $\sum_{a \ \mbox{\tiny even}}
\Big[ \frac{qa}{p} \Big]$ is the number of integer pairs (in the
rectangle $ABCD$) with even $x$-coordinate that are below the line
$AC$.

We add some additional points: $E = (\frac{p}{2},0)$, $F =
(\frac{p}{2},\frac{q}{2})$, $G = (0,\frac{q}{2})$ and $H =
(\frac{p}{2},q)$. We prove

\begin{lem} The number of integer pairs under the line $AC$ (inside
the rectangle) with even $x$-coordinate is congruent mod $2$ to
the number of integer pairs under the line $AF$. \end{lem}

Let $a > \frac{p}{2}$ be an even integer. The integer pairs on the
line $x = a$ are $(a,0)$, $(a,1), \dots, (a,q)$. There are $q+1$
pairs. As $q$ is odd, there are an even number of integer pairs on
the line $x = a$. As there are no integer pairs on the line $AC$,
for a fixed $a
> \frac{p}{2}$, mod $2$ there are the same number of integer pairs
\emph{above} $AC$ as there are \emph{below} $AC$.

Further, the number of integer pairs \emph{above} $AC$ is
equivalent mod $2$ to the number of integer pairs below $AF$ on
the line $x = p-a$. To see this, consider the map which takes
$(x,y)$ to $(p-x,q-y)$. As $a > \frac{p}{2}$ and is even, $p-a <
\frac{p}{2}$ and is odd. Further, every odd $a < \frac{p}{2}$ is
hit (given $a_{odd} < \frac{p}{2}$, start with the even number $p
- a_{odd} > \frac{p}{2})$.

Let $\# FCH_{even}$ be the number of integer pairs $(x,y)$ in
triangle $FCH$ with $x$ even.

Let $\# EBCH$ be the number of integer pairs in the rectangle
$EBCH$; $\# EBCH \equiv 0$ mod $2$ (we've shown each vertical line
has an even number of pairs).

Let $\# AFE_{even}$ be the number of integer pairs $(x,y)$ in the
triangle $AFE$ with $x$ even, and let $\# AFE$ be the number of
integer pairs in the triangle $AFE$.

We need to calculate $\sum_{a \ \mbox{\tiny even}} \Big[
\frac{qa}{p} \Big]$ mod $2$:

\bea \sum_{a \ \mbox{\tiny even}} \Bigg[ \frac{qa}{p} \Bigg] & = &
\# AFE_{even} + \# EBCH - \# FCH \nonumber\\ & \equiv & \#
AFE_{even} + \# EBCH + \# FCH \nonumber\\ & = & \# AFE_{even} + \#
FCH + \# EBCH \nonumber\\ &=& \# AFE + \# EBCH \nonumber\\ &=& \#
AFE. \eea

Therefore, $\mu = \sum_{a \ \mbox{\tiny even}} \Big[ \frac{qa}{p}
\Big] \equiv \# AFE$ mod $2$, and we have

\be \js{q} = (-1)^\mu. \ee

Reversing the rolls of $p$ and $q$, we see that

\be \lag{p}{q} = (-1)^\nu, \ee

where $\nu \equiv \# AFG$ mod $2$, with $\# AFG$ equal to the
number of integer pairs in the triangle $AFG$.

Now, $\mu + \nu = \# AFE + \# AFG$, which is the number of integer
pairs in the rectangle $AEFG$. There are $\frac{p-1}{2}$ choices
for $x$ and $\frac{q-1}{2}$ choices for $y$, giving
$\frac{p-1}{2}\frac{q-1}{2}$ pairs of integers in the rectangle
$AEFG$.

Thus,

\bea \js{q} \lag{p}{q} & = & (-1)^{\mu + \nu} \nonumber\\ &=&
(-1)^{\# AFE + \# AFG} \nonumber\\ &=&
(-1)^{\frac{p-1}{2}\frac{q-1}{2} }, \eea

which completes the proof of Quadratic Reciprocity. $\Box$.






\section{Central Limit Theorem}
$X_1, X_2, X_3, \dots $ an infinite sequence of random variables
such that the $X_j$ are independent identically distributed random
variables (abbreviated i.i.d.r.v.) with $E[X_j] = \bar{X_j} = 0$
(can always renormalize by shifting) and variance $E[X_j^2] = 1$.
Let $S_N = \sum_{j=1}^N X_j$.

\begin{thm} Fix $-\infty < a \le b < \infty$. Then as $N
\rightarrow \infty$,

\be \mbox{Prob}\Big( \frac{S_N}{\sqrt{N}} \in [a,b] \Big)
\rightarrow \frac{1}{\sqrt{2\pi}} \int_a^b e^{-\frac{t^2}{2}} dt.
\ee
\end{thm}

The probability function is called the Gaussian or the Normal
distribution. This is the universal curve of probability. Note how
robust the Central Limit Theorem is: it doesn't depend on fine
properties of the $X_j$.

\section{Possible Problems}

\subsection{Combinatorics and Probability}

\textbf{Combinatorics} is the number of ways of doing something. A
\textbf{graph} is a set of \textbf{vertices} ($V$) and
\textbf{edges} ($E$) between them. Thus, edges are unordered pairs
of vertices.

Four Color Theorem (proved by an exhaustive search by the
computer). Say you have a graph in the plane (thus, if you draw
the vertices and edges in the plane, no two edges cross). Call
such a graph a \textbf{planar graph}. Can you color the vertices
such that if two vertices are joined by an edge, they have
different colors? Sure, by using $|V|$ colors! What is the least
number of colors needed?

\begin{thm}[Four Coloring Theorem] You can color the vertices of a
planar graph using at most four colors such that no two joined
vertices have the same color. \end{thm}

A \textbf{$k$-regular graph} is a graph such that there are
$k$-edges out of each vertex. A $2$-regular graph has no freedom:
you get a closed cycle.

Consider $3$-regular graphs. To each graph associate the
\textbf{adjacency matrix $A$}. (First to study may be Kirchhoff).
Say $G = (V,E)$ has $|V| = n$ vertices. For now, assume there are
no \textbf{multiple edges} (ie, between any two vertices is at
most one edge, and there are no edges connecting a vertex to
itself). $A$ is an $n \times n$ matrix, rows and columns indexed
by the vertices, and $A_{ij} = 1$ if there is an edge from $v_i$
to $v_j$ and $0$ otherwise.

Thus, the adjacency matrix is a matrix with $0$s and $1$s, and is
symmetric. \\

\textbf{Problem: What is the second largest eigenvalue of $A$? How
does it vary? What do we expect?} \\

In Linear Algebra, we learn we can diagonalize a real symmetric
matrix. The eigenvalues are real, and satisfy $p_A(\lambda) =
\mbox{det}(\lambda I - A)$, the characteristic polynomial. This is
a polynomial in $\lambda$ of degree $n$ with integer coefficients.
Thus, the eigenvalues are algebraic numbers. The leading
coefficient is $\lambda^n$, the constant term is $\mbox{det}(A)$.

Thus, $p_A(\lambda) = \lambda^n + \cdots + \mbox{det}(A)$, and by
the Fundamental Theorem of Algebra, there are $n$ complex roots.
If the leading coefficient of the defining polynomial is $1$, we
say the roots are \textbf{algebraic integers}. These roots are the
eigenvalues of $A$.

Why must the eigenvalues be real? Want $Av = \lambda v$, $v$ a
non-zero vector.

Fact: if $v = (1,1,\dots,1)^T$, $v$ is an eigenvector of $A$ with
eigenvalue $k$. Why? As each vertex is connected to $k$ distinct
vertices, each row has exactly $k$ entries that are $1$ and $n-k$
entries that are $0$. Thus, $k$ is an eigenvalue, denote by
$\lambda_0$.

\begin{exe} Show, for such adjacency matrices $A$, that all
eigenvalues satisfy $-k \le \lambda \le k$. \end{exe}

Consider connected graphs $G$. How big is $\lambda_1(G)$ for the
random $3$-regular graph?

\begin{thm}[Kirchhoff's Theorem] Let $\mbox{det}^{*}(kI-A)$ be
the product of the non-zero eigenvalues of $A$. Then
$\mbox{det}^{*}(kI-A)$ equals the number of vertices times the
number of spanning trees. \end{thm}

A tree is a connected graph with no cycles. A \textbf{spanning
tree} is a connected sub-graph containing all the vertices.
Sometimes called complexity of the graph.





%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\chapter{Efficient Algorithms, Probability, Alg+Transcendental, Pidgeon Hole, Chebychev}

We review many basic number theory results. We give efficient
algorithms for polynomial evaluation, calculating $x^n$, and
finding the greatest common divisor. We briefly review probability
theory. After an introduction to Algebraic and Transcendental
Numbers, we review Dirichlet's Box Principle (aka the Pidgeonhole
Principle), and give an application. We prove a weak version of
Chebyshev's Theorem on the approximate number of primes. Lecture
by Steven J. Miller. Notes by Steven J. Miller (and Florin Spinu,
who helped write up the notes on Dirichlet's Box Principle and
Chebyshev).

\section{Notation}


\ben

\item $\W$ : the set of whole numbers: $\{1, 2, 3, 4, \dots \}$.

\item $\N$ : the set of natural numbers: $\{0, 1, 2, 3, \dots \}$.

\item $\Z$ : the set of integers: $\{ \dots, -2, -1, 0, 1, 2,
\dots \}$.

\item $\Q$: the set of rational numbers: $\{x: x =
\frac{p}{q}, p,q \in \Z, q \neq 0\}$.

\item $\R$: the set of real numbers.

\item $\C$: the set of complex numbers: $\{z: z = x + iy,\ x,
y \in \R\}$.

\item $\Z / n\Z$ : the additive group of integers mod $n$.

\item $(\Z / n\Z)^{*}$ : the multiplicative group of invertible
elements mod $n$.

\item $a|b$ : $a$ divides $b$, \ie the remainder after integer division $\frac{b}{a}$
is 0.

\item $(a,b)$ : greatest common divisor (gcd) of $a$ and $b$,
often written $\gcd(a,b)$.

\item $x \equiv y (\mod n)$ : there exists an integer $a$ such that $x = y + an$.

\item wlog : without loss of generality.

\item \st : such that.

\item $\forall$ : for all.

\item $\exists$ : there exists.

\item big O notation :
$A(x) = O(B(x))$, read ``$A(x)$ is of order $B(x)$'', means
$\exists C>0$ such that $\forall x, |A(x)| \le C\, B(x)$.

\item $|S|$ or $\# S$ : number of elements in the set $S$.

\item $p$ : usually a prime number.

\item $n$ : usually an integer.

\een


\section{Efficient Algorithms}

For computational purposes, often having an algorithm to compute a
quantity is not enough; we need an algorithm which will compute
\emph{quickly}. Below we study three standard problems, and show
how to either rearrange the operations more efficiently, or give a
more efficient algorithm than the obvious candidate.

\subsection{Polynomial Evaluation}

Let $f(x) = a_n x^n + a_{n-1} x^{n-1} + \cdots + a_1 x + a_0$. The
obvious way to evaluate is to calculate $x^n$ and multiply by
$a_n$ ($n$ multiplications), calculate $x^{n-1}$ and multiply by
$a_{n-1}$ ($n-1$ multiplications) and add, et cetera. There are
$n$ additions and $\sum_{k=0}^n k$ multiplications, for a total of
$n + \frac{n(n+1)}{2}$ operations. Thus, the standard method leads
to $O(n^2)$ computations.

Instead, consider the following:

\be \Bigg( \Big((a_n x + a_{n-1})x + a_{n-2} \Big)x + \cdots +
a_1\Bigg)x + a_0. \ee

For example,

\be 7x^4 + 4x^3 - 3x^2 - 11x + 2 = \Bigg( \Big((7x + 4)x - 3\Big)x
- 11 \Bigg)x + 2. \ee

Evaluating the long way takes $14$ steps; cleverly rearranging
takes $8$ steps.

\begin{exe} Prove that the second method takes at most $2n$ steps to
evaluate $a_n x^n + \cdots a_0$.
\end{exe}

\subsection{Exponentiation}

Consider $x^n$. The obvious way to evaluate involves $n-1$
multiplications. By writing $n$ in base two, we can evaluate $x^n$
in at most $2 \log_2 n$ steps.

Let $k$ be the largest integer such that $2^k \le n$. Then
$\exists a_i \in \{0,1\}$ such that

\be n \ = \ a_k 2^k + a_{k-1} 2^{k-1} + \cdots + a_1 2 + a_0. \ee

It costs $k$ multiplications to evaluate $x^{2^i}$, $i \le k$.
How? Consider $y_0 = x^{2^0}$, $y_1 = y_0 \cdot y_0 = x^{2^0}
\cdot x^{2^0} = x^{2^1}$, $y_2 = y_1 \cdot y_1 = x^{2^2}$, $\dots,
y_k = y^{k-1} \cdot y^{k-1}$ $= x^{2^k}$.

Then

\bea x^n & = & x^{a_k 2^k + a_{k-1} 2^{k-1} + \cdots + a_1 2 +
a_0} \nonumber\\ & = & x^{a_k 2^k} \cdot x^{a_{k-1} 2^{k-1} }
\cdots x^{a_1 2} \cdot x^{a_0} \nonumber\\ & = & \Big( x^{2^k}
\Big)^{a_k} \cdot \Big( x^{2^{k-1}} \Big)^{a_{k-1}} \cdots \Big(
x^{2} \Big)^{a_1} \cdot \Big( x^{1} \Big)^{a_0} \nonumber\\ & = &
y_k^{a_k} \cdot y_{k-1}^{a_{k-1}} \cdots y_1^{a_1} \cdot
y_0^{a_0}. \eea

As each $a_i \in \{0, 1\}$, we have at most $k+1$ multiplications
above (if $a_i = 1$ we have the term $y_i$ in the product, if $a_i
= 0$ we don't).

Thus, it costs $k$ multiplications to evaluate the $x^{2^i}$ ($i
\le k$), and at most another $k$ multiplications to finish
calculating $x^n$. As $k \le \log_2 n$, we see that $x^n$ can be
determined in at most $2 \log_2 n$ steps.

Note, however, that we do need more storage space for this method,
as we need to store the values $y_i = x^{2^i}$, $i \le \log_2n$.

\begin{exe} Instead of expanding $n$ in base two, expand $n$ in
base three. How many calculations are needed to evaluate $x^n$
this way? Why is it preferable to expand in base two rather than
any other base?
\end{exe}


\subsection{Euclidean Algorithm}

The Euclidean Algorithm is an efficient way to determine the
greatest common divisor of $x$ and $y$, denoted $\gcd(x,y)$ or
$(x,y)$. Without loss of generality, assume $1 < x < y$.

The obvious way to determine $\gcd(x,y)$ is to divide $x$ and $y$
by all positive integers up to $x$. This takes at most $2x$ steps.

Let $[z]$ denote the greatest integer less than or equal to $z$.
We write

\be y = \frac{y}{x} \cdot x + r_1, \ 0 \le r_1 < x. \ee

\begin{exe} Prove that $r_1 \in \{0, 1, \dots, x-1\}$.
\end{exe}

\begin{exe} Prove $\gcd(x,y) = \gcd(r_1,x)$. Hint: $r_1 = y -
\frac{y}{x} \cdot x$.
\end{exe}

We proceed in this manner until $r_k$ equals zero or one. As each
execution results in $r_i < r_{i-1}$, we proceed at most $x$ times
(although later we prove we need to apply these steps at most $2
\log_2 x$ times).

\bea x &=& \frac{x}{r_1} \cdot r_1 + r_2, \ 0 \le r_2 < r_1
\nonumber\\ r_1 &=& \frac{r_1}{r_2} \cdot r_2 + r_3, \ 0 \le r_3 <
r_2 \nonumber\\ r_2 &=& \frac{r_2}{r_3} \cdot r_3 + r_4, \ 0 \le
r_4 < r_3 \nonumber\\ & \vdots & \nonumber\\ r_{k-2} &=&
\frac{r_{k-2}}{r_{k-1}} \cdot r_{k-1} + r_k, \ 0 \le r_k <
r_{k-1}. \eea

\begin{exe} Prove that if $r_k = 0$, then $\gcd(x,y) = r_{k-1}$,
while if $r_k = 1$, then $\gcd(x,y) = 1$.
\end{exe}

We now analyze how large $k$ can be. The key observation is the
following:

\begin{lem}\label{lemnumstepseuclid}Consider three adjacent remainders
in the expansion: $r_{i-1}$, $r_i$ and $r_{i+1}$ (where $y =
r_{-1}$ and $x = r_0$). Then $\gcd(r_i,r_{i-1}) =
\gcd(r_{i+1},r_i)$, and $r_{i+1} < \frac{r_{i-1}}{2}$.
\end{lem}

Proof: We have the following relation:

\be r_{i-1} = \frac{r_{i-1}}{r_i} \cdot r_i + r_{i+1}, \ 0 \le
r_{i+1} < r_i. \ee

If $r_i \le \frac{r_{i-1}}{2}$, then as $r_{i+1} < r_i$, we
immediately conclude that $r_{i+1} < r_i$. If $r_i >
\frac{r_{i-1}}{2}$, then we note that

\be r_{i+1} = r_{i-1} - \frac{r_{i-1}}{r_i} \cdot r_i. \ee

But $\frac{r_{i-1}}{r_i} = 1$ (easy exercise). Thus $r_{i-1} <
\frac{r_{i-1}}{2}$. $\Box$ \\

We count how often we apply Euclid's Algorithm. Going from $(x,y)
= (r_0,r_{-1})$ to $(r_1,r_0)$ costs one application. Every two
applications leads to the first entry in the last pair being at
most half of the second entry of the first pair.

Thus, if $k$ is the largest integer such that $2^k \le x$, we see
we apply Euclid's Algorithm at most $1 + 2k \le 1 + 2\log_2 x$
times. Each application requires one integer division, where the
remainder is the input for the next step.

We have proven

\begin{lem} Euclid's Algorithm requires at most $1 + 2 \log_2 x$
divisions to find the greatest common denominator of $x$ and $y$.
\end{lem}

Let us assume that $r_i = \gcd(x,y)$. Thus, the last equation
before Euclid's Algorithm terminated was

\be r_{i-2} = \frac{ r_{i-2} }{r_{i-1}} \cdot r_{i-1} + r_i, \ 0
\le r_i < r_{i-1}. \ee

Therefore, we can find integers $a_{i-1}$ and $b_{i-2}$ such that

\be r_i = a_{i-1} r_{i-1} + b_{i-2} r_{i-2}. \ee

Looking at the second to last application of Euclid's algorithm,
we find that there are integers $a_{i-2}'$ and $b_{i-3}'$ such
that

\be r_{i-1} = a_{i-2}' r_{i-2} + b_{i-3}' r_{i-3}. \ee

Substituting for $r_{i-1} = r_{i-1}(r_{i-2},r_{i-3})$ in the
expansion of $r_i$ yields that there are integers $a_{i-2}$ and
$b_{i-3}$ such that

\be r_i = a_{i-2} r_{i-2} + b_{i-3} r_{i-3}. \ee

Continuing by induction, and recalling $r_i = \gcd(x,y)$ yields

\begin{lem} There exist integers $a$ and $b$ such that $\gcd(x,y)
= ax + by$. Moreover, Euclid's Algorithm gives a constructive
procedure to find $a$ and $b$. \end{lem}

\begin{exe} Find $a$ and $b$ such that $a \cdot 244 + b \cdot 313 =
\gcd(244,313)$. \end{exe}

\begin{exe} Add details to complete an alternate proof of the
existence of $a$ and $b$ with $ax + by = \gcd(x,y)$:

\ben

\item Let $d$ be the smallest positive value attained by $ax + by$
as we vary $a, b \in \Z$. Such a $d$ exists: consider $(a,b) =
(1,0)$ or $(0,1)$. Thus, $d = ax + by$. We now show $d =
\gcd(x,y)$.

\item $\gcd(x,y) | d$.

\item Let $e = Ax + By > 0$. Then $d|e$. Therefore, for any choice
of $A, B \in \Z$, $d|(Ax+By)$.

\item $d|x$ and $d|y$ (consider clever choices of $A$ and $B$; one
choice gives $d|x$, one gives $d|y$). Therefore $d| \gcd(x,y)$. As
we've shown $\gcd(x,y)|d$, this completes the proof.

\een

Note this is a non-constructive proof. By minimizing $ax+by$, we
obtain $\gcd(x,y)$, but we have no idea how many steps is
required. Prove that a solution will be found either among pairs
$(a,b)$ with $a \in \{1, \dots, y-1\}$ and $-b \in \{1, \dots,
x-1\}$, or $-a \in \{1, \dots, y-1\}$ and $b \in \{1, \dots,
x-1\}$.

\end{exe}

\section{Probabilities of Discrete Events}

\subsection{Introduction}

Let $\Omega = \{\omega_1, \omega_2, \omega_3, \dots \}$ be an at
most countable set of events. We call $\Omega$ the \textbf{sample
(or outcome) space}. We call the elements $\omega \in \Omega$ the
\textbf{events}. Let $x: \Omega \rightarrow \R$. That is, for each
event $\omega \in \Omega$, we attach a real number $x(\omega)$. We
call $x$ a \textbf{random variable}.

\begin{exa} Flip a fair coin $3$ times. The possible
outcomes are $\Omega = \{HHH, HHT, HTH, THH, HTT, THT, TTH,
TTT\}$. One possible random variable is $x(\omega)$ equals the
number of heads in $\omega$. Thus, $x(HHT) = 2$ and $x(TTT) = 0$.
\end{exa}

\begin{exa} Let $\Omega$ be the space of all flips of a fair coin where
all but the last flip are tails, and the last is a head. Thus,
$\Omega = \{H, TH, TTH, TTTH, \dots \}$. One possible random
variable is $x(\omega)$ is the number of tails; another is
$x(\omega)$ equals the number of the flip which is a head.
\end{exa}

We say $p(\omega)$ is a \textbf{probability function} on $\Omega$
if

\ben
\item $0 \le p(\omega_i) \le 1$ for all $\omega_i \in \Omega$.

\item $p(\omega) = 0$ if $\omega \not\in \Omega$.

\item $\sum_i p(\omega_i) = 1$.
\een

We call $p(\omega)$ the probability of event $\omega$.

Often, we have a random variables where $x(\omega) = \omega$. In a
convenient abuse of notation, we write $X$ for $\Omega$ and $x$
for $x(\omega)$ and $\omega$. For example, consider two rolls of a
fair die. Let $X$ be the result of the first roll, and $Y$ of the
second. Then the sample space is $X = Y = \{1, 2, 3, 4, 5, 6\}$.

In general, consider $X$ and $Y$ with $x_i$ occurring with
probability $p(x_i)$ and $y_j$ occurring with probability
$q(y_j)$. We analyze the \textbf{joint probability $r(x,y)$} of
observing $x$ and $y$.

$X$ and $Y$ are \textbf{independent} if $\forall x, y$, $r(x,y) =
p(x)q(y)$. In the example of rolling a fair die twice, $r(x,y) =
p(x)q(y) = \frac{1}{6}\cdot \frac{1}{6}$ if $x, y \in X = Y$, and
$0$ otherwise.


\begin{exe} Consider again two rolls of a fair die. Now, let $X$ represent the
first roll, and $Y$ the sum of the first two rolls. Prove $X$ and
$Y$ are not independent.
\end{exe}

Events $X_1$ through $X_N$ are \textbf{independent} if
$p(x_1,\dots, x_N) = p_1(x_1)\cdots p(x_N)$.

\begin{exe} Construct three events such that any two are
independent, but all three are not independent. Hint: roll a fair
die twice.
\end{exe}

\subsection{Means}

If $x(\omega) = \omega$, the \textbf{mean (or expected value)} of
an event $x$ is defined by

\be \bar{x} \ = \ \sum_i x_i p(x_i). \ee

More generally, for a sample space $\Omega$ with events $\omega$
and a random variable $x(\omega)$, we have

\be \bar{x}(\omega) \ = \ \sum_i x(\omega_i) p(\omega_i). \ee

For example, the mean of one roll of a fair die is $3.5$.

\begin{exe} Let $X$ be the number of tosses of a fair coin needed
before getting the first head. Thus, $X = \{1, 2, \dots \}$.
Calculate $p(x_i)$ and $\bar{x}$. We could let $\Omega$ be the
space of all tosses of a fair coin where all but the last toss are
tails, and the last toss is a head. Then $x(\omega)$ is the number
of tosses of $\omega$.
\end{exe}

Instead of writing $\bar{x}$, we often write $E[x]$ or $E[X]$,
read as \textbf{the expected value of $x$ or $X$}. More generally,
we would have $\bar{x}(\omega)$ and $E[x(\omega)]$.

\textbf{The $k^{th}$ moment of $X$} is the expected value of
$x^k$:

\be E[x^k] \ = \ \sum_i x_i^k p(x_i) \ee

or

\be E[x^k(\omega)] \ = \ \sum_i x^k(\omega_i) p(\omega_i). \ee


\begin{lem}[Additivity of the Means]\label{lemmeans} Let $X$ and $Y$ be two
independent events with joint probability $r(x,y) = p(x)q(y)$. Let
$z = x + y$. Then $E[z] = E[x+y] = E[x] + E[y]$.
\end{lem}

Proof:

\bea E[x+y] &=& \sum_{(i,j)} (x_i + y_j) r(x_i,y_j) \nonumber\\
&=& \sum_i \sum_j (x_i + y_j) p(x_i) q(y_j) \nonumber\\ &=& \sum_i
\sum_j x_i p(x_i) q(y_j) + \sum_i \sum_j y_j p(x_i) q(y_j)
\nonumber\\ &=&  \sum_i x_i p(x_i) \sum_j q(y_j) + \sum_i p(x_i)
\sum_j y_j q(y_j) \nonumber\\ &=& E[x] \cdot 1 + 1 \cdot E[y] =
E[x] + E[y]. \eea

The astute reader may notice that some care is needed to
interchange the order of summations. If $\sum_i \sum_j |x_i y_j|
r(x_i,y_j) < \infty$, then Fubini's Theorem is applicable, and we
may interchange the summations at will.

We used the two events were independent to go from $\sum_{(i,j)}
x_i r(x_i,y_j)$ to $\sum_i x_i p(x_i) \sum_j q(y_j) = E[x]$. Lemma
\ref{lemmeans} is true even if the two events are not independent.

If the events are not independent, we encounter sums like $\sum_i
\sum_j x_i r(x_i,y_j)$; however, $\sum_j r(x_i,y_j) = p(x_i)$.
Why? By summing over all possible $y$, we are asking what is the
probability that $x = x_i$; we do not care what $y$ is. Thus,
$\sum_i \sum_j x_i r(x_i,y_j) = \sum_i x_i p(x_i) = E[x]$, and
similarly for the other piece.

\begin{exe} Write out the proof of the generalization of Lemma
\ref{lemmeans}, where $X$ and $Y$ are not assumed independent.
\end{exe}

Given an outcome space $X = \{x_1, x_2, \dots \}$ with
probabilities $p(x_i)$, let $aX$ be shorthand for the event $a$
times $X$ with outcome space $\{ax_1, ax_2, \dots \}$ and
probabilities $p_a(ax_i) = p(x_i)$.

\begin{lem}\label{lemmeanofsum} Let $X_1$ through $X_N$ be a finite
collection of independent events. Let $a_1$ through $a_N$ be real
constants. Then \be E[a_1 x_1 + \cdots + a_N x_N] = a_1 E[x_1] +
\cdots + a_N E[x_N]. \ee
\end{lem}

\begin{lem}\label{lemmeanofproduct} Let $X$ and $Y$ be independent
events. Then $E[xy] = E[x]E[y]$.
\end{lem}

\begin{exe} Prove Lemmas \ref{lemmeanofsum} and
\ref{lemmeanofproduct}. \end{exe}


\subsection{Variances}

The \textbf{variance} $\sigma^2_x$ (and its square-root, the
\textbf{standard deviation} $\sigma_x$) measure how spread out a
probability distribution is. Assume $x(\omega) = \omega$. Given an
event $X$ with mean $\bar{x}$, we define the standard deviation
$\sigma^2_x$ by

\be\label{eqdefvariance}  \sigma^2_x \ = \ \sum_i (x_i -
\bar{x})p(x_i). \ee

More generally, given a sample space $\Omega$, events $\omega$,
and a random variable $x:\Omega \rightarrow \R$,

\be \sigma^2_{x(\omega)} \ = \ \sum_i \Big( x(\omega_i) -
\bar{x}(\omega) \Big) p(\omega_i). \ee

\begin{exe} Let $X = \{0, 25, 50, 75, 100\}$ with probabilities
$\{.2, .2, .2, .2, .2\}$. Let $Y$ be the same outcome space, but
with probabilities $\{.1, .25, .3, .25, .1\}$. Calculate the means
and the variances of $X$ and $Y$.
\end{exe}

For computing variances, instead of equation \ref{eqdefvariance}
one often uses

\begin{lem} $\sigma^2_x = E[x^2] - E[x]^2$. \end{lem}

Proof: Recall $\bar{x} = E[x]$. Then

\bea \sigma^2_x & = & \sum_i \Big(x_i - E[x]\Big)^2 p(x_i)
\nonumber\\ & = & \sum_i (x_i^2 - 2 x_i E[x] +  E[x]^2) p(x_i)
\nonumber\\ & = & \sum_i x_i^2 p(x_i) - 2E[x]\sum_i x_i p(x_i) +
E[x]^2 \sum_i p(x_i) \nonumber\\ &=& E[x^2] - 2E[x]^2 + E[x]^2 =
E[x^2] - E[x]^2. \eea

The main result on variances is

\begin{lem}[Variance of a Sum] Let $X$ and $Y$ be two independent
events. Then $\sigma^2_{x+y} = \sigma^2_x + \sigma^2_y$.
\end{lem}

Proof: We constantly use the expected value of a sum of
independent events is the sum of expected values (Lemma
\ref{lemmeans} and Lemma \ref{lemmeanofsum}).

\bea \sigma^2_{x+y} &=& E[(x+y)^2] - E[(x+y)]^2 \nonumber\\ &=&
E[x^2 + 2xy + y^2] - \Big(E[x] + E[y]\Big)^2 \nonumber\\ &=&
\Big(E[x^2] + 2E[xy] + E[y^2]\Big) - \Big(E[x]^2 + 2E[x]E[y] +
E[y]^2\Big) \nonumber\\ &=& \Big(E[x^2] - E[x]^2\Big) +
\Big(E[y^2] - E[y]^2\Big) + 2\Big(E[xy] - E[x]E[y] \Big)
\nonumber\\ &=& \sigma^2_x + \sigma^2_y + 2\Big(E[xy] - E[x]E[y]
\Big). \eea

By Lemma \ref{lemmeanofproduct}, $E[xy] = E[x]E[y]$, completing
the proof.

\begin{lem}\label{lemumvariances} Consider $n$ independent copies of
the same event (for example, $n$ flips of a coin or $n$ rolls of a
die). Then $\sigma_{nx} = \sqrt{n} \sigma_x$.
\end{lem}

\begin{exe} Prove Lemma \ref{lemumvariances}. \end{exe}


Note that, if the event $X$ has units of meters, then the variance
$\sigma^2_x$ has units meters-squared, and the standard deviation
$\sigma_x$ and the mean $\bar{x}$ have units meters. Thus, it is
the standard deviation that gives a good measure of the deviations
of an event around the mean.

There are, of course, alternate measures one can use. For example,
one could consider

\be \sum_i (x_i - \bar{x}) p(x_i). \ee

Unfortunately, this is a signed quantity, and large positive
deviations can cancel with large negatives. This leads us to
consider

\be \sum_i |x_i - \bar{x}| p(x_i). \ee

While this has the advantage of avoiding cancellation of errors
(as well as having the same units as the events), the absolute
value function is not a good function analytically. For example,
it is not differentiable. This is primarily why we consider the
standard deviation (the square-root of the variance).

\begin{exe} Consider the following set of data: for $i \in \{1,
\dots, n\}$, given $x_i$ one observes $y_i$. Believing that $X$
and $Y$ are linearly related, find the best fit straight line.
Namely, determine constants $a$ and $b$ that minimize the error
(calculated via the variance)

\be \sum_{i=1}^n \Big(y_i - (ax_i + b) \Big)^2 \ = \ \sum_{i=1}^n
\Big(\mbox{Observed}_i - \mbox{Predicted}_i\Big)^2. \ee

Hint: use Multi-variable Calculus to find linear equations for $a$
and $b$, and then solve with Linear Algebra.

If instead of measuring total error by the squares of the
individual error (for example, using the absolute value), closed
form expressions for $a$ and $b$ become significantly harder.

If one requires that $a = 0$, show that the $b$ leading to least
error is $b = \bar{y} = \frac{1}{n} \sum_i y_i$.

\end{exe}

\subsection{Random Walks}

Consider the classical problem of a drunk staggering home from a
lamp post late at night. We flip a fair coin $N$ times. With
probability $\foh$ we get heads (tails). For each head (tail) the
drunk staggers one unit to the right (left). How far do we expect
the drunk to be?

It is very unlikely the drunk will be very far to the left or
right.

\begin{exe} Let $x$ be $+1$ if we flip a head, $-1$ for a tail.
For a fair coin, prove $E[x] = 0$, $\sigma^2_x = 1$, $\sigma_x =
1$.
\end{exe}

\begin{exe} Let $p_N(y)$ be the probability that after $N$ flips of
a fair coin, the drunk is $y$ units to the right of the origin
(lamp post).

\ben

\item Prove $p_N(y) = p_N(-y)$.

\item Consider $N = 2M$. Prove $p_{2M}(2k) = {2M \choose M + k}
\frac{1}{2^{2M}}$, where ${n \choose r}  = \frac{n!}{r!(n-r)!} $

\item Use Stirling's formula ($n! \approx n^n e^{-n}\sqrt{2\pi
n}$ $= \sqrt{2\pi} n^{n+\foh} e^{-n}$) to approximate $p_N(y)$.

\een
\end{exe}

Label the coin tosses $X_1$ through $X_N$. Let $X$ denote a
generic toss of the coin, and $Y_N$ be the distance of the
drunkard after $N$ tosses. By Lemma \ref{lemmeanofsum}, $E[y_N] =
E[x_1 + \cdots + x_N] = E[x_1] + \cdots + E[x_N]$. As each $E[x_i]
= E[x] = 0$, $E[y_N] = 0$.

Thus, we expect the drunkard to be at the lamp post. How spread
out is his expected position? By Lemma \ref{lemumvariances},

\be \sigma_{y_N} = \sigma_{N x} = \sqrt{N} \sigma_x = \sqrt{N}.\ee

This means that a \emph{typical} distance from the origin is
$\sqrt{N}$. This is called a \emph{diffusion process} and is very
common in the real world.


\subsection{Bernoulli Process}

Recall ${N \choose r}  = \frac{N!}{r!(N-r)!}$ is the number of
ways to choose $r$ objects from $N$ objects when order does not
matter. Consider $n$ independent repetitions of an event with only
two possible outcomes. We typically call one outcome
\textbf{success} and the other \textbf{failure}, the event a
\textbf{Bernoulli Trial}, and a collection of independent
Bernoulli Trials a \textbf{Bernoulli Process}.

In each Bernoulli Trial, let there be probability $p$ of success
and $q = 1-p$ of failure. Often, we represent a success with $1$
and a failure with $0$.

\begin{exe} For a Bernoulli Trial, show $\bar{x} = p$, $\sigma^2_x
= pq$, and $\sigma_x = \sqrt{pq}$. \end{exe}

Let $Y_N$ be the number of successes in $N$ trials. Clearly, the
possible values are $Y_N = \{0, 1, \cdots, N\}$. We analyze
$p_N(k)$. Rigorously, the sample space $\Omega$ is all possible
sequences of $N$ trials, and the random variable $y_N: \Omega
\rightarrow \R$ is given by $y_N(\omega)$ equals the number of
successes in $\omega$.

If $k \in Y_N$, we need $k$ successes and $N-k$ failures. We don't
care what order we have them (ie, if $k = 4$ and $N = 6$ then
$SSFSSF$ and $FSSSSF$ both contribute). Each such string of $k$
successes and $N-k$ failures has probability of $p^k \cdot
(1-p)^{N-k}$. There are $N \choose k$ such strings.

Thus, $p_N(k) = {N \choose k} p^k \cdot (1-p)^{N-k}$ if $k \in
\{0,1,\cdots, N\}$ and $0$ otherwise.

By clever algebraic manipulations, one can directly evaluate the
mean $\overline{y_N}$ and the variance $\sigma^2_{y_N}$; however,
Lemmas \ref{lemmeanofsum} and \ref{lemumvariances} allow one to
calculate both quantities immediately, once one knows the mean and
variance for one occurrence.

\begin{lem}\label{lembernoulli} For a Bernouilli Process with $N$
trials, each having probability $p$ of success, the expected
number of successes is $\overline{y_N} = Np$, and the variance is
$\sigma^2_{y_N} = Npq$.
\end{lem}

\begin{exe} Prove Lemma \ref{lembernoulli}. \end{exe}

Consider the following problem: Let $Z = \{0, 1, 2, \dots \}$ be
the number of trials before the first success. What is $\bar{z}$
and $\sigma^2_z$?

First, we determine $p(k)$, the probability that the first success
occurs after $k$ trials. Clearly this probability is non-zero only
for $k$ a positive integer, in which case the string of results
must be $k-1$ failures followed by $1$ success. Therefore,

\be p(k) \ = \ p \cdot (1-p)^{k-1} \ \mbox{if} \ k \in
\{1,2,\dots\}, \ \mbox{and} \ 0 \ \mbox{otherwise}. \ee

To determine the mean $\bar{z}$ we must evaluate

\bea \bar{z} & \ = \ & \sum_{k=1}^\infty k \cdot  p \cdot
(1-p)^{k-1} \nonumber\\ & \ = \ & p \sum_{k=1}^\infty k q^{k-1}, \
0 < q = 1-p < 1. \eea

Consider the geometric series

\be f(q) \ = \ \sum_{k=0}^\infty q^k\ = \ \frac{1}{1-q}. \ee

A careful analysis shows we can differentiate term by term if $0
\le q < 1$. Then

\be f'(q) \ = \ \sum_{k=0}^\infty k q^{k-1} \ = \
\frac{1}{(1-q)^2}. \ee

Recalling $q = 1-p$ and substituting yields

\bea \bar{z} & \ = \ & p \sum_{k=1}^\infty k q^{k-1} \nonumber\\ &
\ = \ & \frac{p}{\Big(1 - (1-p)\Big)^2} \ = \ \frac{1}{p}. \eea

Differentiating under the summation sign is a powerful tool in
Probability Theory.

\begin{exe} Calculate $\sigma^2_z$. Hint: differentiate $f(q)$ twice.
\end{exe}

\subsection{Poisson Distribution}

Divide the unit interval into $N$ equal pieces. Consider $N$
independent Bernoulli Trials, one for each sub-interval. If the
probability of a success is $\frac{\lambda}{N}$, then by Lemma
\ref{lembernoulli} the expected number of successes is $N \cdot
\frac{\lambda}{N} = \lambda$.

We consider the limit as $N \rightarrow \infty$. Obviously, we
still expect $\lambda$ successes in each interval, but what is the
probability of $3\lambda$ successes? How long do we expect to wait
between successes?

We call this a \textbf{Poisson process with parameter $\lambda$}.
For example, look at the midpoints of the $N$ intervals. At each
midpoint we have a Bernoulli Trial with probability of success
$\frac{\lambda}{N}$ and failure $1 - \frac{\lambda}{N}$.

We determine the $N \rightarrow \infty$ limits. For fixed $N$, the
probability of $k$ successes in a unit interval is

\bea p_N(k) &=& {N \choose k} \Big( \frac{\lambda}{N} \Big)^k
\Big( 1 - \frac{\lambda}{N} \Big)^{N-k} \nonumber\\ &=&
\frac{N!}{k!(N-k)!} \frac{\lambda^k}{N^k} \Big(1 -
\frac{\lambda}{N} \Big)^{N-k} \nonumber\\ &=& \frac{N \cdot (N-1)
\cdots (N-k+1)}{N \cdot N \cdots N} \frac{\lambda^k}{k!} \Big(1 -
\frac{\lambda}{N} \Big)^N \Big(1 - \frac{\lambda}{N} \Big)^{-k}
\nonumber\\ &=& 1 \cdot \Big(1 - \frac{1}{N}\Big) \cdots \Big(1 -
\frac{k-1}{N}\Big) \frac{\lambda^k}{k!} \Big(1 - \frac{\lambda}{N}
\Big)^N \Big(1 - \frac{\lambda}{N} \Big)^{-k}. \eea



For fixed, finite $k$, as $N \rightarrow \infty$, the first $k$
factors in $p_N(k)$ tend to $1$, $\Big(1 - \frac{\lambda}{N}
\Big)^N \rightarrow e^{-\lambda}$, and $\Big(1 - \frac{\lambda}{N}
\Big)^{-k} \rightarrow 1$.

Thus, we are led to the \textbf{Poisson Distribution}: Given a
parameter $\lambda$ (interpreted as the expected number of
occurrences per unit interval), the probability of $k$ occurrences
in a unit interval is $p(k) = \frac{\lambda^k}{k!}e^{-\lambda}$
for $k \in \{0, 1, 2, \dots \}$.

\begin{exe} Check that $p(k)$ given above is a probability
distribution. Namely, show $\sum_{k\ge 0} p(k) = 1$. \end{exe}

\begin{exe} Show, for the Poisson Distribution, that the mean
$\bar{x} = \lambda$ and the variance $\sigma^2_x = \lambda$. Hint:
let

\be f(\lambda) \ = \ \sum_{k=0}^\infty \frac{\lambda^k}{k!} \ = \
e^\lambda. \ee

Differentiate once to determine the mean, twice to determine the
variance.

\end{exe}

\subsection{Continuous Poisson Distribution}

We calculate a very important quantity related to the Poisson
Distribution (with parameter $\lambda$), namely, how long does one
expect to wait between successes?

We've discussed that we expect $\lambda$ successes per unit
interval, and we've calculated the probability of $k$ successes
per unit interval.

Start counting at $0$, and assume the first success is at $x$.
What is $p_S(x)$? As before, we divide each unit interval into $N$
equal pieces, and consider a Bernoulli Trial at the midpoint of
each sub-interval, with probability $\frac{\lambda}{N}$ of
success.

We have approximately $\frac{x-0}{1/N} = Nx$ midpoints from $0$ to
$x$ (with $N$ midpoints per unit interval). Let $\lceil y \rceil$
be the smallest integer greater than or equal to $y$. Then we have
$\lceil Nx \rceil$ midpoints, where the results of the Bernoulli
Trials of the first $\lceil Nx \rceil - 1$ midpoints are all
failures and the last is a success.

Thus, the probability of the first success occuring in an interval
of length $\frac{1}{N}$ containing $x$ (with $N$ divisions per
unit interval) is

\bea p_{N,S}(x) & \ = \ & \Bigg(1 - \frac{\lambda}{N}
\Bigg)^{\lceil Nx \rceil - 1} \cdot \Bigg( \frac{\lambda}{N}
\Bigg)^1. \eea

For $N$ large, the above converges to $e^{-\lambda x}
\frac{\lambda}{N}$. \\

We say $p(x)$ is a \textbf{continuous probability distribution on
$\R$} if

\ben
\item $p(x) \ge 0$ for all $x \in \R$.
\item $\int_{\R} p(x)dx = 1$.
\item $\mbox{Probability}(a \le x \le b) = \int_a^b p(x)dx$.
\een

We call $p(x)$ the \textbf{probability density function}.

Thus, as $N \rightarrow \infty$, we see the probability density
function $p_S(x) = \lambda e^{-\lambda x}$. In the special case of
$\lambda = 1$, we get the standard exponential decay, $e^{-x}$.

For instance, let $\pi(M)$ be the number of primes that are at
most $M$. The Prime Number Theorem states $\pi(M) = \frac{M}{\log
M}$ plus lower order terms.

Thus, the average spacing between primes around $M$ is about $\log
M$. We can model the distribution of primes as a Poisson Process,
with parameter $\lambda = \lambda_M = \frac{1}{\log M}$. While
possible locations of primes (obviously) is discrete (it must be
an integer, and in fact the location of primes aren't
independent), a Poisson model often gives very good heuristics.

We can often renormalize so that $\lambda = 1$. This is denoted
\textbf{unit mean spacing}. For example, one can show the $M^{th}$
prime $p_M$ is about $M \log M$, and spacings between primes
around $p_M$ is about $\log M$. Then the normalized primes, $q_M
\approx \frac{p_M}{\log M}$ will have unit mean spacing and
$\lambda = 1$.


\subsection{Central Limit Theorem} $X_1, X_2, X_3, \dots $ are an
infinite sequence of random variables such that the $X_j$ are
independent identically distributed random variables (abbreviated
i.i.d.r.v.) with $E[X_j] = \bar{X_j} = 0$ (can always renormalize
by shifting) and variance $E[X_j^2] = 1$. Let $S_N = \sum_{j=1}^N
X_j$.

\begin{thm} Fix $-\infty < a \le b < \infty$. Then as $N
\rightarrow \infty$,

\be \mbox{Prob}\Big( \frac{S_N}{\sqrt{N}} \in [a,b] \Big)
\rightarrow \frac{1}{\sqrt{2\pi}} \int_a^b e^{-\frac{t^2}{2}} dt.
\ee
\end{thm}

The probability function is called the Gaussian or the Normal
distribution. This is the universal curve of probability. Note how
robust the Central Limit Theorem is: it doesn't depend on fine
properties of the $X_j$.



\section{Algebraic and Transcendental Numbers}

\subsection{Definitions}

A function $f:A \rightarrow B$ is \textbf{one-to-one} if $f(x) =
f(y)$ implies $x = y$; $f$ is \textbf{onto} if given any $b\in B$,
$\exists a\in A$ with $f(a) = b$. $f$ is a \textbf{bijection} if
$f$ is a one-to-one and onto function.

We say two sets $A$ and $B$ \textbf{have the same cardinality}
(ie, are the same size) if there is a bijection $f: A \rightarrow
B$. We denote this by $|A| = |B|$. If $A$ has finitely many
elements (say $n$ elements), $A$ is \textbf{finite} and $|A| = n <
\infty$.

\begin{exe} Show two finite sets have the same cardinality if and
only if they have the same number of elements. \end{exe}

\begin{exe} If $f$ is a bijection from $A$ to $B$, prove there is a
bijection $g = f^{-1}$ from $B$ to $A$. \end{exe}

$A$ is \textbf{countable} if there is a bijection between $A$ and
the integers $\Z$. $A$ is \textbf{at most countable} if $A$ is
either finite or countable.

Recall a binary relation $R$ is an \textbf{equivalence relation}
if

\ben
\item Reflexive: $R(x,x)$ is true ($x$ is equivalent to $x$).
\item Symmetric: $R(x,y)$ true implies $R(y,x)$ is true (if $x$ is equivalent
to $y$ then $y$ is equivalent to $x$).
\item Transitive: $R(x,y)$ and $R(y,z)$ true imply $R(x,z)$ is true (if $x$ is
equivalent to $y$ and $y$ is equivalent to $z$, then $x$ is
equivalent to $z$). \een

We often denote equivalence by $\equiv$ or $=$.

\begin{exe} Let $x,y,z \in \Z$, and let $n \in \Z$ be given. Define
$R(x,y)$ to be true if $n|(x-y)$ and false otherwise. Prove $R$ is
an equivalence relation. We denote it by $x \equiv y$. \end{exe}

\begin{exe} Let $x, y, z$ be subsets of $X$ (for example, $X = \Q,
\R, \C, \R^n$, et cetera). Define $R(x,y)$ to be true if $|x| =
|y|$ (the two sets have the same cardinality), and false
otherwise. Prove $R$ is an equivalence relation. \end{exe}


\subsection{Countable Sets}

We show several sets are countable. Consider the set of
non-negative integers $\N$. Define $f:\N \rightarrow \Z$ by $f(2n)
= n$, $f(2n+1) = -n-1$. By inspection, we see $f$ gives the
desired bijection.

Consider $\W = \{1,2,3,\dots \}$ (the positive integers). Then
$f:\W \rightarrow \Z$ defined by $f(2n) = n$, $f(2n+1)=-n$ gives
the desired bijection.

Thus, we have proved

\begin{lem} To show a set $S$ is countable, it is sufficient to
find a bijection from $S$ to either $\Z$, $\N$ or $\W$.
\end{lem}

We need the intuitively plausible

\begin{lem}\label{lemcardsubsetone} If $A \subset B$, then $|A| \le |B|$. \end{lem}

We can then prove

\begin{lem}\label{lemcardsubsettwo} If $f:A \rightarrow C$ is a one-to-one
function (not necessarily onto), then $|A| \le |C|$. Further, if
$C \subset A$, then $|A| = |C|$. \end{lem}

\begin{exe} Prove Lemmas \ref{lemcardsubsetone} and
\ref{lemcardsubsettwo}. \end{exe}

If $A$ and $B$ are sets, the \textbf{cartesian product} $A \times
B$ is $\{(a,b):a\in A, b\in B\}$.

\begin{thm}\label{thmcountableab} If $A$ and $B$ are countable, so is
$A \cup B$ and $A \times B$. \end{thm}

Proof: we have bijections $f:\N \rightarrow A$ and $g:\N
\rightarrow B$. Thus, we can label the elements of $A$ and $B$ by

\bea A & \ = \ & \{a_0, a_1, a_2, a_3, \dots \} \nonumber\\  B & \
= \ & \{b_0, b_1, b_2, b_3, \dots \}. \eea

Assume $A \cap B$ is empty. Define $h:\N \rightarrow A \cup B $ by
$h(2n) = a_n$ and $h(2n+1) = b_{n-1}$. We leave to the reader the
case when $A \cap B$ is not empty.

To prove the second claim, consider the following function $h: W
\rightarrow A \times B$:

\bea & & h(1) = (a_0,b_0) \nonumber\\ & & h(2) = (a_1,b_0), h(3) =
(a_1,b_1), h(4) = (a_0,b_1) \nonumber\\ & & h(5) = (a_2,b_0), h(6)
=  (a_2,b_1), h(7) = (a_2,b_2), h(8) = (a_1,b_2), h(9) = (a_0,b_2)
\nonumber\\ & & \ \ \ \ \ \  \ \vdots \nonumber\\ & & h(n^2+1) =
(a_n,b_0), h(n^2+2) = (a_n,b_{n-1}), \dots, \nonumber\\ & & \ \ \
\ \ \ \ \ \ \ \ \ \ \ \ \ h(n^2+n+1) = (a_n,b_n), h(n^2+n+2) =
(a_{n-1},b_n), \dots, \nonumber\\ & & \ \ \ \ \ \ \ \ \ \ \ \ \ \
\ \ h((n+1)^2) = (a_0,b_n) \nonumber\\ & & \ \ \ \ \ \  \ \vdots
\eea

Basically, look at all pairs of integers in the first quadrant
(including those on the axes). Thus, we have pairs $(a_x,b_y)$.
The above function $h$ starts at $(0,0)$, and then moves through
the first quadrant, hitting each pair once and only once, by going
up and over. Draw the picture! $\Box$

\begin{cor}\label{corcountable} Let $A_i$ be countable $\forall i \in \N$. Then
for any $n$, $A_1 \cup \cdots \cup A_n$ and $A_1 \times \cdots
\times A_n$ are countable, where the last set is all $n$-tuples
$(a_1,\dots,a_n)$, $a_i \in A_i$. Further, $\cup_{i=0}^\infty A_i$
is countable. If each $A_i$ is at most countable, then
$\cup_{i=0}^\infty A_i$ is at most countable.
\end{cor}

\begin{exe} Prove Corollary \ref{corcountable}. Hint: for $\cup_{i=0}^{\infty} A_i$,
mimic the proof used to show $A \times B$ is countable. \end{exe}

As the natural numbers, integers and rationals are countable, by
taking each $A_i = \N$, $\Z$ or $\Q$ we immediately obtain

\begin{cor}\label{cornzq} $\N^n$, $\Z^n$ and $\Q^n$ are countable. Hint:
proceed by induction. For example write $\Q^{n+1}$ as $\Q^n \times
\Q$. \end{cor}

\begin{exe} Prove there are countably many rationals in the
interval $[0,1]$. \end{exe}

\subsection{Algebraic Numbers}

Consider a polynomial $f(x) = 0$ with rational coefficients. By
multiplying by the least common multiple of the denominators, we
can clear the fractions. Thus, without loss of generality it is
sufficient to consider polynomials with integer coefficients.

The \textbf{algebraic numbers}, $\mathcal{A}$, are the set of all
$x \in \C$ such that there is a polynomial of finite degree and
integer coefficients (depending on $x$, of course!) such that
$f(x) = 0$. The remaining complex numbers are the
\textbf{transcendentals}.

The \textbf{algebraic numbers of degree $n$}, $\mathcal{A}_n$, are
the set of all $x \in \mathcal{A}$ such that

\ben
\item there exists a polynomial with integer coefficients of degree $n$
such that $f(x) = 0$

\item there is no polynomial $g$ with integer coefficients and
degree less than $n$ with $g(x) = 0$.

\een

Thus, $\mathcal{A}_n$ is the subset of algebraic numbers $x$ where
for each $x \in \mathcal{A}_n$, the degree of the smallest
polynomial $f$ with integer coefficients and $f(x) = 0$ is $n$.

\begin{exe} Show the following are algebraic: any rational, the square-root of any
rational, the cube-root of any rational, $r^{\frac{p}{q}}$ where
$r, p, q \in \Q$, $i =\sqrt{-1}$, $\sqrt{3\sqrt{2} - 5}$.
\end{exe}

\begin{thm} The Algebraic Numbers are countable. \end{thm}

Proof: If we show each $\mathcal{A}_n$ is at most countable, then
as $\mathcal{A} = \cup_{n=1}^\infty \mathcal{A}_n$, by Corollary
\ref{corcountable} $\mathcal{A}$ is at most countable.

Recall the \textbf{Fundamental Theorem of Algebra (FTA):} Let
$f(x)$ be a polynomial of degree $n$ with complex coefficients.
Then $f(x)$ has $n$ (not necessarily distinct) roots. Of course,
we will only need a weaker version, namely that the Fundamental
Theorem of Algebra holds for polynomials with integer
coefficients.

Fix an $n \in \N$. We now show $\mathcal{A}_n$ is at most
countable. We can represent every integral polynomial $f(x) = a_n
x^n + \cdots + a_0$ by an $(n+1)$-tuple $(a_0,\dots,a_n)$. By
Corollary \ref{cornzq}, the set of all $(n+1)$-tuples with integer
coefficients ($\Z^{n+1}$) is countable. Thus, there is a bijection
from $\N$ to $\Z^{n+1}$, and we can index each $(n+1)$-tuple $a
\in \Z^{n+1}$:

\be \{a: a \in \Z^{n+1}\} \ = \ \bigcup_{i=1}^\infty \{ \alpha_i
\}, \ee

where each $\alpha_i \in \Z^{n+1}$.

For each tuple $\alpha_i$ (or $a \in \Z^{n+1}$), there are $n$
roots. Let $R_{\alpha_i}$ be the roots of the integer polynomial
associated to $\alpha_i$. The roots in $R_{\alpha_i}$ need not be
distinct, and the roots may solve an integer polynomial of smaller
degree. For example, $f(x) = (x^2 - 1)^4$ is a degree $8$
polynomial. It has two roots, $x = 1$ with multiplicity $4$ and $x
= -1$ with multiplicity $4$, and each root is a root of a degree
$1$ polynomial.

Let $R_n = \{x \in \C: x\ \mbox{is a root of a degree} \ n \
\mbox{polynomial}\}$. One can show that

\be R_n \ = \ \bigcup_{i=1}^\infty R_{\alpha_i} \ \supset \
\mathcal{A}_n. \ee

By Lemma \ref{corcountable}, $R_n$ is countable. Thus, by Lemma
\ref{lemcardsubsetone}, as $R_n$ is at most countable,  $A_n$ is
at most countable.

Therefore, each $\mathcal{A}_n$ is at most countable, so by
Corollary \ref{corcountable} $\mathcal{A}$ is at most countable.
As $\mathcal{A}_1 \supset \Q$ (given $\frac{p}{q} \in \Q$,
consider $qx - p = 0$), $\mathcal{A}_1$ is at least countable. As
we've shown $\mathcal{A}_1$ is at most countable, this implies
$\mathcal{A}_1$ is countable. Thus, $\mathcal{A}$ is countable.
$\Box$


\subsection{Transcendental Numbers}

A set is \textbf{uncountable} if there is no bijection between it
and the rationals (or the integers, or any countable set).

\begin{thm} The set of irrationals in $[0,1]$ is uncountable.
\end{thm}

Proof: Let $I = [0,1] - \Q = \{x: 0 \le x \le 1 \ \mbox{and} \ x
\not\in \Q\}$. Assume that $I$ is countable (the case where $I$ is
finite is even easier).

We can write every number in $I$ in a base two expansion, say $y =
.y_1y_2y_3y_4\cdots$, $y_i \in \{0,1\}$, $y = \sum_i y_i 2^{-1}$.
Certain numbers can be written two different ways. For example,
$0.010011111111111\cdots = .0101$. As we are assuming $I$ is
countable, including both representations of these numbers is
equivalent to taking the union of two countable sets, which by
Theorem \ref{thmcountableab} is countable.

Further, we can add back all the rationals in $[0,1]$, as there
are countably many rationals in $[0,1]$. Call this set $S$ (the
union of the irrationals, the alternate representation of some of
the irrationals, and the rationals). As $X$ is contained in the
union of three at most countable sets (and two are countable), $X$
is countable by Theorem \ref{thmcountableab}.

There is therefore a bijection between $\N$ and $X$. We can
enumerate the elements by $\{x_1, x_2, x_3, \dots \}$.

For each $x_i$, let $.x_{i1}x_{i2}x_{i3}\cdots x_{ii} \cdots $ be
its binary expansion. We list the countable members of $X$:

\bea x_1 & \ = \ & x_{11}x_{12}x_{13} x_{14} \cdots \nonumber\\
x_2 & \ = \ & x_{21}x_{22}x_{23} x_{24} \cdots \nonumber\\ x_3 & \
= \ & x_{31}x_{32}x_{33} x_{34} \cdots \nonumber\\ & \ \vdots \ &
\nonumber\\ x_n & \ = \ & x_{n1}x_{n2}x_{n3} x_{n4} \cdots x_{nn}
\cdots \nonumber\\ & \ \vdots \ & \eea

We construct a real number $x \in [0,1]$ not in $X$. As this was
supposed to be (more than a) complete list of all reals in
$[0,1]$, this will contradict the assumption that $I$ is
countable.

Consider the number $z = .z_1z_2z_3\cdots z_n\cdots$ defined by
$z_n = 1 - x_{nn}$. Can $z$ be one of the numbers in our list? For
example, could $z = x_m$?

No, as they differ in the $m^{th}$ digit. Thus, $z$ is not on our
list, violating the assumption that we had a complete enumeration.
Note we had to be careful and make sure we included all equivalent
ways of writing the same number. Thus, while $z$ disagrees with
the base two expansion of $x_m$, it cannot be an equivalent way of
representing $x_m$, as all equivalent ways of representing $x_m$
are in our list. This is merely an annoying technical detail.

Thus, the set of irrationals in $[0,1]$ is not countable. $\Box$.
\\

The above proof is due to Cantor ($1873-1874$), and is known as
\textbf{Cantor's Diagonalization Argument}. Note Cantor's proof
shows that \emph{most} numbers are transcendental, though it
doesn't tell us \emph{which} numbers are transcendental. We can
easily show many numbers (such as $\sqrt{3 + 2^{\frac{3}{5}}
\sqrt{7}}$) are algebraic. What of other numbers, such as $\pi$
and $e$?

Lambert ($1761$), Legendre ($1794$), Hermite ($1873$) and others
proved $\pi$ irrational;  Legendre ($1794$) also proved $\pi$
irrational. In $1882$ Lindemann proved $\pi$ transcental.

What about $e$? Euler ($1737$) proved that $e$ and $e^2$ are
irrational, Liouville  ($1844$) proved $e$ is not an algebraic
number of degree $2$, and Hermite ($1873$) proved $e$ is
transcendental.

Liouville ($1851$) showed transcendental numbers exist; we will
discuss his construction later.






%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Introduction to Number Theory} \setcounter{equation}{0}

\subsection{Dirichlet's Box Principle}

\begin{defi}[Dirichlet's Box Principle / Pidgeon Hole Principle]
Consider $n$ boxes, and place $n+1$ objects in the $n$ boxes. Then
some box contains at least two objects. \end{defi}

We will use Dirichlet's Box Principle to find good rational
approximations to irrational numbers.

\subsubsection{Approximation by Rationals}

Let $\alpha \in \R - \Q$ be an irrational number. We are looking
for a rational number $\frac{p}{q}$ such that
$\Big|\alpha-\frac{p}{q}\Big|$ is small, so that $\frac{p}{q}$ is
a good rational approximation to $\alpha$.


\begin{lem}
Let $\alpha \in \R -\Q$. Then there exist $p,q\in \Z, q\neq 0$
such that:

\be \Bigg|\alpha-\frac{p}{q}\Bigg| \le \frac{1}{q} \ee
\end{lem}
\emph{Proof.} It is enough to prove this for $\alpha \in (0,1)$.
Let $q\geq 1$ and divide the interval $[0,1)$ into $q$ intervals
$[\frac{p}{q},\frac{p+1}{q})$ of length $\frac{1}{q}$. Then
$\alpha$ belongs to one of these intervals. For some $0<p<q$ we
then have:

\be \alpha\in \Bigg[\frac{p}{q},\frac{p+1}{q}\Bigg) \Rightarrow
\Bigg|\alpha-\frac{p}{q}\Bigg|\leq \frac{1}{q}. \ee

To obtain a better approximation, we start with an irrational
number $\alpha\in (0,1)$ and an integer parameter $Q>1$. As
before, divide the interval $(0,1)$ into $Q$ equal pieces
$(\frac{a}{Q},\frac{a+1}{Q})$. Consider the $Q+1$ numbers inside
the interval $(0,1)$:

\be \{\alpha\}, \{2\alpha\}, ..., \{(Q+1)\alpha\}, \ee

where $\{x\}$ denotes the fractional part of $x$. Letting $[x]$
denote the greatest integer less than or equal to $x$, we have $x
= [x] + \{x\}$.

By Dirichlet's Box Principle, at least two of these numbers, say
$\{q_1\alpha\}$ and $\{q_2\alpha\}$, belong to a common interval
of length $\frac{1}{Q}$. Without loss of generality, we may take
$1\leq q_1<q_2\leq Q+1$.

Hence

\be \Big|\{q_2\alpha\}-\{q_1\alpha\}\Big|\leq \frac{1}{Q} \ee

and

\be |(q_2\alpha -n_2)-(q_1\alpha -n_1)|\leq \frac{1}{Q},\quad
n_i=[q_i\alpha].\ee

Now let $q=q_1-q_2$, $1\leq q\leq Q$ and $p=n_1-n_2\in \Z$. Then


\be \Big|q\alpha -p\Big|\leq \frac{1}{Q}\ee

and hence

\be \Big|\alpha-\frac{p}{q}\Big|\leq \frac{1}{qQ}\le
\frac{1}{q^2}.\ee

We have proven

\begin{thm}
Given $\alpha \in \R$, there exist $p,q\in\Z, q\neq 0$, such that
\be \Big|\alpha-\frac{p}{q}\Big|<\frac{1}{q^2}.\ee
\end{thm}

\subsection{Counting the Number of Primes}

\subsubsection{Euclid}

\begin{lem}[Euclid]
There are infinitely many primes.
\end{lem}

Proof by contradiction: Assume there are only finitely many
primes, say $p_1,p_2,\dots,p_n$. Consider

\be x = p_1p_2..p_n + 1. \ee

$x$ cannot be prime, as we are assuming $p_1$ through $p_n$ is a
complete list of primes. Thus, $x$ is composite, and divisible by
a prime. However, $p_i$ cannot divide $x$, as it gives a remainder
of $1$. Thus, $x$ would have to be divisible by some prime not in
our list, again contradicting the assumption that $p_1$ through
$p_n$ is a complete enumeration of the primes. $\Box$

\begin{exe}\label{pi} Try, using Euclid's argument, to find an explicit lower
bound (as weak as you like) to the function: \be \pi(X) = \#\{p: p
\ \mbox{is prime and} \ p \le X \}. \ee
\end{exe}



\subsubsection{Dirichlet's Theorem}

\begin{thm}[Primes in Arithmetic Progressions] Let $a$ and $b$ be
relatively prime integers. Then there are infinitely many primes
in the progression $an+b$. Further, for a fixed $a$, to first
order all relatively prime $b$ give progressions having the same
number of primes. \end{thm}

Notice that the condition $(a,b)=1$ is necessary. If
$\gcd(a,b)>1$, $an+b$ can never be prime. Dirichlet's remarkable
result is that this condition is also sufficient.

\begin{exe}
Dirichlet's theorem is not easy to prove, but try to prove it in
the particular case $a=4, b=-1$, i.e. for the arithmetic
progression $4n-1$, using an argument similar to Euclid's. Proving
there are infinitely many primes of the form $4n+1$ is a lot
harder.
\end{exe}

\subsubsection{Prime Number Theorem}
\begin{thm}(Prime Number Theorem or PNT)
As $X\rightarrow \infty$, \be \pi(X)\sim \frac{X}{\log X} \ee
\end{thm}
The Prime Number Theorem was proved in 1896 by Jacques Hadamard
and Charles Jean Gustave Nicolas Baron de la Vallee Poussin. Of
course, we need to quantify what $\pi(X)\sim \frac{X}{\log X}$
means. Basically, there is an error function $E(X)$ such that
$|\pi(X) - \frac{X}{\log X}| \le E(X)$, and $E(X)$ grows slower
than $\frac{X}{\log X}$.

A weaker version was proved by Pafnuty Chebyshev (around $1850$).

\begin{thm}[Chebyshev]
There exist explicit positive constants $A$ and $B$ such that, for
$n>30$:

\be \frac{AX}{\log X}\leq \frac {\pi (X)}{X}\leq \frac{BX}{\log
X}. \ee
\end{thm}

Chebyshev showed one can take $A= \log\Big(\frac{2^{\foh}
3^{\frac{1}{3}} 4^{\frac{1}{4}}}{30^{\frac{1}{30}}}\Big) \approx
.921$ and $B= \frac{6A}{5} \approx 1.105$, which are indeed very
close to 1. To highlight the method, we will use cruder arguments
and prove the theorem for a smaller $A$ and a larger $B$.

Chebyshev's argument uses an identity using von Mangoldt's Lambda
function $\Lambda(n)$, where $\Lambda(n) = \log p$ if $m = p^k$
for some prime $p$, and $0$ otherwise.


Define the function

\be T(X) =\sum _{1\leq n\leq X}\Lambda(n)\Bigg[\frac{X}{n}\Bigg]=
\sum _{n\geq 1}\Lambda(n)\Bigg[\frac{X}{n}\Bigg].\ee

\begin{exe} Show that
$T(X)=\sum _{n\leq X}\log n$.
\end{exe}

Now, it is easy to see (compare upper and lower sums) that

\be \sum _{n\leq X}\log n = \int_1^X \log \mbox{t dt} + O(\log X)
=X\log X-X+O(\log X), \ee

giving a good approximation to the function $T(X)$. The trick is
to look at

\begin{equation}
\label{ttrick} T(X)-2T\Big(\frac{X}{2}\Big)=\sum _n
\Lambda(n)\Bigg(\Bigg[\frac{X}{n}\Bigg]-
2\Bigg[\frac{X}{2n}\Bigg]\Bigg)
\end{equation}

By the previous remarks, the $\mbox{LHS}=X\log 2 +O(\log X)$.
Also,

\be\label{eqproblemtodo} \mbox{RHS}\leq \sum_{p\leq X}(\log
p)\frac{\log X}{\log p}= \pi (X)\log X. \ee

Hence we immediately obtain the lower bound:
\begin{equation}
\label{lowbdpi} \pi (X)\geq \frac{X\log 2}{\log X}+O(\log X)
\end{equation}

\begin{exe} Prove the bound in Equation \ref{eqproblemtodo}.
\end{exe}

To obtain an upper bound for $\pi (X)$, we notice that, since
$[2\alpha ]\geq 2[\alpha ]$, the sum in equation (\ref{ttrick})
has only positive terms. By dropping terms we get a lower bound.

\bea T(X)-2T\Big(\frac{X}{2}\Big) &\ge & \sum_{X/2< n\leq X}
\Lambda(n) \Bigg( \Bigg[\frac{X}{n} \Bigg]-2 \Bigg[\frac{X}{2n}
\Bigg] \Bigg) \nonumber\\ & \ge & \sum_{X/2< p\leq X}\log p
\nonumber\\ &
\ge & \log\Big(\frac{X}{2}\Big) \sum_{X/2< p\leq X}1 \nonumber\\
&=& \log\Big(\frac{X}{2}\Big)
\Bigg(\pi(X)-\pi\Big(\frac{X}{2}\Big)\Bigg) \eea

Hence we obtain an upper bound for the number of primes between
$\frac{X}{2}$ and $X$:
\begin{equation}
 \label{bertrand}
\pi(X)-\pi(X/2) \le \frac{X\log 2}{\log (\frac{X}{2})} +O(1)
\end{equation}
Now, if we write inequality (\ref{bertrand}) for $X, \frac{X}{2},
\frac{X}{2^2}, \dots$ we get

\bea \pi(X)-\pi(X/2) & \le & 2\frac{X/2}{\log (X/2)} \nonumber\\
\pi (X/2) - \pi(X/2^2) & \le & 2\frac{X/2^2}{\log (X/2^2)}
\nonumber\\  & \vdots & \nonumber\\  \pi(X/2^{k-1})-\pi(X/2^k) &
\le & 2\frac{X/2^k}{\log (X/2^k)} \eea

as long as $\frac{X}{2^k}\geq 1$, i.e. $k\leq [\log _2X]= k_0$.
Summing the above inequalities we get on the left hand side a
telescoping sum. All the terms cancel, except for the leading term
$\pi(X)$ and $\pi(X/2^{k_0}) = 0$.

Thus

\begin{equation}
\label{ineqpi} \pi (X)\leq 2\sum_{k=1}^{k_0}\frac{X/2^k}{\log
(X/2^k)}
\end{equation}

To evaluate the sum in the above inequality we split it into two
parts, $k$ "small" and $k$ "large". More precisely, let $n_0=\log
_2(X^{1/10})$ so that $2^{n_0} = X^{1/10}$ and note that:

\be \label{nlarge} 2\sum _{k>n_0} \frac{X/2^k}{\log (X/2^k)} \leq
2\sum_{k>n_0} \frac{X}{2^k} \le \frac{2X}{2^{n_0}}
=\frac{2X}{X^{1/10}} = 2X^{9/10}. \ee

Hence the contribution from $k$ "large" is very small compared to
what we expect (i.e. order of magnitude $\frac{X}{\log X}$), or we
can say that the main term comes from the sum over $k$ small.

We now evaluate the contribution from small $k$.

\begin{equation}\label{nsmall}
2\sum_{k=1}^{n_0}\frac{X}{2^k}\frac{1}{\log (X/2^k)} \leq
\frac{2X}{\log (X/2^{n_0})}\sum _{k=1}^{n_0}\frac{1}{2^{n_0}} \leq
\frac{2X}{\log (X^{9/10})} = \frac{20}{9}\frac{X}{\log X}
\end{equation}

 Hence the right hand side of the equation (\ref{ineqpi}) is made
 up of two parts, a main term of size $\frac{BX}{\log X}$ coming
 from equation (\ref{nsmall}), and a lower order term coming
 from equation (\ref{nlarge}).

 For $X$ sufficiently large,

 \begin{equation}
 \label{uppbdpi}
 \pi (X)\leq \frac{BX}{\log X}
 \end{equation}
 where $B$ can be any constant strictly bigger than $\frac{20}{9}$.

To obtain Chebyshev's better constant we would have to work a
little harder along these lines, but it is the same method.

Gathering equations (\ref{lowbdpi}) and (\ref{uppbdpi}) we see we
have proven

\be \frac{AX}{\log X}\leq \pi (X)\leq \frac{BX}{\log X}. \ee

While this is not an asymptotic for $\pi(X)$, it does give the
right order of magnitude for $\pi(X)$, namely $\frac{X}{\log X}$.

\begin{exe} Using Chebyshev's Theorem, Prove Bertrand's Postulate:
for any integer $n \ge 1$, there is always a prime number between
$n$ and $2n$. \end{exe}




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\chapter{More of an Introduction to Graph Theory}

We review some basic definitions of Graph Theory, and prove a
simple result about the size of the eigenvalues of adjacency
graphs. Lecture by Peter Sarnak; notes by Steven J. Miller.

\section{Definitions}

\begin{defi} For a graph $G$, let $V(G)$ be the
set of vertices and $E(G)$ the set of edges (an edge is a pair
$(v,w)$ with $v, w \in V$). We often just write $V$ and $E$.
\end{defi}

\begin{defi} [Connected Graph] A graph $G$ is connected if for any two
vertices $v, w \in G$, there is a path of edges in $E$ starting at
$v$ and ending at $w$.
\end{defi}

\begin{defi}[Boundary] $\partial A = \{v \in V - A:$ there is a $w \in
A$ with $(v,w) \in E$\}. \end{defi}

\begin{defi}[$k$-regular] A graph $G$ is $k$-regular if there are
$k$ edges coming out from each vertex. \end{defi}

\begin{defi}[Expander Graph] Let $G$ be a connected graph
with $n$ vertices. Let $A \subset V(G)$ be any subset of vertices
with $|A| \le \frac{n}{2}$ (ie, at most half of the vertices). We
say $G$ is a $(n,c,k)$ expander if for any such $A$, $|\partial A|
\ge c|A|$.
\end{defi}

\begin{exa} Let $G$ be a $2$-regular graph with $n$ vertices. For
definateness, label the vertices $\{1,2,\dots,n\}$. Let the edges
be $(1,2)$, $(2,3)$, $(3,4), \dots, (n,1)$. Let $A$ be the first
half of the vertices: $A = \{1, 2, \dots,
\Big[\frac{n}{2}\Big]\}$. Then $|\partial A| = 2$, and $G$ is not
an $(n,c,2)$ expander.
\end{exa}


\section{Size of Eigenvalues}

Consider a $3$-regular graph with $n$ vertices. We want the graph
to have certain desirable connectivity properties.

Let $A = (a_{vw})$ is the adjacency matrix attached to the graph
$G$, and $a_{vw} = 1$ if there is an edge from $v$ to $w$ and $0$
otherwise. This is a very sparse matrix (only three non-zero
entries in each row or column). Compute its eigenvalues (real
numbers).

\begin{lem} The eigenvalues $\lambda_i \in [-3,3]$. \end{lem}

Proof: Let $f: V \rightarrow \R$, define the action of the
adjacency matrix $A$ on $f$ by

\be Af(v) = \sum_{ (v,w) \in E } f(w). \ee

As there are finitely many vertices ($n$, in fact), we can regard
the function $f(v)$ as living in $\R^n$, with coordinates
$\Big(f(v_1), \dots, f(v_n)\Big)$.

Suppose $\forall w \in V$, $Af(w) = \lambda f(w)$, $f \neq 0$.
Then $\lambda$ is an eigenvalue (which must be real as $A$ is real
symmetric).

We use the Maximum Modulus Principle. As $f \neq 0$, let $w_0$ be
such that $f(w_0)$ is the maximum value of $f(w)$ (not zero, and
exists as we have finitely many vertices). Then

\be f(w_0) = \frac{1}{\lambda} \sum_{ (w,w_0) } f(w). \ee

If $\lambda > 3$, this cannot happen (we're assuming the graph is
$k$-regular and connected). If $\lambda = 3$, then $f$ is
constant. Working with absolute values, we similarly obtain
$\lambda
> -3$.

\begin{exe} Prove a $k$-regular graph $G$ is connected if and only if $\lambda = k$ is
a simple eigenvalue. \end{exe}

Let $\lambda_1$ be the second largest eigenvalue; $\lambda = k$ is
always an eigenvalue for a $k$-regular connected graph. How big
can the gap be between $\lambda_1$ and $k$?




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\chapter{Linear Algebra Review, especially Spectral Theorem for Real Symmetric Matrice}

We review some basic facts about Linear Algebra and Matrix Groups,
and give an introduction to Random Matrix Theory. Lecture by
Steven J. Miller; notes by Steven J. Miller and Alex Barnett.

\section{Linear Algebra Review}

Matrices can either be thought of as rectangular (often square)
arrays of numbers, or as linear transformations from one space to
another (or possible to the same space). The former picture is the
simplest starting point, but as Professor Sarnak emphasized, it is
the latter, geometric view that gives a deeper understanding.

To connect with the simpler vector case, a vector can be thought
of as a list of real numbers which change in a certain way when
the coordinate system changes, or as a geometric object with
length and direction. The latter object is {\em
coordinate-independent}, and has different representations in
different choices of coordinate axes. Try to keep the geometric
picture in mind for matrices.

\subsection{Definitions}

Given an $n \times m$ matrix $A$ (where $n$ is the number of rows
and $m$ is the number of columns), the \tbf{transpose of $A$},
denoted $A^T$, is the $m \times n$ matrix where the rows of $A^T$
are the columns of $A$ (or, equivalently, the columns of $A^T$ are
the rows of $A$).

\begin{lem} $(AB)^T = B^T A^T$ and $(A^T)^T = A$. \end{lem}

We leave the proof to the reader.

If an $n \times n$ matrix (also called a \tbf{square} matrix) $A$
satisfies $A^T = A$, then we say $A$ is \tbf{symmetric}

\begin{exa} Let $A$ be the matrix
\be \left(\begin{array}{ccccc}
                        2  & 2  &  4 &  2 &  \\
                        1  & 1  & -2 &  2 &  \\
                        -2 & 0  &  0 &  1 &  \\
                        1  & 1  &  2 &  1 &
                          \end{array}\right)
\ee

Then $A^T$ is \be \left(\begin{array}{ccccc}
                        2  & 1  &  -2 &  1 &  \\
                        2  & 1  &  0 &  1 &  \\
                        4 &  -2  &  0 &  2 &  \\
                        2  & 2  &  1 &  1 &
                          \end{array}\right)
\ee Note the above matrix is not symmetric.
\end{exa}

The number of {\em degrees of freedom} in a symmetric matrix (\ie
independent real numbers needed to completely specify the matrix)
is $n(n+1)/2$. Why? There are $n^2$ entries, $n$ on the diagonal.
If you specify all entries above the diagonal and all entries on
the diagonal, then you know the symmetric matrix.

There are $n^2 - n$ non-diagonal entries (half above the diagonal,
half below). Thus, one needs to specify $\frac{n^2 - n}{2} + n =
\frac{n^2 + n}{2}$ entries.


\begin{exe} If $A$ and $B$ are symmetric, show $AB$ is symmetric.
\end{exe}

{\bf Matrix multiplication.} We call the element in the $i^{th}$
row and $j^{th}$ column $a_{ij}$. Think of $i=1\cdots n$ going
down the left side, and $j=1\cdots M$ going across the top. A
vector $v$ we represent as a column of elements with the $i^{th}$
being $v_i$. A nice way to see matrix-vector multiplication is
that the $v_i$ give the {\em coefficients} by which the columns of
$A$ are linearly mixed together. For the product $w = A v$ to make
sense, the length (dimension) of $v$ must equal $m$, and the
dimension of $w$ will be $n$. $A$ is therefore a linear transform
from $m$-dim space to $n$-dim space.

Multiple transformations appear written backwards: if we apply $A$
then $B$ then $C$ to a vector, we write \be
    w \; = \; C B A v .
\ee

Note that taking the product of two $n\times n$ matrices requires
$O(n^3)$ effort.

\begin{exe}
Show that there are two ways to evaluate triple matrix products of
the type $C B A$. The slow way involves $O(n^4)$ effort. How about
the fast way? How do these results scale for the case of a product
of $k$ matrices?\end{exe}

\begin{defi}[Invertible Martices]
$A$ is invertible if a matrix $B$ can be found such that $B A  = A
B  = I$. The inverse is then written $B = A^{-1}$. Invertibility
requires $A$ to be square.
\end{defi}

{\bf Transformations of a matrix.} Just as with vectors, we can
find out how the components of a square matrix $A$ change under
transformation. Say we have a scalar quantity $x  = w^T A v$. We
transform our coordinate system linearly such that the vector $v$
has components $v' = M v$, where $M$ is some invertible matrix
representing the transformation. Therefore also $w' = M w$. The
only way that $x$ can remain unchanged by the transformation (as
any scalar must), for all choices of $v$ and $w$, is if the
transformed matrix is written $A' = M^{-T} A M^{-1}$. Check this
via \be \label{eq:sim} x' = w'^T A' v' = (M w)^T (M^{-T} A M^{-1})
(M v) = w^T I A I v = w^T A v = x. \ee This is called a {\em
similarity transformation}, or a {\em conjugation}. Really we have
one object, the transformation $A$, but it may have different
representations by a matrix of numbers, depending on the choice of
basis.


\begin{defi}[Orthogonal Matrices] $Q$ is an orthogonal $n \times n$ matrix if
it has real entries and $Q^T Q = Q Q^T = I$. \end{defi}

$Q$ is invertible, with inverse $Q^T$. The geometric meaning of
$Q$ is a {\em rotation}: the vector $w = Q v$ is just $v$ rotated
(about the origin).


The number of degrees of freedom in an orthogonal matrix is
$n(n-1)/2$.

\begin{exe}
In 3 dimensions a general rotation involves 3 angles (for example,
azimuth, elevation, and `roll'). How many angles are needed in 4
dimensions? In 3d you rotate about a line-like axis (the set of
points which do not move under rotation); what object do you
rotate about in 4d?
\end{exe}

\begin{exe}
Show that the identity matrix $I$, always has representation
$I_{ij} = \delta_{ij}$ regardless of the choice of basis. Hint:
perform orthogonal tranformation on the matrix $\delta_{ij}$.
\end{exe}


The set of orthogonal matrices of order $n$ forms a {\em
continuous} (or {\em topological}) group, which we call $O(n)$.
(Not to be confused with ``of order N''). Group properties: \bi
\item Associativity follows from that of matrix multiplication.
\item The identity matrix acts as an identity element, since it
is in the group.
\item Inverse is the transpose (see above): $Q^{-1} = Q^T$.
\item Closure is satisfied because any product $QR$ of orthogonal
matrices is itself orthogonal. \ei

\begin{exe}
Prove the last assertion.
\end{exe}

However, not all the elements of $O(n)$ can `talk' to each other,
\ie you cannot reach all the elements by continuous transformation
from the identity $I$.

Example for $n=2$: a general order-2 orthogonal matrix can be
written

\bea
\left(\begin{array}{ll}\cos \theta & -\sin\theta\\
\sin \theta & \cos\theta\end{array}\right)
\;\;\;\;\mbox{or}\;\;\;\;
\left(\begin{array}{ll}\cos \theta & -\sin\theta\\
-\sin \theta & -\cos\theta\end{array}\right) , \eea where $0 \le
\theta < 2\pi$ is a real angle. The first has determinant $+1$ and
defines the `special' (\ie unit determinant) group $SO(2)$ which
is a subgroup of $O(2)$ with identity $I$. The second has
determinant $-1$ and corresponds to rotations with a reflection;
this subgroup is disjoint from $SO(2)$, and has the weird
(reflecting) identity can be written in some basis as \bea
\left(\begin{array}{ll}1 & 0\\
0 & -1 \end{array}\right) . \eea

Note that $SO(2)$, alternatively written as the family of planar
rotations $R(\theta)$, is {\em isomorphic} to the unit length
complex numbers under the multiplication operation: \be R(\theta)
\; \longleftrightarrow \; e^{i\theta} . \ee Therefore we have
$R(\theta_1)R(\theta_2) = R(\theta_1 + \theta_2)$. This
commutativity relation does {\em not} hold in higher $n > 2$.


{\bf Orthogonal transformations.} If an orthogonal matrix $Q$ is
used for conjugation of a general square matrix $A$, then the rule
Eq.~\ref{eq:sim} for transformation becomes, \be
    A' \; = \; Q A Q^T.
\ee This tells you how to `rotate' a (square) matrix.


\begin{defi}[Complex Conjugate Transpose] Let $A$ be an $n \times m$
matrix. Then the complex conjugate transpose of $A$, denoted
$A^{*}$, is obtained by the following: (1) take the complex
conjugate of $A$; ie, replace every entry $a_{jk} = x_{jk} + i
y_{jk}$ with $\overline{a_{jk}} = x_{jk} - i y_{jk}$, and call
this matrix $A_1$; (2) take the transpose of $A_1$. \end{defi}

\begin{exe} Prove that $(AB)^{*} = B^{*}A^{*}$. \end{exe}

\begin{defi}[Dot or Inner Product] The dot (or inner) product of
two real vectors $v$ and $w$ is defined as $v^T w$; if the vectors
are complex, we instead use $v^{*} w$. \end{defi}

\begin{exe}
Show that the dot product is invariant under orthogonal
transformation. That is, show that given two vectors, transforming
them using the same orthogonal matrix leaves their dot product
unchanged.
\end{exe}

\begin{defi}[Length of a vector] The length of a real vector $v$
is $|v|^2 = v^T v$; for a complex vector, we have $|v|^2 = v^{*}
v$. \end{defi}

\begin{defi}[Orthogonality] Two real vectors are orthogonal (also
called perpendicular) if $v^T w = 0$; for two complex vectors, the
equivalent condition is $v^{*} w = 0$. \end{defi}



\begin{defi}[Eigenvalue, Eigenvector] Recall $\lambda$ is an \tbf{eigenvalue}
and $v$ is an \tbf{eigenvector} if $Av = \lambda v$ and $v$ is not
the zero vector. \end{defi}

\begin{exe} If $v$ is an eigenvector of $A$ with eigenvalue
$\lambda$, show $w = av$, $a \in \C$, is also an eigenvector of
$A$ with eigenvalue $\lambda$. \end{exe}

\begin{exe} Show that given an eigenvalue $\lambda$ and an
eigenvector $v$, you can always find an eigenvector $w$ with the
same eigenvalue, but $|w| = 1$. \end{exe}

To find the eigenvalues, we solve the equation $\det(\lambda I -
A) = 0$. This gives a polynomial $p(\lambda) = 0$. We call
$p(\lambda)$ the \tbf{characteristic polynomial}.




The \tbf{trace} of a matrix $A$, denote $\mbox{Tr}(A)$ is the sum
of the diagonal entries of $A$:

\be \mbox{Tr}(A) = \sum_{i=1}^n a_{ii}. \ee

\begin{lem} $\mbox{Tr}(A) = \sum_{i=1}^n \lambda_i$. \end{lem}

The proof relies on writing out the characteristic equation and
comparing powers of $\lambda$ with the factorized version. By the
fact that the polynomial has roots $\lambda_i$ we can write

\be \det(\lambda I - A) \; = \; p(\lambda) \; = \; \prod_{i=1}^n (
\lambda - \lambda_i). \ee

Note the coefficient of $\lambda^n$ is $1$, thus we have $\prod_i
( \lambda - \lambda_i)$ and not $c \prod_i ( \lambda - \lambda_i)$
for some constant $c$.

By expanding out the RHS, the coefficient of $\lambda^{n-1}$ is
$-\sum_{i=1}^n \lambda_i$, which we will show is $-\mbox{Tr}(A)$.
Expanding the LHS, we want to find the corresponding coefficient
in \bea \left|\begin{array}{llll}\lambda-a_{11} & -a_{12} & \cdots
& -a_{1n} \\ -a_{21} & \lambda-a_{22} & & \\ \vdots&&\ddots &\\
-a_{n1}&&& \lambda-a_{nn}
\end{array} \right| . \nonumber
\eea We have to remember the expansion of the determinant. Taking
the top-left-most $2\times 2$ block, we see its determinant is
$(\lambda-a_{11})(\lambda-a_{22}) - a_{12}a_{21} = \lambda^2 -
(a_{11} + a_{22})\lambda + (a_{11}a_{22} - a_{12}a_{21})$. The
determinant of the top-left-most $3\times3$ block is then formed
by $(\lambda-a_{33})$ times the above $2\times2$ determinant, plus
two other multiples of determinants which can give only a highest
power of $\lambda$ of $\lambda^1$. Thus we see that the
coefficient in $\lambda^2$ is $-(a_{11} + a_{22} + a_{33})$.
Repeating this argument for $4\times4$ block up to $ n\times n$
gives us the coefficient of $\lambda^{n-1}$ in the full
determinant is $-\sum_{i=1}^n a_{ii}$. Since the LHS and RHS must
be equal $\forall \lambda$, the LHS and RHS coefficients in
$\lambda^{n-1}$ are equal. $\done$

\begin{cor}
Tr($A$) is invariant under rotation of basis.
\end{cor}
The proof follows immediately from the invariance of the
eigenvalues under rotation of basis. We need the following:

\begin{lem} $\det(AB) = \det(A) \det(B)$. Further, by induction one
can show $\det(AB \cdots Z) = \det(A) \det(B) \cdots \det(Z)$.
Further, $\det(I) = 1$.
\end{lem}

Proof of Corollary: Let $A = Q^TBQ$. We show $A$ and $B$ have the
same trace by showing $A$ and $B$ have the same eigenvalues. To
find the eigenvalues of $A$ we must solve:

\bea \det(\lambda I - A) &=& \det(\lambda I - Q^T B Q) \nonumber\\
&=& \det(\lambda Q^T Q - Q^T B Q) \nonumber\\ &=& \det(Q^T \lambda
I Q - Q^T B Q) \nonumber\\ &=& \det\Big( Q^T (\lambda I - B) Q
\Big) \nonumber\\ &=& \det(Q^T) \det(\lambda I - B) \det(Q)
\nonumber\\ &=& \det(Q^T) \det(Q) \det(\lambda I - B) \nonumber\\
&=& \det(Q^T Q) \det(\lambda I - B) = \det(I) \det(\lambda I - B)
= \det(\lambda I - B). \nonumber\\ \eea

As the eigenvalues of $A$ and $B$ satisfy the same equation, they
are equal. $\Box$

\subsection{Spectral Theorem for Real Symmetric Matrices}

The main theorem we will prove is

\begin{thm}[Spectral Theorem] Let $A$ be a real symmetric $n
\times n$ matrix. Then there exists an orthogonal $n \times n$
matrix $Q$ and a diagonal matrix $\Lambda$ such that $Q^T A Q =
\Lambda$. Moreover, the $n$ eigenvalues of $A$ are the diagonal
entries of $\Lambda$. \end{thm}

This result is remarkable: it tells you that any real, symmetric
matrix is diagonal when rotated into an appropriate basis (recall
the rotation effect of conjugation using $Q$). In other words, the
operation of matrix $A$ on a vector $v$ can be broken down into
three steps: \be Av = Q \Lambda Q^Tv  = (\mbox{undo the rotation})
(\mbox{stretch along coord axes}) (\mbox{rotation}) v . \ee Recall
the ordering of transformations is read like Hebrew, right to
left. The rotation is just the rotation into the basis of
eigenvectors.

Furthermore, the eigenvalues $\lambda_i$ (= diag els of $\Lambda$)
are a set of numbers invariant under rotations of $A$. In other
words, if $A' = P A P^T$ is an orthogonally-conjugated (\ie P is
orthogonal) version of $A$, then $A'$ has the same $\{\lambda_i\}$
as $A$. Of course the ordering of the $\lambda_i$ has to be chosen
the same.


For the Spectral Theorem we prove a sequence of needed lemmas:

\begin{lem} The eigenvalues of a real symmetric matrix are real.
\end{lem}

Let $A$ be a real symmetric matrix with eigenvalue $\lambda$ and
eigenvector $v$. Note that we do not yet know that $v$ has only
real coordinates!

Therefore, $Av = \lambda v$. Take the dot (or inner) product of
both sides with the vector $v^{*}$, the complex conjugate
transpose of $v$:

\be v^{*} Av = \lambda v^{*} v. \ee

But the left hand side is real. The two sides are clearly complex
numbers (ie, $1$-dimensional matrices). Taking the complex
conjugate transpose of the LHS gives

\be \Big(v^{*} (Av)\Big)^{*} = (Av)^{*} (v^{*})^{*} = v^{*} A v.
\ee

Therefore, the LHS is real, implying the RHS is real. But clearly
$v^{*} v$ is real (similar calculation). Thus, $\lambda$ is real.
$\Box$ \\

We will only prove the Spectral Theorem when all the eigenvalues
are distinct. Henceforth, we shall always assume $A$ is a real
symmetric matrix.

\begin{lem} The eigenvectors of a real symmetric matrix are real.
\end{lem}

The eigenvalues solve the equation $(\lambda I - A)v = 0$. Let
$\lambda$ be an eigenvalue. Then $\det(\lambda I - A) = 0$.
Therefore the matrix $B = \lambda I - A$ is not invertible.
Therefore it send a vector to $0$ (standard linear algebra
calculation).

\begin{lem} If $\lambda_1$ and $\lambda_2$ are two distinct
eigenvalues of a real symmetric matrix $A$, then their
corresponding eigenvectors are perpendicular. \end{lem}

We study $v_1^T A v_2$. Now

\be v_1^T A v_2 = v_1^T (Av_2) = v_1^T (\lambda_2 v_2) = \lambda_2
v_1^T v_2. \ee

Also, \be v_1^T A v_2 = v_1^T A^T v_2 = (v_1^T A^T) v_2 = (Av_1)^T
v_2 = (\lambda_1 v_1)^T v_2 = \lambda_1 v_1^T v_2. \ee

Therefore \be \lambda_2 v_1^T v_2 = \lambda_1 v_1^T v_2 \ \
\mbox{or} \ \ (\lambda_1 - \lambda_2) v_1^T v_2 = 0. \ee

As $\lambda_1 \neq \lambda_2$, $v_1^T v_2 = 0$. Thus, the
eigenvectors $v_1$ and $v_2$ are perpendicular. $\Box$ \\

We can now prove the Spectral Theorem for real symmetric matrices
\tbf{if there are $n$ distinct eigenvectors}.

Let $\lambda_1$ to $\lambda_n$ be the $n$ distinct eigenvectors,
and let $v_1$ to $v_n$ be the corresponding eigenvectors chosen so
that each $v_i$ has length $1$.

Consider the matrix $Q$, where the first column of $Q$ is $v_1$,
the second column of $Q$ is $v_2$, all the way to the last column
of $Q$ which is $v_n$:

\be Q = \left(\begin{array}{ccccc}
                        \uparrow  &  \uparrow & \ &  \uparrow &  \\
                        v_1 & v_2  &  \cdots &  v_n &  \\
                        \downarrow  & \downarrow  &  \  &  \downarrow &
                          \end{array}\right)
\ee

The transpose of $Q$ is

\be Q^T = \left(\begin{array}{cccc}
                        \leftarrow  &  v_1 &  \rightarrow &  \\
                        \ &   \vdots & \ &  \\
                        \leftarrow  & v_n   &  \rightarrow &
                          \end{array}\right)
\ee

\begin{exe} Show that $Q$ is an orthogonal matrix. Use the fact
that the $v_i$ all have length one, and are orthogonal
(perpendicular) to each other. \end{exe}

Consider $Q^T A Q$. This is a matrix, call it $B$. To find its
entry in the $i^{th}$ row and $j^{th}$ column, we look at

\be e_i^T B e_j \ee

where the $e_k$ are column vectors which are $1$ in the $k^{th}$
position and $0$ elsewhere:

\be e_k = \left(\begin{array}{cc}
                        0 &  \\
                        \vdots &  \\
                        0 & \\
                        1 & \\
                        0 & \\
                        \vdots & \\
                        0 &
                          \end{array}\right)
\ee

Thus, we need only show that $e_i^T B e_j = 0$ if $i \neq j$ and
equals $\lambda_j$ if $i = j$.

\begin{exe} Show $Q e_i = v_i$ and $Q^T v_i = e_i$. \end{exe}

We calculate

\bea e_i^T B e_j & = & e_i^T Q^T A Q e_j \nonumber\\ &=& (e_i^T
Q^T) A (Q e_j) \nonumber\\ &=& (Q e_i)^T A (Q e_j) \nonumber\\ &=&
v_i^T A v_j \nonumber\\ &=& v_i^T (A v_j) \nonumber\\ &=& v_i^T
\lambda_j v_j = \lambda_j v_i^T v_j. \eea

As $v_i^T v_j$ equals $0$ if $i \neq j$ and $1$ if $i = j$, this
proves the claim.

Thus, the off-diagonal entries of $Q^T A Q$ are zero, and the
diagonal entries are the eigenvalues $\lambda_j$. This shows that
$Q^T A Q$ is a diagonal matrix whose entries are the $n$
eigenvalues of $A$. $\Box$
\\

Note that, in the case of $n$ distinct eigenvalues, not only can
we write down the diagonal matrix, we can easily write down what
$Q$ should be. Further, by reordering the columns of $Q$, we see
we reorder the positioning of the eigenvalues on the diagonal.



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\chapter{Central Limit Theorem, Spectral Theorem  for Real Symmetric, Spectral Gaps}

Proof of the Central Limit Theorem (via the Fourier Transform).
Proof of the Spectral Theorem for Real Symmetric Matrices (via
maximization). Spectral Gap and Families of Expanders. Lecture by
Peter Sarnak; notes by Steven J. Miller.

\section{Central Limit Theorem}

Let $X: \Omega \rightarrow \R$ be a random variable ($x(\omega)
\in \R$). Define the density function $\mu$ on $\R$ by

\be \mu[a,b] = \mbox{Prob}\Big(\omega: x(\omega) \in [a,b]\Big).
\ee

Thus, $X$ gives rise to the probability measure $\mu$ on $\R$.

Assume

\ben

\item $X_1, X_2, \dots$ are independent identically distributed
random variables (iidrv) with density $\mu$.

\item $E(X) = \int_\Omega x(\omega) dp(\omega) = \int_\R x
d\mu(x)$.

\item $\mbox{Var}(X) = \int_\Omega x^2(\omega) dp(\omega) = \int_R
x^2 d\mu(x) = 1$.

\een

Define $S_N = \sum X_j$, and let $N(0,1)$ denote the standard
Gaussian or normal distribution (mean zero, variance one).

\begin{thm}

\be \frac{S_N}{\sqrt{N}} \ \ \ \ { \longrightarrow \atop in \
probability} \ \ \ \  N(0,1), \ee

ie,

\be \mbox{Prob}\Big( \frac{S_N}{\sqrt{N}} \in [a,b] \Big)
\rightarrow \frac{1}{\sqrt{2\pi}} \int_a^b e^{-\frac{x^2}{2}} dx.
\ee
\end{thm}

Proof: We have a random variable $X$, induces a probability
measure $\mu$ on the real line $\R$. Thus, $d\mu = f(x)dx$, $f(x)$
is a nice function. As $\mu$ is a probability measure on $\R$, we
must have $f(x) \ge 0$ and $\int_R f(x)dx = 1$.

The given assumptions about the $X_i$s imply

\ben

\item $\int_R xf(x)dx = 0$.

\item $\int_R x^2 f(x)dx = 1$.

\een

\begin{defi}[Fourier Transform]
\be \hat{f}(\xi) \ = \ \int_R f(x)e^{-2\pi i x \xi} dx. \ee
\end{defi}

Clearly, $|\hat{f}(\xi) \le \int_R f(x)dx \le 1$. Further,
$\hat{f}(0) = \int_R f(x)dx = 1$.

Now

\be \hat{f}'(\xi) = \int_R (2\pi i x) f(x) e^{-2\pi i x \xi} dx.
\ee

Thus, $\hat{f}'(0) = 0$ (from $E(x) = 0$).

We will assume $\hat{f}$ is continuous (although this is implied
by our assumptions).

Further,

\be \hat{f}''(\xi) = -4\pi^2 \int_R x^2f(x)e^{-2\pi i x \xi}dx.
\ee

Therefore, $\hat{f}''(0) = -4\pi^2$ (by our assumption on the
variance).

Using Taylor to expand $\hat{f}$ we obtain

\bea \hat{f}(\xi) &=& 1 + \frac{f''(0)}{2} \xi^2 + \cdots
\nonumber\\ &=& 1 - 2\pi^2 \xi^2 + \cdots \eea

Near the origin, the above shows $\hat{f}$ looks like a concave
down parabola.

\begin{thm}[Fourier Inversion] If $f$ is a nice function (so all
the quantities below make sense, ie, $f$ decays fast enough): \be
f(x) = \int_R \hat{f}(\xi) e^{2\pi i x \xi} d\xi. \ee
\end{thm}

\begin{defi} $e(y) = e^{2\pi i y}$. \end{defi}

\begin{exe}
If $\phi_\xi(x+y) = e\Big((x+y)\xi\Big)$, prove $\phi_\xi(x+y) =
\phi_\xi(x) \phi_\xi(y)$. $\phi_\xi$ is a \textbf{character} of
$(\R,+)$.
\end{exe}

\begin{exe} Is there a $\psi:\R \rightarrow \R$ such that $\psi(x+y)
= \psi(x) + \psi(y)$? IE, can you find a homomorphism that takes
addition to addition? If yes (of course: see above!) what can you
say about $\psi$? If we assume $\psi$ is continuous, it must be of
the form $\phi_\xi$.

Think of $\R$ as a vector space over $\Q$ (ie, the scalars are
$\Q$). What is the dimension of $\R$ over $\Q$, and what is a
basis? As the reals are uncountable and the rationals are
countable, there are uncountably many basis vectors.

Any linear transformation will satisfy the desired condition! For
example, choose any basis (called a \textbf{Hamel Basis}) of $\R$
over $\Q$ (very hard! need the Axiom of Choice).

To show every character of the reals is of the form $\phi_\xi$,
you need more. If you assume the character is continous, then it
must be of the form $\phi_\xi$.
\end{exe}

Suppose we have random variables $X$ and $Y$ with measures $\mu$
and $\nu$, with induced functions $f(x)$ and $g(x)$. If we choose
$X$ and $Y$ independently, what is the distribution of $X + Y$?

\begin{lem} The distribution of $X+Y$ clearly cannot be the sum (as that
won't be a probability measure). It is $f \ast g$, and $f \ast g$
is a probability measure. \end{lem}

\begin{defi}[Convolution]
\be (f \ast g)(x) \ = \ \int_R f(x-y)g(y)dy \ee
\end{defi}

We show the convolution is a probability measure. We assume our
functions $f$ and $g$ are nice (ie, that we may use Fubini to
interchange the order of integration).

\bea \int_R (f \ast g)(x)dx &=& \int_R \int_R f(x-y)g(y)dydx
\nonumber\\ &=& \int_R \int_R f(x-y)g(y)dxdy \nonumber\\ &=&
\int_R g(y) \Bigg( \int_R f(x-y)dx \Bigg)dy \nonumber\\ &=& \int_R
g(y) \Bigg( \int_R f(t)dt \Bigg)dy \nonumber\\ &=& \int_R g(y)
\cdot 1 dy = 1. \eea

\begin{exe} $\widehat{f \ast g}(\xi) = \hat{f}(\xi) \cdot \hat{g}(\xi)$.
Thus, Fourier Transform converts convolution to multiplication.
\end{exe}

Let $Z = X+Y$. What is \be \mbox{Prob}\Big( Z \in [z,z+dz] \Big) =
h(z)dz. \ee

$x$ can be anywhere; given $x$, $y$ must lie between $z-x$ and
$z-x+dz$.

\bea \mbox{Prob}\Big(X+Y \in [z,z+dz] \Big) &=& \int_{x \in \R}
\mbox{Prob}\Big(X \in [x,x+dx] \ \mbox{and} \nonumber\\ & & \ \ \
\ \ \ \ \ \ \ \ \ \ \ Y \in [z-x,z-x+dz] \Big)f(x)dx \nonumber\\
&=& \int_{x \in \R} f(x)g(z-x)dx = h(z), \eea

where the last step follows from the definition of the density
functions.

By Induction, we see $X_1 + \cdots X_N$ has distribution $f \ast
\cdots \ast f$ (as the random variables are iidrv).

However, we want to study $S_N = \frac{X_1 + \cdots
X_N}{\sqrt{N}}$.

\begin{defi}[FT] Let $FT(f) = \hat{f}$; $FT$ denotes the Fourier
Transform. \end{defi}

\begin{lem}
\be FT\Bigg(\mbox{the distribution of} \ \ \frac{X_1 + \cdots +
X_N}{\sqrt{N}}\Bigg) = \Bigg[\hat{f}\Big( \frac{\xi}{\sqrt{N}}
\Big) \Bigg]^N. \ee
\end{lem}

We take the limit as $N \rightarrow \infty$ for \textbf{fixed}
$\xi$. Recall we showed that $\hat{f}(\xi) = 1 - 2\pi^2 \xi^2 +
\cdots$. Thus, we  have to study

\be \Bigg[1 - \frac{2\pi^2 \xi^2}{N} +
O\Bigg(\frac{|\xi|^3}{N^{\frac{3}{2}}}\Bigg) \Bigg]^N. \ee

But as $N \rightarrow \infty$, the above goes to

\be e^{-2\pi \xi^2}. \ee

\textbf{The universality arises because \emph{only} terms up to
the second order in the Taylor Series contribute.}

\begin{exe} Show the Fourier Transform of the Gaussian is the
Gaussian. \end{exe}

Key point:

\begin{itemize}

\item Used Fourier Analysis to study the sum of independent identically
distributed random variables, as it converts convolution to
multiplication.

\end{itemize}


\begin{exe} Fix $g$ a nice, smooth, rapidly decreasing function. Consider the
linear transformation $A$: \be (Af)(x) = \int_R g(x-y)f(y)dy. \ee

Any convolution operator is diagonalized by these characters
(exponentials). At a formal level,

\bea (A\phi_\xi)(x) &=& \int_R g(x-y)\phi_\xi(y)dy \nonumber\\ &=&
\int_R g(x-y)e^{-2\pi i \xi y} dy \nonumber\\ &=& \int_R g(t)
e^{-2\pi i \xi(x-t)}dt \nonumber\\ &=& e^{-2\pi i x \xi} \int_R
g(t) e^{2\pi i \xi t}dt = \hat{g}(\xi) \phi_\xi(x). \eea

Thus, $\phi_\xi$ is an eigenvector of $A$ with eigenvalue
$\hat{g}(\xi)$.

\end{exe}


\section{Spectral Theorem for Real Symmetric Matrices}

Let $A$ be a real symmetric matrix acting on $\R^n$. Then $A$ has
an orthonormal basis $v_1, \dots, v_n$ such that $Av_j = \lambda_j
v_j$.

\emph{A simpler proof, assuming all eigenvalues are distinct, is
available in the September $25^{th}$ lecture notes.}

Write the inner or dot product $\langle v, w\rangle = v^t w$. As
$A$ is symmetric, $\langle Av, w\rangle = \langle v, Aw \rangle$.

\begin{defi} $V^\bot = \{w: \forall v \in V, \langle w,v \rangle =
0\}$. \end{defi}

\begin{lem}\label{leminvsubspace} Suppose $V \subset \R^n$ is an \textbf{invariant vector
subspace} under $A$ ($if v \in V$, then $Av \in V$). Then $V^\bot$
is also $A$-invariant: $A(V^\bot) \subset V^\bot$. \end{lem}

This proves the spectral theorem. Suppose we find a $v_0 \neq 0$
such that $Av_0 = \lambda_0 v_0$. Take $V = \{\mu v_0: \mu \in \R
\}$ for the invariant subspace.

By Lemma \ref{leminvsubspace}, $V^\bot$ is left invariant under
$A$, and is one dimension less. Thus, by whatever method we used
to find an eigenvector, we apply the same method on $V^\bot$.

Thus, all we must show is given an $A$-invariant subspace, there
is an eigenvector.

Consider

\be \max_{v \ with \ \langle v, v\rangle = 1} \Big\{ \langle Av,
v\rangle \Big\}. \ee

Standard fact: every continuous function on a compact set attains
its maximum (not necessarily uniquely). See, for example, W.
Rudin, \emph{Principles of Mathematical Analysis}.

Let $v_0$ be a vector giving the maximum value, and denote this
maximum value by $\lambda_0$. As $\langle v_0,v_0\rangle = 1$,
$v_0$ is not the zero vector.

\begin{lem} $Av_0 = \lambda_0 v_0$. \end{lem}

Clearly, if $Av_0$ is a multiple of $v_0$ it has to be $\lambda_0$
(from the definition of $v_0$ and $\lambda_0$).

Thus, it is sufficient to show

\begin{lem} $\{\mu v_0: \mu \in \R \}$ is an $A$-invariant
subspace. \end{lem}

Proof: let $w$ be an arbitrary vector perpendicular to $v_0$, and
$\epsilon$ be an arbitrary small real number. Consider

\be \langle A(v_0 + \epsilon w), v_0 + \epsilon w \rangle \ee

We need to renormalize, as $v_0 + \epsilon w$ is not unit length;
it has length $1 + \epsilon^2 \langle w,w\rangle$. As $v_0$ was
chosen to maximize $\langle Av, v\rangle$ subject to $\langle v,v
\rangle = 1$, after normalizing the above cannot be larger. Thus,

\bea\label{eqavnotepsilonw} \langle A(v_0 + \epsilon w), v_0 +
\epsilon w \rangle &=& \langle Av_0, v_0 \rangle + 2\epsilon
\langle Av_0,w \rangle + \epsilon^2 \langle w,w\rangle. \eea

Normalizing the vector $v_0 + \epsilon w$ by its length, we see
that in Equation \ref{eqavnotepsilonw}, the order $\epsilon$ terms
must be zero. Thus,

\be \langle Av_0, w \rangle = 0; \ee

however, this implies $Av_0$ is in the space spanned by $v_0$ (as
$w$ was an arbitrary vector perpendicular to $v_0$), completing
our proof. $\Box$

\begin{cor} Any local maximum will lead to an
eigenvalue-eigenvector pair. \end{cor}

The second largest eigenvector (denoted $\lambda_1$) is

\be \lambda_1 \ = \ \max_{ \langle v, v_0 \rangle = 0} \frac{
\langle Av,v \rangle }{\langle v, v\rangle }. \ee

We can either divide by $\langle v, v\rangle$, or restrict to unit
length vectors.


\section{Applications to Graph Theory}

Let $G$ be a $k$-regular graph, $f: V \rightarrow \R$. Recall $v
\sim w$ if there is an edge connecting $v$ and $w$. Let $A$ be the
adjacency matrix of $G$, and define

\bea Af(v) &=& \sum_{v \sim w} f(w) \nonumber\\ \langle f,g
\rangle &=& \sum_{v \in V} f(v) \overline{g}(v). \eea

Consider the function $f_0(v) = 1$ for all $v \in V$. Then

\be Af_0(v) = \sum_{v \sim w} f_0(w) = kf_0(v). \ee

Thus, $f_0$ is an eigenfunction with eigenvalue $k$.

\begin{thm}[Expander Families]
Fix $k$. Suppose we have a sequence of $k$-regular graphs $G_j$
with $|V_j| \rightarrow \infty$ and suppose $k - \lambda_1(G) \ge
\delta > 0$, $\delta$ fixed. Then $G_j$ is an expander family.
\end{thm}

\begin{rek}If you have an algorithm that has a random element, then one
can show there is another algorithm which does what this algorithm
does without having a random component. (More or less, some slight
of hand). \end{rek}

Suppose we have a \textbf{bipartite graph}: there are two sets of
vertices $I$ (inputs) and $O$ (outputs). Edges run only between
$I$ and $O$; there are no edges between two vertices in $I$ or
between two vertices in $O$. Let there be $n$ inputs and $n$
outputs, and join each input with $k$ outputs.

Fix $\delta_0 > 0$. We want, for any $B \subset I$, $|\partial B|
\ge \delta_0 |B|$.

We give a sketch of the proof. We will show that knowledge of a
spectral gap ensures that the boundary of \emph{any} subset of $I$
will be big. We do bipartite for simplicity.

Let $B \subset I$. Define

\be f(v) = \begin{cases} 2n - |B| & \mbox{for} \ v \in B \\ -|B| &
\mbox{otherwise}.
\end{cases} \ee

%Define $f(v) = 2n - |B|$ for $v \in B$ and $-|B|$ otherwise.
Then

\be \sum_{v \in V} f(v) = |B| \cdot (2n - |B|) + (2n - |B|) \cdot
(-|B|) = 0. \ee

Then

\be \lambda_1 = \max_{ \langle \tilde{f},f_0 \rangle = 0 } =
\frac{ \langle A \tilde{f}, \tilde{f} \rangle }{ \langle
\tilde{f}, \tilde{f}\rangle }. \ee

In particular,

\be \frac{ \langle Af_1, f_1 \rangle }{ \langle f_1, f_1 \rangle }
\le \lambda_1. \ee

\begin{defi}[Laplacian] $\Delta = kI - A$.
\end{defi}

Thus, $\Delta f_0 = k f_0 - Af_0 = 0 \cdot f_0$.

The eigenvalues of $\Delta$ are trivially related to the
eigenvalues of $A$.

\begin{rek}[Motivation for Laplacian] On the line we have
$\frac{d^2}{dx^2}$. A discrete version is (exercise)

\be \frac{ f(x+h) - 2f(x) + f(x-h) }{h^2}. \ee

In the plane, we would have $\frac{d^2}{dx_1^2} +
\frac{d^2}{dx_2^2}$. Integrating by parts we have

\bea \int_\Omega (\Delta f) \cdot g dx_1dx_2 &=& -\int_\Omega
\nabla f \cdot \nabla g dx_1dx_2 \nonumber\\ &=& \int_\Omega f
\cdot (\Delta g) dx_1dx_2. \eea

We want to integrate by parts on a graph!
\end{rek}

\be (\Delta F)(x) = kF(x) - \sum_{x \sim y} F(y). \ee

Therefore

\bea \langle \Delta F, F\rangle &=& \sum_{x\in V}\Bigg( k F(x) -
\sum_{y \sim x} F(y) \Bigg) F(x) \nonumber\\ &=& k \sum_{x \in V}
F(x)^2 - \sum_{x \in V} \sum_{x \sim y} F(x)F(y). \eea

For each edge $e$, orient it by $e^+$ and $e^-$. The analogue of
the Laplacian becomes

\bea \sum_e \Big( F(e^+) - F(e^-) \Big)^2 &=& \sum_e F^2(e^+) -
2F(e^+)F(e^-) + F^2(e^-) \nonumber\\ &=& 2 \langle \Delta F,
F\rangle, \eea

where the last line follows by thinking about what it means for
vertices to be connected, and how often each vertex is hit.

Recall

\be k - \lambda_1 \ = \ \min_{ \langle F, f_0 \rangle = 0} \frac{
\langle \Delta F, F \rangle }{\langle F, F\rangle } \ = \ \min_{
\langle F, f_0 \rangle = 0} \foh \frac{ ||dF||^2}{\langle F,
F\rangle }. \ee

Plug in the function $f$ defined above, namely,

\be f(v) = \begin{cases} 2n - |B| & \mbox{for} \ v \in B \\ -|B| &
\mbox{otherwise}.
\end{cases} \ee

If the edge runs from input to input or output to output, we get
zero. The only way we get non-zero contribution is from an input
to an output (or vice-versa). For our $f$,

\be \foh \sum_{e \in E} \Big| f(e^+) - f(e^-) \Big|^2 = \foh
(2n)^2 \cdot \#\{\mbox{edges e running from} \ B \ \mbox{to} \
B^c\}, \ee

where $B^c$ is the complement of $B$. We want $0 < \delta_0 = k -
\lambda_1$. Divide the previous equation by $\langle F, F\rangle$,
where $\langle F, F\rangle = (2n - |B|)^2 \cdot |B| + |B|^2
\cdot(2n - |B|)$ $= |B| \cdot (2n - |B|) \cdot 2n$.

Thus,

\be \delta_0 \le \frac{ \foh (2n) \# \{edges\} }{|B| \cdot (2n -
|B|) \cdot 2n }. \ee

Therefore,

\bea \#\{edges\} & \ge & \frac{\delta_0 |B| \cdot (2n - |B|)}{n}
\nonumber\\ & \ge & \delta_0 |B|, \eea

as $|B| \le n$.

Thus, the total number of edges is at least $\delta_0 |B|$. But
each vertex gets $k$ edges. Thus,

\be |\partial B| \ \ge \ \frac{\delta_0 |B|}{k}. \ee


\section{$2\sqrt{k-1}$}

Let $G$ be a $k$-regular connected graph. $A$ is the adjacency
matrix. Biggest eigenvalue is $\lambda_0 = k$. Eigenvalues cannot
be smaller than $-k$. How big is the gap between $k$ and
$\lambda_1$?

\begin{thm}[Alon-Boppana] Fix $k$. Take any sequencce of graphs
where the number of vertices $|G| \rightarrow \infty$. Then

\be \overline{lim}_{|G| \rightarrow \infty }  \lambda_1(A_G) \ \ge
\ 2\sqrt{k-1}. \ee
\end{thm}

\begin{rek} In $1$-dimension, with probability one the drunk
returns home; same in $2$-dimensions. He escapes with finite
probability in $3$ and higher dimensions! \end{rek}

Consider a $3$ regular graph with many vertices. Go to $v$, look
locally. If there are no short circuits, know what it looks like
locally: it will look like a tree. (This is $p$-adic hyperbolic
geometry).

Let $T$ be the infinite tree where each vertex is connected to
three other vertices (and a vertex cannot be connected to itself).
Suppose a drunk is walking on a tree. The only way he can get back
is to exactly undo what he's done.


Consider the following operator: consider an infinite dimensional
Hilbert space $l_2(V)$, the set of all $f:V \rightarrow \R$ such
that $\sum_v |f(v)|^2 < \infty$. This space is infinite
dimensional (for each $v$, take the function $f_v(w) = 1$ if $w =
v$ and $0$ otherwise).

Using $|ab| \le \frac{a^2 + b^2}{2}$,

\be \langle f, g\rangle = \sum_v f(v)g(v) \ee

exists (and is our inner product). We define

\be Af(v) = \sum_{w \sim v} f(w). \ee

It is not obvious that there are any eigenvectors (and, in fact,
there are no eigenvectors!). There is still a notion of spectrum.
We will show the spectrum of this operator ($k = 3$) is
$[-2\sqrt{2},2\sqrt{2}]$.

Note the constant function is horrendously not in this space (not
even \emph{close} to being square-integrable.



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\chapter{Properties of Eigenvalues of Adjacency Matrices of Random Graphs}

We discuss properties of eigenvalues of adjacency matrices arising
from Random Graphs. Lecture by Peter Sarnak; notes by Steven J.
Miller.

\section{Definitions}

Let $G$ be a connected, simple (no multiple bonds or edges)
$k$-regular graph with adjacency matrix $A = (a_{v,w})$. Here

\be \twocase{a_{v,w} =}{1}{if $v \sim w$}{0}{otherwise} \ee

Thus, $a_{v,w}$ is the number of paths of length one from $v$ to
$w$.

Let $A^2 = (a^{(2)}_{v,w})$, and here

\be a^{(2)}_{v,w} = \text{number of paths of length $2$ from $v$
to $w$}. \ee

Thus, \be a^{(2)}_{v,w} = \sum_{v'} a_{v,v'} a_{v',w}. \ee

Similarly let $A^n = (a^{(n)}_{v,w})$, and here

\be a^{(n)}_{v,w} = \text{number of paths of length $n$ from $v$
to $w$}. \ee

Recall

\be \text{Trace}(B) \ = \ Tr(B) \ = \ \sum_v b_{v,v}. \ee

Given an adjacency matrix $A$, let $D$ be the diagonal matrix of
eigenvalues.

\be D = \left(\begin{array}{cccc}
                        \lambda_0  &   &   &     \\
                          & \lambda_1  &  &     \\
                         &   &  \ddots &     \\
                         &   &   &  \lambda_{N-1}
                          \end{array}\right)
\ee

$A = Q^{-1} D Q$ for some orthogonal matrix $Q$. Thus, $A^n =
Q^{-1} D Q$, and we find:

\begin{lem} $Tr(A^n) = Tr(D^n)$. \end{lem}

\begin{lem}[Trace Formula] For any $n \ge 0$,
\be \sum_{j=0}^{N-1} \lambda_j^n \ = \ \sum_{v \in V}
a^{(n)}_{v,v}. \ee \end{lem}



\section{$\rho_k(2n)$ and $\lambda_{\max}$}

To count walks of length $n$ from $v$ to $v$, it is clearly at
least as many walks as there are on a $k$-regular infinite tree.

A tree is a homogeneous object: any vertex looks exactly the same
as any other. There is no special vertex on a tree, though we will
often name a vertex \tbf{the root}.

\begin{defi} $\rho_k(n)$ is the number of paths of length $n$ from
$v$ to $v$, where $v$ is \emph{any} vertex of \emph{the}
$k$-regular tree. \end{defi}

\begin{rek} $\rho_k(n) = 0$ for $n$ odd. \end{rek}

\begin{rek} The number of paths of length $n$ from $v$ to $v$ on
our graph $G$ is at least the number of paths of length $n$ from
any vertex to itself on the infinite tree. Thus,

\be \forall v \in V, \ a^{(n)}_{v,v} \ \ge \ \rho_k(n). \ee

\end{rek}

Remember that we've labeled the $N$ eigenvalues by $\lambda_0 = k,
\dots, \lambda_{N-1}$.

In the trace formula, we find

\be \sum_{j=0}^{N-1} \lambda_j^{2n} \ \ge \ N \rho_k(2n). \ee

Therefore

\be \frac{1}{N} \sum_{j=0}^{N-1} \lambda_j^{2n} \ \ge \
\rho_k(2n).
 \ee

and as $\lambda_0 = k$

\be\label{eqlambdarho} \frac{k^{2n}}{N} + \frac{1}{N}
\sum_{j=1}^{N-1} \lambda_j^{2n} \ \ge \  \rho_k(2n). \ee

Fix $n$ and let $N \to \infty$.

Let $\lambda_{\max} = \max\Big( |\lambda_1|, |\lambda_{N-1}|
\Big)$.

Thus, substituting into Equation \ref{eqlambdarho} we find

\be \frac{k^{2n}}{N} + \lambda_{\max}^{2n} \ge \rho_k(2n) \ee

in the limit as $N \rightarrow \infty$ (as we have $\lambda_{max}$
a total of $N-1$ times, and we divide by $N$; in the limit,
$\frac{N-1}{N} \to 1$.

As $N \to \infty$, we find

\bea \lambda_{\max}^{2n} & \ge & \rho_k(2n) \nonumber\\ \text{or}
\ \lambda_{\max} & \ge & \Big( \rho_k(2n) \Big)^{\frac{1}{2n}}.
\eea

\begin{exe} Show
\ben
\item $\rho_k(2n) \ge \frac{1}{m} \ncr{2m-2}{m-1} k(k-1)^{m-1}$.
\item $\Big( \rho_k(2n) \Big)^{\frac{1}{2n}} \to 2\sqrt{k-1}$.
\een
\end{exe}

Using the above exercise, we now find that

\be \lambda_{\max} \ge 2\sqrt{k-1}. \ee

\section{Measure from the Eigenvalues}

The trace formula told us that

\be \sum_{j=0}^{N-1} \lambda_j^n = \sum_{v \in V} a^{(n)}_{v,v}.
\ee

\begin{defi}[girth] The girth of a graph is the length of the
shortest closed cycle that returns to the starting vertex without
any backtracking. \end{defi}


\tbf{Assume that the girth of $G_N$ tends to $\infty$ as $N \to
\infty$. }

Fix $n$, let $N$ be very large. Then by assumption the girth is
greater than say $2n+1$. Thus, $a^{(n)}_{v,v}$ cannot have any
contribution from cycles without backtracking. Thus, locally, to
calculate $a^{(n)}_{v,v}$, we look like a tree, and we find
$a^{(n)}_{v,v} = \rho_k(n)$ for every $v \in V$. It is
\emph{essential} that we have fixed $n$.

Therefore, we now have (for fixed $n$ under our assumption) that

\bea \sum_{j=0}^{N-1} \lambda_j^n &=& N \rho_k(n) \nonumber\\
\frac{1}{N} \sum_{j=0}^{N-1} \lambda_j^n &=& \rho_k(n). \eea

The left hand side looks like a Riemann sum.

Suppose the density of the eigenvalues of the $3$-regular graph is
$d\mu = f(x)dx$.

We have just shown, for polynomials $p_n(x) = x^n$, that

\be \frac{1}{N} \sum_{j=0}^{N-1} p_n(\lambda_j) \to \int_{-3}^3
p_n(x) d\mu(x), \ee

where the above converges to $\rho_k(n)$ (here $k = 3$).

Thus, we are looking for a density function $f(x)$ such that

\be \int_{-3}^3 x^n f(x) dx = \rho_3(n), \ n \ge 0. \ee

Is there such a function? Is it unique? What does it look like?
This is the \tbf{Inverse Moment Problem}.

If there were two such functions, they would have to be equal by
the \tbf{Weierstrass Approximation Theorem}, as their difference
integrates to zero against any polynomial.

\begin{exe} Compute the generating function
\be F(z) = \sum_{n=0}^\infty \rho_3(n) z^n, \ee

which is something like

\be \frac{1}{\sqrt{4(k-1)^2 - z^2 } } \ee

if $|z|$ is small (or maybe complex and outside $[-k,k]$).

\bea \sum_{n=0}^\infty \rho_3(n) z^n &=&  \sum_{n=0}^\infty
\Bigg(\int_{-3}^3 x^n f(x)dx \Bigg) z^n \nonumber\\ &=&
\int_{-3}^3 f(x) \sum_{n=0}^\infty (zx)^n dx \nonumber\\ &=&
\int_{-3}^3 \frac{ f(x)dx }{ 1 - zx } = F(z). \eea

If we let $z = \frac{1}{w}$ we find

\bea F(w) &=& \sum_{n=0}^\infty \rho_3(n) \frac{1}{w^n}
\nonumber\\ &=& w \int_{-3}^3 \frac{ f(x) dx }{w - x}. \eea

Let $w \in C$ be such that $w \not\in [-3,3]$. Letting $w \to a
\in [-3,3]$, one gets a different value (a jump) if $w$ approaches
from above or below, and the jump is basically $f(a)$.

Look at $a + ib$ and $a - ib$, $b \to 0$.

\end{exe}

\section{Summary}

The above is all based on the assumption that the girth was big.
For the random graph, there are very few short closed cycles.
Thus, when we use the trace formula, we now have

\be \frac{1}{N} \sum_{j=0}^{N-1} \lambda_j^n = \rho_k(n) + O\Big(
\frac{1}{N} \Big). \ee


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\chapter{Spacings of Eigenvalues of Real Symmetric Matrices; Semi-Circle Law}

Joint Density Function for eigenvalues of real symmetric matrices;
spacing of eigenvalues for $2 \times 2$ real symmetric matrices;
Semi-Circle Rule. Lecture by Steven J. Miller; notes by Steven J.
Miller and Alex Barnett.

\section{Joint density function of eigenvalues of real symmetric matrices (`GOE')}

\subsection{Dirac Notation}

The derivation handed out in lecture used physics notation which
should be explained. The matrix is called the `Hamiltonian'
(meaning that it happened to arise in a quantum physics problem).
Vectors are often called {\em states} (referring to quantum
states), however they can be thought of as your usual vectors.
(Quantum mechanics is just linear algebra, amazingly). A general
vector in 2D is written \be \ket{u} \;\;\; \mbox{equivalent to}
\;\;\; \mbf{u} = \left(\begin{array}{l}u_1\\ u_2\end{array}
\right), \ee the latter being its coordinate representation in
some basis. The unit vectors are \be \ket{1},\; \ket{2} \;\;\;
\mbox{equivalent to}  \;\;\; \left(\begin{array}{l}1\\
0\end{array}\right), \; \left(\begin{array}{l}0\\
1\end{array}\right). \ee The $\ket{u}$ is a column vector, and
$\bra{u} \; \equiv \; \ket{u}^T$ is a row vector. Inner product
can be written as $\braket{v}{u} = \mbf{v}^T\cdot\mbf{u}$. General
bilinear product can be written $\braOket{v}{M}{u} = \mbf{v}^T
\cdot M \cdot \mbf{u}$.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{$2\times 2$ Gaussian Orthogonal Ensemble (GOE)}

We consider $2\times 2$ real symmetric matrices, \be A \; \equiv
\; \sym{x}{y}{z}. \ee Understanding this case is {\em vital} to
building intuition about Random Matrix Theory for $N \times N$
matrices.

$A$ can always be diagonalized by an orthogonal matrix $Q$ as
follows, \be \label{eq:diag} Q^T \sym{x}{y}{z} Q  \; = \;
\sym{\lambda_1}{0}{\lambda_2} \; \equiv \; D. \ee In $2\times 2$
case, the characteristic equation $\det (A - \lambda I) = 0$ is
quadratic:

\be \lambda^2 - \text{Tr}(A) \lambda + \det(A) \; = \; 0, \ee

where \be \text{Tr}(A) = x + z, \ \det(A) = xz - y^2. \ee

Solutions are \be \lambda_{1,2} \; = \; \frac{x+z}{2} \pm
\sqrt{\left(\frac{x-z}{2}\right)^2 + y^2}, \ee where 1 is the $+$
case, 2 the $-$.

If the two eigenvalues are equal, we say the matrix is degenerate.
Initially we are in a three-dimensional space (as $x, y$ and $z$
are arbitrary). Degeneracy requires that $x, y$ and $z$ satisfy

\be \Big(\frac{x-z}{2}\Big)^2 + y^2 = 0, \ee

or, equivalently,

\be x - z = 0, \ y = 0. \ee

Thus, we lose two degrees of freedom, because there are two
equations which must be satisfied. The set of solutions is
$\{(x,y,z) = (x,0,x) \}$.

\begin{exe} Show that $\lambda_1 - \lambda_2$ is twice the
distance from the origin in this 2D subspace. \end{exe}

Corresponding eigenvectors are, \be \mbf{v}_1 =
\left(\begin{array}{l}c\\ s\end{array}\right), \hspace{1in}
\mbf{v}_2 = \left(\begin{array}{l}-s\\ c\end{array}\right). \ee We
use abbreviations $c \equiv \cos \theta$ and $s \equiv \sin
\theta$.

Why can we write the eigenvectors as above? We can always
normalize the eigenvector attached to a given eigenvalue to have
length $1$. We have previously shown that, if the eigenvalues are
distinct, then the eigenvectors of a real symmetric matrix are
perpendicular. This forces the above form for the two
eigenvectors, at least when $\lambda_1 \neq \lambda_2$.

One rotation angle $\theta$ defines the orthogonal matrix, \be Q =
Q(\theta) = \biggl(\mbf{v}_1 \; \mbf{v}_2\biggr)  =
\mat{c}{-s}{s}{c}. \ee The structure of the eigenvectors is
actually quite rich.

\begin{exe}
Find $\theta$ in terms of $x,y,z$. Hint: use trigonometric
identities to simplify the resulting form. Hint: solve $(A-
\lambda_1 \mbf{v}_1) = \mbf{0}$.
\end{exe}

\begin{exe}
Show that a general $A$ can be written \be A \; = \; \alpha
\sym{\cos \beta}{\sin \beta}{-\cos \beta} \; + \;
\gamma\sym{1}{0}{1} \ee
\end{exe}

\begin{exe}
Find $\lambda_{1,2}$ in terms of $\alpha, \beta, \gamma$. Show
that the eigenvector angle is given by $\theta = \beta/2$. This
result is quite deep; for instance notice that taking a complete
$2\pi$ cycle in $\beta$ reverses the signs of the eigenvectors!
This isn't that relevant for the rest of this lecture.
\end{exe}


We adopt two assumptions about the joint distribution over $A$,
called $p(A) \equiv p(x,y,z)$: \ben
\item Invariance of $p$ under orthogonal transformations
(aka `basis-invariance'), $p(M^T A M) = p(A)$ for all orthogonal
$M$.
\item Independence of distributions of individual
matrix elements, $p(x,y,z) = p_x(x) p_y(y)p_z(z)$. \een Section
2C-1 of the handout reminds us that these two assumptions taken
together demand a unique form of distribution, \be
    p(x,y,z) \; \propto \; e^{-C\text{Tr}(A^2)},
\ee depending on only one parameter $C$; we choose $C=1$. Note
$\propto$ means proportional to; the constant of proportionality
is what is needed to make $P9x,y,z)$ a probability distribution
(ie, the integral of $\int \int \int p(x,y,z) dx dy dz = 1$.


This corresponds to Gaussian distributions of matrix elements,
\bea
    p_y(y) = \sqrt{\frac{2}{\pi}} e^{-2 y^2} \hspace{1in} \mbox{off-diag}
\nonumber \\
    p_x(x) = p_z(x) = \frac{1}{\sqrt{\pi}} e^{-x^2} \hspace{1in}
\mbox{diag.} \eea Note that the diag elements have variance
$\half$, the off-diag variance $\sfrac{4}$. We show how to compute
the normalization prefactors later on. This form (for general $C$)
is the so-called GOE. The $n\times n$ case is derived in Miller's
handout of 9/25/02.




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Transformation to diagonal representation}

The operation of diagonalizing $A$ can be viewed as the
transformation from one 3D space to another 3D space, \be \mbf{r}
\equiv \underbrace{(x,y,z)}_{A} \; \longleftrightarrow \; \mbf{r}'
\equiv \underbrace{(\lambda_1,\lambda_2, \theta)}_{D,Q}. \ee

This is 1-to-1 apart from the set of measure zero (ie, a lower
dimensional subspace) corresponding to degenerate eigenvalues.
Looking at Eq.~\ref{eq:diag} we can see the transformation is
linear in the eigenvalues, nonlinear in $\theta$. We are
interested in the {\em marginal} distribution of the eigenvalues,

\be
    p'(\lambda_1,\lambda_2) \;\equiv \;
\int d\theta \, p'(\lambda_1,\lambda_2,\theta), \ee in other words
we don't care what $\theta$ is. We use primes to signify
distributions over final $(Q,D)$ variables.

We want to know how to transform probability density from
$\mbf{r}$ space to $\mbf{r}'$ space. In general this must follow
the law, \be
    p(\mbf{r}) d\mbf{r} \; = \; p'(\mbf{r}') d\mbf{r}',
\ee giving \be \label{eq:p}
    p'(\mbf{r}') \; = \; \det(J) p(\mbf{r}).
\ee The ratio of the volume elements is $|\det J|$ where $J$ is
the $3\times 3$ Jacobean matrix of the transformation. $J$ has
elements $J_{ij} = \partial r_j / \partial r'_i$.

Inverting Eq.~\ref{eq:diag} we can write $A(\mbf{r}')$ as \bea
\sym{x}{y}{z} = Q D Q^T &=
&\mat{c}{-s}{s}{c}\sym{\lambda_1}{0}{\lambda_2}
\mat{c}{s}{-s}{c} \nonumber \\
&=& \mat{\lambda_1 c^2 + \lambda_2 s^2}{(\lambda_1-\lambda_2)sc}%
{(\lambda_1-\lambda_2)sc}{\lambda_1 s^2 + \lambda_2 c^2} \eea We
evaluate $J$ for this case, \be
    J \; \equiv \; \left(\begin{array}{lll}
\pd{x}{\lambda_1} & \pd{y}{\lambda_1} & \pd{z}{\lambda_1} \\
\pd{x}{\lambda_2} & \ddots & \\ \pd{x}{\theta} & & \end{array}
\right). \ee We see $\lambda$'s only appear in the bottom three
entries, and furthermore they only appear as factors $(\lambda_1 -
\lambda_2)$ in each entry.
\begin{exe}
Evaluate the bottom row of $J$ to prove the above.
\end{exe}
Therefore this factor of a row of $J$ can be brought out in
evaluating the determinant: \be \det(J) = \biggl| \mbox{messy
$\theta$-dep $3\times 3$ matrix} \biggr| \cdot (\lambda_1 -
\lambda_2) \; = \; g(\theta) (\lambda_1 - \lambda_2). \ee

\textbf{Warning!} The Jacobian is the absolute value of the
determinant. Thus, we need $|\lambda_1 - \lambda_2|$ above, or we
need to adopt the convention that we label the eigenvalues so that
$\lambda_1 \ge \lambda_2$.

The only dependence on the $\lambda$'s is given by the second
factor. Plugging into Eq.~\ref{eq:p} and marginalizing over
$\theta$ gives, \bea \label{eq:p2} p'(\lambda_1,\lambda_2) & =&
\int d\theta\,  g(\theta) \,  (\lambda_1 - \lambda_2)
e^{-(\lambda_1^2 + \lambda_2^2)} \nonumber \\
& \propto & (\lambda_1 - \lambda_2) e^{-(\lambda_1^2 +
\lambda_2^2)}. \eea Note that we do not need the absolute value
sign around $(\lambda_1 - \lambda_2)$ because we chose $\lambda_1
> \lambda_2$. This is the joint density of the eigenvalues in
$2\times 2$ GOE.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Generalization to $n\times n$ case}

The above generalizes quite easily, with the dimension of the two
spaces being $N = \half n(n+1)$. The $\half n(n+1) $ degrees of
freedom in $A$ equal $n$ degrees of freedom in $D$ (namely the
eigenvalues $\{\lambda_i\}$) plus $\half n(n-1)$ degrees of
freedom in $Q$ (namely the generalized angles $\Omega$). We sort
the eigenvalues such that $\lambda_1 \ge \lambda_2 \ge \cdots \ge
\lambda_n$.

\begin{thm}
If $\lambda_i = \lambda_j$ for some $1 \le i < j \le n$, then the
Jacobean of the transformation $A \leftrightarrow (D,Q)$ vanishes,
that is $\det(J) = 0$.
\end{thm}

The proof relies on realising that the two eigenvectors
$\mbf{v}_i$ and $\mbf{v}_j$ span a 2D subspace, invariant under
$A$. (Recall here we refer to a subspace of the $n$-dim vector
space upon which $A$ operates by multiplication). The invariance
means that the choices of directions of the eigenvectors is
arbitrary in this 2D plane. Therefore there is one angle degree of
freedom in $\Omega$ which in not constrained by $A$, that is, it
is independent of the elements of $A$. Now think of the inverse
transformation from $(D,Q) \rightarrow A$. An infinitesimal volume
element is transformed as \be
    d\mbf{r} \;=\; \det(J) d\mbf{r}'.
\ee Changes of eigenvector angle within the 2D subspace have no
effect on $A$, so the volume element $d\mbf{r}$ is collapsed to
zero. (Another way of putting this is that $J$ acquires a
null-space of dimension 1). Therefore $\det(J) = 0$. $\done$.

This vanishing of the Jacobean at degeneracies renders the
non-uniqueness of the forward map $A \rightarrow (D,Q)$ at these
points harmless in the following.

The upper $n$ rows of $J$ are messy functions of angles $\Omega$,
and the bottom $\half n(n-1)$ rows contain entries each which is
{\em linear} in the eigenvalues. Therefore $\det(J)$ is a
polynomial of degree $\half n(n-1)$ in the eigenvalues
$\lambda_i$. Further, $\det(J) = 0$ if any two eigenvalues are
equal.

Consider the polynomial $\prod_{1\le i<j\le n}
(\lambda_i-\lambda_j)$. First, note that this polynomial vanishes
whenever two eigenvalues are the same. We claim it is a polynomial
of degree $\foh n(n-1)$ in the eigenvalues. For each $j$, there
are $j-1$ choices for $i$. Thus, the degree is

\be \sum_{j=2}^n j-1 = \sum_{k=1}^{n-1} k = \frac{(n-1)(n-1+1)}{2}
= \frac{n(n-1)}{2}. \ee

Thus, $\det(J)$ and $\prod_{1\le i<j\le n} (\lambda_i-\lambda_j)$
both vanish whenever two eigenvalues are equal, and they have the
same degree. Therefore, they must be scalar multiples of each
other.

So,

\be \det(J) \propto \prod_{1\le i<j\le n} (\lambda_i-\lambda_j).
\ee

Combining with the GOE form of $p(A)$ gives, after marginalizing
over $\Omega$ as before, \be p(\{\lambda_i\}) \; = \; \prod_{1\le
i<j\le n} (\lambda_i-\lambda_j) \cdot e^{-\sum_{i=1}^n
\lambda_i^2}. \ee The vanishing of this probability density as any
two eigenvalues come close is called {\em level repulsion}.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Eigenvalue spacing distribution in $2\times 2$ real symmetric matrices}

\subsection{Reminder: Integral of the Gaussian}

We want \be I = \infint e^{-x^2} dx. \ee Square it and rearrange
the summation over area by using polar coordinates: \bea I^2 & = &
\infint e^{-x^2} dx \cdot \infint e^{-y^2} dy \; = \;
\infint \infint e^{-x^2 - y^2} dx\,dy \nonumber \\
& = & \int_0^{2\pi} d\theta \int_0^{\infty} r\, dr \,e^{-r^2} \; =
\;
2\pi \cdot \left[ -\frac{1}{2} e^{-r^2}\right]_0^{\infty} \nonumber \\
& = & \pi. \eea Introduction of the radius factor $r$ produced
$re^{-r^2}$, a known differential. So, \be I \; = \; \sqrt{\pi}.
\ee Changing the variable in the above, and rearranging, gives \be
\frac{1}{\sqrt{2\pi\sigma^2}} \infint e^{-\frac{x^2}{2\sigma^2}}
dx \; = \; 1. \ee This is therefore the correct normalization for
a 1D Gaussian probability density, of variance $\sigma^2$.


\subsection{Spacing distribution}

\begin{comment}
  \begin{figure}[ht]
\bc
\includegraphics[width=0.6\textwidth]{2x2_GOE.eps}
\ec \caption{Change of coordinates to get spacing distribution in
$E$. Light shading suggests form of density across the 2D plane.
Dark shading shows a graph of its projection onto the $E$ axis.}
\label{fig:goe}
\end{figure}
\end{comment}

Here for convenience we present a slightly simpler derivation than
in lecture. Given the 2D density $p'(\lambda_1,\lambda_2)$ we want
the 1D density of the difference $E \equiv \lambda_1 - \lambda_2$.
This will require marginalizing again, since there is a reduction
in dimensionality. We define $S \equiv \lambda_1 + \lambda_2$. The
linear transformation $(\lambda_1,\lambda_2) \rightarrow (E,S)$
has fixed Jacobean (it is a rotation by $-45^o$ and a compression
by $\sqrt{2}$ in each axis). See Fig.~\ref{fig:goe}.

Therefore, substituting in $\lambda_1 = (S+E)/2$ and  $\lambda_1 =
(S-E)/2$ into Eq.~\ref{eq:p2} gives \bea p'(E,S) & \propto &
p'(\lambda_1(E,S),\lambda_2(E,S)) \; = \;
E e^{-\sfrac{4}[(S+E)^2 + (S-E)^2]} \nonumber \\
& = & E e^{-E^2/2} \cdot e^{-S^2/2}, \eea which is separable.
Therefore integrating over $S$ gives an $E$-independent number,
and \be p'(E) \; \propto  \;  E e^{-E^2/2}. \ee This is the
so-called `Wigner Surmise' for the eigenvalue spacing density.
Remarkably, in the $n\times n$ case, even for large $n$, this
density is very close to the true spacing distribution of adjacent
eigenvalues. The limiting powerlaw $\lim_{E\rightarrow0} p'(E)
\propto E^\beta$ with $\beta = 1$ is intimately related to the
matrix symmetry class GOE that we close. It is also possible to
achieve $\beta = 2$ and $\beta = 4$ by choosing different symmetry
classes.

Finally, let's say you couldn't be bothered to construct your
second variable $S$. Instead you could derive the above using the
Dirac delta-function (see below) to marginalize: \be p'(E) \; = \;
\infint d\lambda_1 \int_{-\infty}^{\lambda_1} d\lambda_2 \,
p'(\lambda_1,\lambda_2) \, \delta(E - (\lambda_1 - \lambda_2)).
\ee Apart from the unusual limits (due to ordering of
eigenvalues), this is the standard procedure to extract a marginal
density.

\begin{exe}
Simplify the above to arrive at $p'(E)$.
\end{exe}





\section{Delta Function(al)}

Let $f(x)$ be a nice function; for example, let $f(x)$ be an
infinitely differentiable function whose Taylor Series converges
to $f(x)$:

\be f(x) = f(0) + \frac{f'(0)}{1!}x + \frac{f''(0)}{2!}x^2 +
\cdots \ee

Let

\be \twocase{\delta_n(x) = }{n}{if $x \in
\Big[-\frac{1}{2n},\frac{1}{2n}\Big]$}{0}{otherwise} \ee

\begin{exe} Show that

\be \int_{-\infty}^\infty f(x) \delta_n(x) dx = f(0) + O\Big(
\frac{1}{n} \Big). \ee

\end{exe}

Let $\delta$ be the limit as $n \to \infty$ of $\delta_n$. We find
/ define

\be \lim_{n \to \infty} \int_{-\infty}^\infty f(x) \delta_n(x)dx \
= \ \int_{-\infty}^\infty f(x) \delta(x) dx \ = \ f(0). \ee

\begin{exe} Show that
\be \int_{-\infty}^\infty f(x) \delta(x-a) dx \ = \ f(a). \ee
\end{exe}

A good analogy for the $\delta$ functional is a point mass. A
point mass has no extension (no length, width or height) but
finite mass. Therefore, a point mass has infinite density.

A probability density must integrate to one. This corresponds to
$\int 1 \cdot \delta(x) dx = 1$. We often refer to $\delta(x)$ as
a point mass at the origin, and $\delta(x-a)$ as a point mass at
$a$.

\section{Definition of the Semi-Circle Density}

Consider

\be \twocase{P(x) = }{\frac{2}{\pi} \sqrt{1 - x^2}}{if $|x| \le
1$}{0}{otherwise} \ee

\begin{exe} Show that $P(x)$ is a probability density. IE, show
that it is non-negative and integrates to $1$. Graph $P(x)$.
\end{exe}

We call $P(x)$ the semi-circle density.



\section{Semi-Circle Rule: Preliminaries}

Let $\lambda_j$ be the eigenvalues of a real, symmetric $N \times
N$ matrix $A$. We normalize the eigenvalues of $A$ by dividing by
$2\sqrt{N}$.

Define

\be \mu_{A,N}(x) \ = \ \frac{1}{N} \sum_{j=1}^N \delta\Big( x -
\frac{\lambda_j(A)}{2\sqrt{N}} \Big). \ee

$\delta\Big( x - \frac{\lambda_j(A)}{2\sqrt{N}} \Big)$ is a point
mass at $\frac{\lambda_j(A)}{2\sqrt{N}}$. By summing these point
masses and dividing by $N$, we have a probability distribution.
For example,

\be \int_{-\infty}^\infty f(x) \mu_{A,N}(x)dx \ = \ \sum_{j=1}^N
f\Big( \frac{\lambda_j(A)}{2\sqrt{N}} \Big). \ee

We will show that, as $N \to \infty$, the above converges to the
integral of $f$ against the semi-circle density:

\be \int_{-\infty}^\infty f(x) P(x)dx. \ee

What does this mean?

\be \sum_{j=1}^N f\Big( \frac{\lambda_j(A)}{2\sqrt{N}} \Big) \ee

looks like a Riemann Sum. The statement that, for nice $f(x)$,

\be \sum_{j=1}^N f\Big( \frac{\lambda_j(A)}{2\sqrt{N}} \Big) \ \to
\ \int_{-\infty}^\infty f(x) P(x)dx \ee

means that as $N \to \infty$, the number of eigenvalues of a
random $A$ in $[a,b]$ equals

\be \int_a^b P(x)dx. \ee


\begin{thm} Choose the entries $a_{ij}$ of a real, symmetric matrix
independently from a fixed probability distribution $p$ with mean
zero, variance one, and finite higher moments. For each $A$, form
the probability measure $\mu_{A,N}$. As $N \to \infty$, with
probability one the measures $\mu_{A,n}(x)dx$ converge to the
semi-circle probability $P(x)dx$.
\end{thm}

This is not the most general version; however, it is rich enough
for our purposes.

\section{Sketch of Proof of the Semi-Circle Law}

\subsection{Calculation of Moments via Trace Formula}

We will show that the expected value of the moments of the
$\mu_{A,N}(x)$ equal the moments of the semi-circle.

\begin{defi} $M_{A,N}(k)$ is the $k^{th}$ moment of the probability
measure attached to $\mu_{A,N}(x)dx$:

\be M_{A,N}(k) \ = \ \int x^k \mu_{A,N}(x)dx  \ =  \ \frac{1}{N}
\sum_{j=1}^N \Big( \frac{\lambda_j(A)}{2\sqrt{N}} \Big)^k. \ee
\end{defi}

Note that $\sum \lambda_j(A)^k = \text{Trace}(A^k)$. Thus, we have

\be M_{A,N}(k) \ = \ \frac{1}{2^k N^{1 + \frac{k}{2}}}
\text{Trace}(A^k). \ee

We now calculate the expected values of the first few moments ($k
= 0, 1$, $2$ and $3$).

\begin{lem} The expected value of $M_{A,N}(0) = 1$. \end{lem}

Proof:

\bea E\Big[M_{A,N}(0)\Big] & \ = \ & \frac{1}{N}
E\Big[\text{Trace}(I)\Big] \ = \ 1. \eea

Note that summing the eigenvalues to the zeroth power is the same
as taking the trace of the identity matrix. $\Box$

\begin{lem}The expected value of $M_{A,N}(1) = 0$. \end{lem}

Proof:

\bea E\Big[M_{A,N}(1)\Big] & \ = \ & \frac{1}{2N^{3/2}}
E\Big[\text{Trace}(A)\Big] \nonumber\\ & \ = \ &
\frac{1}{2N^{3/2}} E\Big[ \sum_i a_{ii} \Big] \nonumber\\ & \ = \
& \frac{1}{2N^{3/2}} \sum_i E[a_{ii}] \ = \ 0, \eea

because we have assumed that each $a_{ij}$ is drawn from a
probability distribution with mean zero. $\Box$

\begin{lem}The expected value of $M_{A,N}(2) = \frac{1}{4}$. \end{lem}

Proof: Note that

\be \text{Trace}(A^2) = \sum_i \sum_j a_{ij}a_{ji}. \ee

As our matrix is symmetric, $a_{ij} = a_{ji}$. Thus, the trace is
$\sum_i \sum_j a_{ij}^2$.

Now

\bea E\Big[M_{A,N}(2)\Big] & \ = \ & \frac{1}{4N^2}
E\Big[\text{Trace}(A^2)\Big] \nonumber\\ & \ = \ & \frac{1}{4N^2}
E\Big[ \sum_i \sum_j a_{ij}^2 \Big] \nonumber\\ & \ = \ &
\frac{1}{4N^2} \sum_i \sum_j E[a_{ij}^2] \ = \ \frac{1}{4}, \eea

where the last line follows from each $a_{ij}$ has variance $1$.
As their means are zero, the variance $E[a_{ij}^2] - E[a_{ij}]^2 =
1$ implies $E[a_{ij}] = 1$. There are $N^2$ pairs $(i,j)$. Thus,
we have $\frac{1}{4N^2} \cdot (N \cdot 1) = \frac{1}{4}$. $\Box$

\begin{lem}The expected value of $M_{A,N}(3) = 0$ as $N \to \infty$. \end{lem}

We need \be \text{Trace}(A^3) \ = \ \sum_i \sum_j \sum_k a_{ij}
a_{jk} a_{ki}. \ee

We find

\bea E\Big[M_{A,N}(3)\Big] & \ = \ & \frac{1}{8N^{2.5}}
E\Big[\text{Trace}(A^3)\Big] \nonumber\\ & \ = \ &
\frac{1}{8N^{2.5}} E\Big[ \sum_i \sum_j \sum_k a_{ij} a_{jk}
a_{ki} \Big] \nonumber\\ & \ = \ & \frac{1}{8N^{2.5}} \sum_i
\sum_j \sum_k E[a_{ij} a_{jk} a_{ki}]. \eea

There are three cases. If the subscripts $i, j$ and $k$ are all
distinct, then $a_{ij}, a_{jk}$, and $a_{ki}$ are three
independent variables. Hence

\be E[a_{ij} a_{jk} a_{ki}] \ = \ E[a_{ij}] \cdot E[a_{jk}] \cdot
E[a_{ki}] \ = \ 0. \ee

If two of the subscripts are the same (say $i = j$) and the third
is distinct, we have

\be E[a_{ii} a_{ik} a_{ki}] \ = \ E[a_{ii}] \cdot E[a_{ik}^2] \ =
\ 0 \cdot 1 \ = \ 0. \ee

If all three subscripts are the same, we have

\be E[a_{ii}^3] \ee

This is the third moment of $a_{ii}$. It is the same for all
variables $a_{ii}$, and is finite by assumption. There are $N$
triples where $i = j = k$.

Thus,

\bea E\Big[M_{A,N}(3)\Big] & \ = \ & \frac{1}{8N^{2.5}} \cdot N
E[a_{11}^3] \ = \ \frac{E[a_{11}^3]}{8} \cdot \frac{1}{N^{1.5}}.
\eea

Thus, as $N \to \infty$, the expected value of the third moment is
zero. $\Box$ \\

To calculate the higher moments requires significantly more
delicate combinatorial arguments.

\subsection{Calculation of Moments from the Semi-Circle}

We now calculate the moments of the semi-circle. For $k \le 3$,
the $k^{th}$ moment of the semi-circle $C(k)$ equals the expected
$k^{th}$ moment of $\mu_{A,N}(x)$ as $N \to \infty$.

\be C(k) \ = \ \int_{-\infty}^\infty x^k P(x)dx \ = \
\frac{2}{\pi}\int_{-1}^1 x^k \sqrt{1 - x^2}dx. \ee

We note that, by symmetry, $C(k) = 0$ for $k$ odd, and $C(0) = 1$
as $P(x)$ is a probability density.

For $k = 2m$ even, we change variables $x = \sin \theta$.

\be C(2m) \ = \ \frac{2}{\pi}\int_{-\frac{\pi}{2}}^{\frac{\pi}{2}}
\sin^{2m} \theta \cdot \cos^2 \theta d\theta. \ee

Using $\sin^2 \theta = 1 - \cos^2 \theta$ gives

\be C(2m) \ = \ \frac{2}{\pi}\int_{-\frac{\pi}{2}}^{\frac{\pi}{2}}
\sin^{2m} \theta d\theta \ - \ \frac{2}{\pi}
\int_{-\frac{\pi}{2}}^{\frac{\pi}{2}} \sin^{2m+2} \theta d\theta.
\ee

The above integrals can be evaluated exactly. We constantly use

\bea \cos^2(\phi) & \ = \ & \foh + \foh \cos(2\phi) \nonumber\\
\sin^2(\phi) & \ = \ & \foh - \foh \cos(2\phi). \eea

Repeated applications of the above allow us to write
$\sin^{2m}(\theta)$ as a linear combination of $1$,
$\cos(2\theta), \dots$, $\cos(2m\theta)$.

Let

\be \twocase{n!! = }{n \cdot (n-2) \cdots 2}{if $n$ is even}{n
\cdot (n-2) \cdots 1}{if $n$ is odd} \ee

We find (either prove directly or by induction) that

\be \frac{2}{\pi}\int_{-\frac{\pi}{2}}^{\frac{\pi}{2}} \sin^{2m}
\theta d\theta \ = \ 2 \frac{(2m-1)!!}{(2m)!!}. \ee

\begin{exe} Show the above gives
\be C(2m) \ = \ 2 \frac{(k-1)!!}{(k+2)!!}. \ee Also, show $C(2)$
agrees with our earlier calculation.
\end{exe}





%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\chapter{More Graphs, Maps mod p, Fourier Series and {n alpha}}

More on Graphs; Arithmetic maps mod $p$. Lecture by Peter Sarnak;
notes by Steven J. Miller. Appendix: Introduction to Fourier
Series and $\{n \alpha\}$, by Steven J. Miller.

\section{Kesten's Measure}

For a $k$-regular graph, define

\be \twocase{d\mu_k(t) = }{\frac{c_k \sqrt{(k-1) - \frac{t^2}{4}}
}{ 1 - (\frac{t}{k})^2 }dt}{if $|t| \le
2\sqrt{k-1}$}{0}{otherwise} \ee

Fix $a$ and $b$, and consider the $N$ eigenvalues $\lambda_j$.
Count

\be \frac{ \# \{j: \lambda_j \in [a,b] \} }{N}. \ee

Then

\begin{cla}
\be \lim_{N \to \infty} \frac{ \# \{j: \lambda_j \in [a,b] \} }{N}
\ = \ \mu_k\Big([a,b]\Big). \ee
\end{cla}

We have

\be \Big( \rho_k(2n) \Big)^{\frac{1}{2n}} \ \to \ 2 \sqrt{k-1}.
\ee

\section{Generating Functions on $k$-Regular Trees}

\subsection{$R(z)$}

Fix $k$. The \tbf{generating function $R(z)$} is

\be R(z) \ = \ \sum_{n=0}^\infty r_n z^n, \ee

where

\be r_n \ = \ \frac{\rho(n)}{k^n} \ = \ \text{Probability that we
return to $v$ in $n$ steps}. \ee

For $|z| < 1$, the series expansion for $R(z)$ converges.

Let $q_n$ be the probability of starting at $v$ and ending at $v$
for the first time (after $n$ steps).

\subsection{$Q(z)$}

Define

\be Q(z) \ = \ \sum_{n=0}^\infty q_n z^n. \ee

\begin{exe} Prove
\be R(z) \ = \ \frac{1}{1 - Q(z)}. \ee
\end{exe}

\subsection{$T(z)$}

Define

\be T(z) \ = \ \sum_{n=0}^\infty t_n z^n, \ee

where for $w$ adjacent to $v$, $t_n$ is the probability of going
from $w$ to $v$ in $n$-steps for the first time.

Further, let $t_{w,v}(n)$ be the probability of going from $w$ to
$v$ in $n$-steps first time and $d(w,v) = m \ge 1$. Remember that
$d(w,v)$ is the distance from $w$ to $v$.

\begin{exe} Prove
\ben
\item $Q(z) = zT(z)$.
\item $\sum_{n=0}^\infty t_{w,v}(n) z^n = \Big( T(z) \Big)^n$.
\een
\end{exe}

\begin{exe} Prove
\be T(z) \ = \ \frac{z}{k} + \frac{k-1}{k}z T^2(z). \ee
\end{exe}

Note that this explicitly gives us $T(z)$ by application of the
quadratic formula:

\be T(z) \ = \ \frac{1 \pm \sqrt{1 -  4 \Big( \frac{k-1}{k}z
\Big) \frac{z}{k} } }{2\frac{k-1}{k}z}. \ee

Now that we have $T(z)$ we have $Q(z)$, from which we get $R(z)$.
$T(z)$ will have a square-root -- it will be an algebraic function
of $z$.


\section{Recovering the Measure $f(x)$ from $R(z)$}

We have

\be R(z) \ = \ \sum_{n=0}^\infty r_n z^n. \ee

As we know $T(z)$, we know $R(z)$, hence we know the numbers
$r_n$.

Now,

\be r_n \ = \ \int_{\infty}^\infty x^n f(x)dx \ = \ \int_{-k}^k
x^n f(x)dx. \ee

How do we recover $f(x)$ given the numbers $r_n$? We've now
normalized the eigenvalues to lie in $[-1,1]$.

\bea R(z) & \ = \ & \sum_{n=0}^\infty \Bigg( \int_{-1}^1 x^n
f(x)dx \Bigg) \nonumber\\ & \ = \ & \int_{-1}^1 \Bigg(
\sum_{n=0}^\infty (xz)^n \Bigg) f(x) dx \nonumber\\ & \ = \ &
\int_{-1}^1 \frac{ f(x) }{1 - xz } dx \nonumber\\ B(z) \ = \
\frac{1}{z} R\Big(\frac{1}{z}\Big) & \ = \ & \int_{-1}^1 \frac{
f(x) }{z - x}dx. \eea

Suppose we know the LHS. Can we recover $f(x)$? If $z \in [-1,1]$,
the function will have a singularity. Thus, if $f(x)$ is a nice
function, we do not expect to be able to make sense of the above
relation if $z \in [-1,1]$. We will, however, consider $z$
\emph{close} to the interval $[-1,1]$.

Let $z = \xi + iy$, $\xi \in [-1,1]$, $y > 0$. Later we will take
$z = \xi - iy$.

Look at

\bea B(\xi + iy) - B(\xi - iy) & \ = \ & \int_{-1}^1 f(x) \Bigg[
\frac{1}{\xi + iy - x} - \frac{1}{\xi - iy - x} \Bigg]dx
\nonumber\\ & \ = \ & 2i \int_{-1}^1 \frac{ yf(x) }{ (\xi - x)^2 +
y^2 }dx. \eea

We will study the above as $y \to 0$.

\subsection{Poisson Kernel}

Recall $\xi \in [-1,1]$, $f(x)$ fixed, we are integrating $f(x)$
against the \textbf{Poisson Kernel}

\be \frac{y}{( \xi - x)^2 + y^2}. \ee

As $y \to 0$, the above looks singular at $x = \xi$.

At $x = \xi$, the kernel has height $\frac{1}{y}$, which is quite
large.

If $x = \xi + \epsilon$, then as $y \to 0$, the kernel goes to $0$
very rapidly.

Basically, as $y \to 0$, the kernel becomes a higher, thinner
spike centered at $\xi$.

Now

\bea \int_{\infty}^\infty \frac{y}{ (x - \xi)^2 + y^2} dx & \ = \
& \int_{-\infty}^\infty \frac{y}{t^2 + y^2} dt \nonumber\\ & \ = \
& \int_{-\infty}^\infty \frac{ y^2}{ y^2 \eta^2 + y^2} d\eta, \
\text{from} \ \frac{t}{y} = \eta \nonumber\\ & \ = \ &
\int_{-\infty}^\infty \frac{1}{1 + \eta^2} d\eta \nonumber\\ & \ =
\ & \pi. \eea

This is an \textbf{approximation to the identity}.

Thus,

\be B(\xi + iy) - B(\xi - iy) \ \to \ 2\pi i f(\xi). \ee


\subsection{Cauchy Integral Formula}

If you have an analytic function $f(z)$ and $\gamma$ is a curve
enclosing $z$ then

\be \frac{1}{2\pi i} \int_\gamma \frac{ f(\zeta) }{z - \zeta}
d\zeta  \ee

In our case above, we cannot apply Cauchy's Integral Formula, as
our function $f(x)$ is not analytic. It is compactly supported,
and no non-zero analytic function is compactly supported.

Call this permutation $\phi$:

\be \phi: \Fpf \to \Fpf, \ee

where $\phi^2$ is the identity.

\begin{que} Does $\phi$ behave like a random permutation?
\end{que}




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Third Problem}

\subsection{Introduction}

Let $p$ be a large prime, and consider the map

\be x \mapsto x^{-1} \ \text{mod} \ p, \ x \neq 0. \ee

This is a map from $\Fp \to \Fp$. The map is not completely
random, as

\bea 1 & \mapsto & 1 \nonumber\\ 2 & \mapsto & \frac{p+1}{2}
\nonumber\\ p-1 & \mapsto & p-1. \eea

The map which sends $x \to x^{-1}$ is a permutation of $\Fpf$. It
is not a completely arbitrary permutation, as it pairs $x$ with
$x^{-1}$ (away from a few very special $x$'s, such as $p-1$ and
$1$).

Thus, this permutation is a product of transpositions.


\subsection{Character Sums}

Let

\be 1 \le A \le B \le p, \ B - A \ \text{large}. \ee

Let

\be \overline{m} \ \text{be the inverse of $m$ mod $p$} \ee

IE, $m \overline{m} \equiv 1$ mod $p$.

Let

\be e(z) \ = \ 2^{2\pi i z}. \ee

For $\nu \in \Z/p\Z$, consider

\be S \ = \ \sum_{A \le m \le B} e\Big( \frac{\overline{m}\nu}{p}
\Big). \ee

These sums will measure how equidistributed or random the map $m
\to \overline{m}$ is.

\begin{exe} Prove the trivial bound for $|S|$:
\be |S| \le B - A. \ee
\end{exe}

Let $N = B - A + 1$. By the Central Limit Theorem, with high
probability if we add $N$ random numbers of modulus one we expect
square-root cancellation. Thus, we expect (if the inverse map is
random) that $|S| \approx \sqrt{N}$.


\subsection{Completing the Square}

Let $a, b \in \Z$, and consider the \textbf{Kloosterman Sum}

\be \text{Kl}(a,b,p) \ = \ \sum_{ {x \mod p \atop x \neq 0} }
e\Big( \frac{ax + b \overline{x}}{p} \Big). \ee

How large can the Kloosterman Sum be? If $a = b = 0$, then
trivially $\text{Kl}(0,0,p) = p-1$.

If $b = 0$ and $a \neq 0$ (or, by symmetry, the other way around)
then

\bea \text{Kl}(a,b,p) & \ = \ & \sum_{ {x \mod p \atop x \neq 0} }
e\Big( \frac{ax}{p} \Big) \nonumber\\ & \ = \ & \sum_{ {y \mod p
\atop y \neq 0} } e\Big( \frac{y}{p} \Big) \nonumber\\ & \ = \ &
\sum_{y=0}^{p-1} e\Big( \frac{y}{p} \Big) \ - \ 1 \ = \ -1. \eea

\begin{exe} Prove $\sum_{y=0}^{p-1}
e\Big( \frac{y}{p} \Big) = 0$. Hint: Let $T$ be this sum. Then
show $e\Big(\frac{1}{p}\Big) T = T$; thus $T = 0$. \end{exe}


\subsection{Weil's Bound}

Let $a \not\equiv 0$ mod $p$. Then

\be |\text{Kl}(a,b,p)| \ \le \ 2\sqrt{p}. \ee

This is a very deep result.

How big is

\be \sum_{a \mod p} |\text{Kl}(a,1,p)|^2 \ee

If we believe Weil's bound, each term is of size at most
$2\sqrt{p}$, we square, then sum $p$ terms. Thus, we expect a size
of at most $4p^2$. We will show on average that Weil's bound is
correct.

\bea \sum_{a \mod p} |\text{Kl}(a,1,p)|^2 &=& \sum_{a (p)} \Bigg|
\sum_{ {x \mod p \atop x \neq 0} } e\Big( \frac{\overline{x} +
ax}{p} \Big) \Bigg| \nonumber\\ &=& \sum_{a (p)} \sum_{ {x_1,x_2
\mod p \atop x_1,x_2 \neq 0} } e\Big( \frac{\overline{x}_1 -
\overline{x}_2 + a(x_1 - x_2)}{p} \Big) \nonumber\\ &=& \sum_{
{x_1,x_2 \mod p \atop x_1,x_2 \neq 0} } e\Big(
\frac{\overline{x}_1 - \overline{x}_2 }{p} \Big) \sum_{a (p)}
e\Big( \frac{ a(x_1 - x_2)}{p} \Big) \nonumber\\ &=& (p-1)p, \eea

where the last line follows from the $a$-sum vanishes unless $x_1
= x_2$, which then collapses the sums. There are $p-1$ ways $x_1 =
x_2$, and when this occurs, the $a$-sum gives $p$.

\begin{exe} Consider
\be \sum_{a (p)} |\text{Kl}(a,1,p)|^4. \ee

Above there are $p$-terms, each term of size $(2\sqrt{p})^4 =
16p^2$. Thus, show the sum is at most $16p^3$. You will find
$cp^3$ for some $c$ independent of $p$.
\end{exe}

By looking at one term, as every summand is positive, we find

\be |\text{Kl}(a,1,p)|^4 \ \le \ c p^3. \ee

Thus, taking the fourth-root yields

\be |\text{Kl}(a,1,p)| \ \le \ c^{\frac{1}{4}} p^{\frac{3}{4}}.
\ee


\subsection{Fourier Expansion of Sums}

Define the indicator function

\be \twocase{I(y) =}{1}{$A \le y \le A+N$}{0}{otherwise} \ee

Consider

\bea S & \ = \ & \sum_{A \le x \le A+N} e\Big( \frac{\nu
\overline{x}}{p} \Big) \nonumber\\ & \ = \ & \sum_{x (p)} e\Big(
\frac{\nu \overline{x} }{p}\Big) I(x).  \eea

We want to write $I(x)$ in terms of its Fourier Coefficients

\be \widehat{I}(m) \ = \ \int_0^1 e(-mt)I(t)dt. \ee

Then

\be I(y) \ = \ \sum_{m=-\infty}^\infty \widehat{I}(m)
e\Big(\frac{my}{p}\Big). \ee


\subsection{Brief Review of Fourier Series}

Consider the unit interval $[0,1]$. Define

\be \phi_m(x) \ = \ e(mx). \ee

Then (if our function is sufficiently nice)

\be f(x) \ = \ \sum_{m \in \Z} \widehat{f}(m) e(mx), \ee

where

\be \widehat{f}(m) \ = \ \int_0^1 f(x)e(-mx)dx. \ee


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%













\section{Fourier Analysis and the Equi-Distribution of
$\{n\alpha\}$}

\subsection{Inner Product of Functions}

We define the exponential function by means of the series

\be e^x = \sum_{n=0}^\infty \frac{x^n}{n!},\ee

which converges everywhere. Given the Taylor series expansion of
$\sin x$ and $\cos x$, we can verify the identity \be e^{i x} =
\cos x + i \sin x.\ee

\begin{exe} Prove $e^x$ converges for all $x \in \R$ (even better,
for all $x \in \C$. Show the series for $e^x$ also equals

\be \lim_{n \rightarrow \infty} \Big(1 + \frac{x}{n} \Big)^n, \ee

which you may remember from compound interest problems. \end{exe}

\begin{exe} Prove, using the series definition, that $e^{x+y} =
e^x e^y$. Use this fact to calculate the derivative of $e^x$. If
instead you try to differentiate the series directly, you must
justify the derivative of the infinite sum is the infinite sum of
the derivatives. \end{exe}

Remember the definition of \tbf{inner or dot product}: for two
vectors $\vec{v} = (v_1,\dotsb,v_n)$, $\vec{w} =
(w_1,\dotsb,w_n)$, we take the {\em inner product} $\vec{v}\cdot
\vec{w}$ (also denoted $\langle v,w\rangle$) to mean

\be \vec{v}\cdot \vec{w} \ = \ \langle v,w\rangle = \sum_i v_i
\bar{w}_i .\ee

Further, the length of a vector $v$ is

\be |v| \ = \ \langle v, v \rangle. \ee

We generalize this for functions. For definiteness, assume $f$ and
$g$ are functions from $[0,1]$ to $\C$. Divide the interval
$[0,1]$ into $n$ equal pieces. Then we can represent the functions
by

\be f(x) \ \ \longleftrightarrow \ \ \Bigg(f(0),
f\Big(\frac{1}{n}\Big), \dots, f\Big(\frac{n-1}{n}\Big) \Bigg),
\ee

and similarly for $g$. Call these vectors $f_n$ and $g_n$. As
before, we consider

\be \langle f_n,g_n\rangle \ = \ \sum_{i=0}^{n-1}
f\Big(\frac{i}{n}\Big) \cdot \bar{g}\Big(\frac{i}{n}\Big). \ee

In general, as we continue to divide the interval ($n \rightarrow
\infty)$, the above sum diverges. For example, if $f$ and $g$ are
identically $1$, the above sum is $n$.

There is a natural rescaling: we multiply each term in the sum by
$\frac{1}{n}$, the size of the sub-interval. Note for the constant
function, the sum is now independent of $n$.

Thus, for good $f$ and $g$ we are led to \be \langle f,g\rangle \
= \ \lim_{n \rightarrow \infty} \sum_{i=0}^{n-1}
f\Big(\frac{i}{n}\Big) \cdot \bar{g}\Big(\frac{i}{n}\Big)
\frac{1}{n} = \int_0^1 f(x) \overline{g(x)}dx. \ee

The last result follows by Riemann Integration.

\begin{defi} We say two continuous functions on $[0,1]$ are
orthogonal (or perpendicular) if their dot product equals zero.
\end{defi}

\begin{exe} Prove $x^n$ and $x^m$ are not perpendicular on $[0,1]$
for $n \neq m$. \end{exe}

We will see that the exponential function behaves very nicely
under the inner product. Define

\be e_n(x) = e^{2\pi i n x} \ \ \mbox{for} \ n \in \Z.\ee

Then a straightforward calculation shows

\be \langle e_n(x),e_m(x)\rangle = \begin{cases} 1 & \text{if
$n=m$}\\ 0 &\text{otherwise.}\end{cases}\ee

Thus $e_0(x), e_1(x), e_2(x),\dotsb$ are an \tbf{orthogonal set}
of functions, which means they are pairwise perpendicular. As each
function has length $1$, we say the functions $e_n(x)$ are an
\tbf{orthonormal set} of functions.

\begin{exe} Prove $\langle e_n(x),e_m(x)\rangle$ is $1$ if $n=m$
and $0$ otherwise. \end{exe}


\subsection{Fourier Series and $\{n \alpha \}$ }

\subsubsection{Fourier Series} Let $f$ be continuous and periodic
on $\mathbb{R}$ with period one. Define the \tbf{$n$th Fourier
coefficient} $\hat{f}(n)$ of $f$ to be

\be \hat{f}(n) = a_n = \langle f(x),e_n(x)\rangle = \int_0^1 f(x)
e^{-2\pi i n x} d x .\ee

Returning to the intuition of $\R^m$, we can think of the
$e_n(x)$'s as an infinite set of perpendicular directions. The
above is simply the projection of $f$ in the direction of
$e_n(x)$.

\begin{exe} Show
\be \langle f(x) - \hat{f}(n) e_n(x), e_n(x)\rangle = 0 .\ee This
agrees with our intuition, namely, that if you remove the
projection in a certain direction, what is left is perpendicular
to that direction.
\end{exe}

The \tbf{$N^{th}$ partial Fourier series} of $f$ is

\be s_N(x) = \sum_{n=-N}^N \hat{f}(n) e_n(x) .\ee

\begin{exe} Prove
\ben
\item $\langle f(x) - s_N(x), e_n(x)\rangle = 0$ if $|n| \le N$.
\item $|\hat{f}(n)| \le \int_0^1 |f(x)|dx$.
\item If $\langle f,f \rangle < \infty$, then
$\sum_{n=-\infty}^\infty |\hat{f}(n)|^2 \le \langle f,f \rangle$.
\item If $\langle f,f \rangle < \infty$, then $\lim_{|n|
\rightarrow \infty} \hat{f}(n) = 0$. \een
\end{exe}

As $\langle f(x) - s_N(x), e_n(x)\rangle = 0$ if $|n|\leq N$, we
might think that we just have to let $N$ go to infinity to obtain
a series $s_\infty$ such that

\be \langle f(x) - s_ \infty(x), e_n(x)\rangle = 0 .\ee

Assume that for a periodic function $g(x)$ to be orthogonal to
$e_n(x)$ for every $n$ it must be zero for every $x$. Then $f(x) -
s_\infty(x) = 0$, and hence $f = s_\infty$. Voil\'{a} -- an
expression for $f$ as a sum of exponentials! Be careful, however.
We have just glossed over the two central issues -- completeness
and, even worse, convergence. We will now see a way of avoiding
some of our problems.

\subsubsection{Weighted partial sums}

Define \be \begin{aligned} D_N(x) &= \sum_{n=-N}^N e_n(x) =
\frac{\sin((2 N + 1) \pi x)}{\sin \pi x},\\ F_N(x) &=
\frac{\sin^2( N \pi x)}{N \sin^2 \pi x} = \frac{1}{N}
\sum_{n=0}^{N-1} D_n(x) .\end{aligned}\ee  Here $F$ stands for
F\'{e}jer, $D$ for Dirichlet. In general, functions which we are
interested in taking their inner product against $f$ are called
\tbf{kernels}; thus, the Dirichlet kernel, the F\'{e}jer kernel,
etc.

Note that, no matter what $N$ is, $F_N(x)$ is positive for all
$x$.

We say that a sequence $f_1(x),f_2(x),f_3(x),\dots$ of functions
is an \tbf{approximation to the identity} if
\begin{enumerate}
\item $f_N(x)\geq 0$ for all $x$ and every $N$;
\item $\int_0^1 f_N(x) d x = 1$;
\item $\lim_{N\to \infty} \int_\delta^{1-\delta} f_N(x) d x = 0$ if $0<\delta<\frac{1}{2}$.
\end{enumerate}

\begin{thm}
The F\'{e}jer kernels $F_1(x), F_2(x), F_3(x),\dots$ are an
approximation to the identity.
\end{thm}

Proof: The first property is immediate. The second follows from
the observation that $F_N(x)$ can be written as

\be F_N(x) = e_0(x) + \frac{N-1}{N}\Big(e_{-1}(x) + e_1(x)\Big) +
\cdots, \ee

and all integrals are zero but the first, which is $1$.

To prove the third property, note that $F_N(x) \le \frac{1}{N
\sin^2 \pi \delta}$ for $\delta \le x \le 1 - \delta$. $\Box$ \\


Let $f$ be a continuous, periodic function on $\R$ with period
one. Thus, we can consider $f$ as a function on just $[0,1]$, with
$f(0) = f(1)$. Define

\be T_N(x) = \int_0^1 f(y) F_N(x-y) d y .\ee

Recall the following definition and theorem:

\begin{defi}[Uniform Continuity] A continuous function is
uniformly continuous if given an $\epsilon > 0$, there exists a
$\delta > 0$ such that $|x-y| < \delta$ implies $|f(x) - f(y)| <
\epsilon$. Note that the same $\delta$ works for all points.
\end{defi}

\begin{thm} Any continuous function on a closed, compact interval
is uniformly continuous. \end{thm}

\begin{exe} Show $x^n$ is uniformly continuous on $[a,b]$ for
$-\infty < a < b < \infty$. \end{exe}

\begin{thm}\label{thmfejer} Given $\epsilon>0$, there is an $N$ such that
\be |f(x) - T_N(x) |\leq \epsilon\ee  for every $x\in \lbrack
0,1\rbrack$.
\end{thm}
\begin{proof}
For any positive $N$,

\be \begin{aligned} T_N(x) - f(x) &= \int_0^1 f(x-y) F_N(y) d y -
f(x)\cdot 1\\
              &= \int_0^1 f(x-y) F_N(y) d y - \int_0^1 f(x) F_n(y) d y \ \mbox{(property 2 of} \ F_N)\\
              &= \int_0^\delta \Big(f(x-y) - f(x)\Big) F_N(y) d y \\ &+
\int_\delta^{1-\delta} \Big(f(x-y) - f(x)\Big) F_N(y) d y  \\ &+
\int_{1-\delta}^1 \Big(f(x-y) - f(x)\Big) F_N(y) d y .
\end{aligned} \ee

Let $\delta\in (0,1/2)$. Then, using the fact that the $F_N(x)$'s
are an approximation to the identity, we find

\be \left| \int_\delta^{1-\delta} \Big(f(x-y) - f(x)\Big) F_N(y) d
y \right| \leq
 2 \max |f(x)| \cdot \int_\delta^{1-\delta} F_N(y) d y .\ee

Since

\be \lim_{N\to \infty} \int_\delta^{1-\delta} F_N(y) d y = 0, \ee

we obtain

\be \lim_{N\to \infty} \int_\delta^{1-\delta} (f(x-y) - f(x))
F_N(y) d y  = 0 .\ee

Thus, by choosing $N$ large enough (where large depends on
$\delta$), we can insure that this piece is at most
$\frac{\epsilon}{3}$.

It remains to estimate what happens near zero. Since $f$ is
continuous and $\lbrack 0,1\rbrack$ is compact, $f$ is uniformly
continuous. Thus, we can choose $\delta$ small enough that
$|f(x-y)-f(x)|< \frac{\epsilon}{3}$ for any $x$ and any positive
$y<\delta$. Then

\be \left|\int_0^\delta \Big(f(x-y) - f(x)\Big) F_N(y) d y \right|
\leq \int_0^\delta \frac{\epsilon}{3} F_N(y) d y \leq
\frac{\epsilon}{3} \int_0^1 F_N(y) d y \leq \frac{\epsilon}{3}.\ee

Similarly

\be \left|\int_{1-\delta}^1 \Big(f(x-y) - f(x)\Big) F_N(y) d y
\right| \leq \frac{\epsilon}{3} .\ee

Therefore \be |T_N(x) - f(x)|\leq \epsilon \ee for all $N$
sufficiently large.
\end{proof}

\begin{defi}[Trigonometric Polynomials] Any finite linear
combination of the functions $e_n(x)$ is called a trigonometric
polynomial. \end{defi}

From Theorem \ref{thmfejer} we immediately get the
Stone-Weierstrass theorem:

\begin{thm}[Stone-Weierstrass] Any continuous period function can be uniformly
approximated by trigonometric polynomials. \end{thm}

\subsection{Equidistribution}
We say that a sequence $\{x_n\}$, $x_n \in \lbrack 0,1\rbrack$ is
{\em equidistributed} if \be \lim_{N\to \infty} \frac{1}{2 N +1}
\# \{n : |n|\leq N, x_n\in (a,b)\} = b-a\ee  for all $(a,b)\subset
\lbrack 0,1\rbrack$.

\begin{thm}[Weyl]
Let $\alpha$ be an irrational number in $\lbrack 0,1\rbrack$. Let
$x_n = \{n \alpha\}$, where $\{y\}$ denotes the fractional part of
$y$.
 Then the sequence $\{x_n\}$ is equidistributed.
\end{thm}
\begin{proof}
We will estimate $\frac{1}{2 N + 1} \sum_{-N}^N \chi_{(a,b)}(x_n)$
as $N\to \infty$, where $\chi_{(a,b)}$ is the function taking the
value $0$ outside $(a,b)$ and $1$ inside $(a,b)$. We call
$\chi_{(a,b)}$ the \tbf{characteristic function} of the interval
$(a,b)$.

Thus, we must show

\be \lim_{N \rightarrow \infty} \frac{1}{2 N + 1} \sum_{n=-N}^N
\chi_{(a,b)}(x_n) = b-a.\ee

Consider $e_k(x) = e^{2\pi ik x}$. Since $x_n = \{n \alpha\} =
n\alpha - [n \alpha]$ and $e_k(x) = e_k(x+m)$ for every integer
$m$,

\be e_k(x_n) = e^{2\pi ik n\alpha} .\ee

Hence

\be \begin{aligned} \frac{1}{2 N + 1} \sum_{n = -N}^N e_k(x_n) &=
\frac{1}{2 N + 1} \sum_{n=-N}^N e_k(n \alpha)
\\ &= \frac{1}{2 N + 1} \sum_{n = -N}^N (e^{2\pi i k \alpha})^n \\
&= \begin{cases} 1 & \text{if $k=0$} \\ \frac{1}{2 N + 1}
\frac{e_k(-N \alpha) - e_k((N+1) \alpha)}{1 - e_k(\alpha)} &
\text{if $k>0$.}  \end{cases}\end{aligned}  \ee

Now for a fixed irrational $\alpha$, $|1 - e_k(\alpha)| > 0$.
Therefore if $k \neq 0$:

\be \lim_{N\to \infty} \frac{1}{2 N + 1} \frac{e_k(-N \alpha) -
e_k((N+1) \alpha)}{1 - e_k(\alpha)} = 0.\ee

Let $P(x) = \sum_k a_k e_k(x)$ be a finite sum (ie, $P(x)$ is a
trigonometric polynomial). By possibly adding some zero
coefficients, we can write $P(x)$ as a sum over a symmetric range:
$P(x) = \sum_{k=-K}^K a_k a_k(x)$.

\begin{exe} Show $\int_0^1 P(x)dx = a_0$. \end{exe}

By the above arguments, we have shown that for any (finite)
trigonometric polynomial $P(x)$:

\be \lim_{N\to \infty} \frac{1}{2 N + 1} \sum_{n=-N}^N P(x_n) \
\to \ a_0 \ = \ \int_0^1 P(x)dx. \ee

Consider two approximations to the characteristic function
$\chi_{(a,b)}$:

\ben
\item $f_{1m}$: $f_{1m}(x) = 1$ if $a + \frac{1}{m} \le x \le b -
\frac{1}{m}$, drops linearly to $0$ at $a$ and $b$, and is zero
elsewhere.
\item $f_{2m}$: $f_{1m}(x) = 1$ if $a \le x \le b$, drops linearly to
$0$ at $a-\frac{1}{m}$ and $b+\frac{1}{m}$, and is zero elsewhere.
\een

Note there are trivial modifications if $a = 0$ or $b = 1$.
Clearly

\be f_{1m}(x) \ \le \ \chi_{(a,b)}(x) \ \le \ f_{2m}(x). \ee

Therefore

\be \frac{1}{2N+1} \sum_{n=-N}^N f_{1m}(x_n) \ \le \
\frac{1}{2N+1} \sum_{n=-N}^N \chi_{(a,b)}(x_n) \ \le \
\frac{1}{2N+1} \sum_{n=-N}^N f_{2m}(x_n). \ee

By Theorem \ref{thmfejer}, for each $m$, given $\epsilon > 0$ we
can find trigonometric polynomials $P_{1m}(x)$ and $P_{2m}(x)$
such that $|P_{1m}(x) - f_{1m}(x)| < \epsilon$ and $|P_{2m}(x) -
f_{2m}(x)| < \epsilon$.

As $f_{1m}$ and $f_{2m}$ are continuous functions, we can replace

\be \frac{1}{2N+1} \sum_{n=-N}^N f_{im}(x_n) \ \ \mbox{with} \ \
\frac{1}{2N+1} \sum_{n=-N}^N P_{im}(x_n) \ee

at a cost of at most $\epsilon$.

As $N \rightarrow \infty$,

\be \frac{1}{2N+1} \sum_{n=-N}^N P_{im}(x_n) \ \rightarrow \
\int_0^1 P_{im}(x)dx. \ee

But $\int_0^1 P_{1m}(x)dx = (b-a) - \frac{1}{m}$ and $\int_0^1
P_{2m}(x)dx = (b-a) + \frac{1}{m}$. Therefore, given $m$ and
$\epsilon$, we can choose $N$ large enough so that

\be (b-a) - \frac{1}{m} - \epsilon \ \le \ \frac{1}{2N+1}
\sum_{n=-N}^N \chi_{(a,b)}(x_n) \ \le \ (b-a) + \frac{1}{m} +
\epsilon. \ee

Letting $m$ tend to $\infty$ and $\epsilon$ tend to $0$, we see
$\frac{1}{2N+1} \sum_{n=-N}^N \chi_{(a,b)}(x_n) \rightarrow b -
a$.
\end{proof}

\begin{exe} Rigorously do the necessary book-keeping to prove the
previous theorem. \end{exe}

\begin{exe} Prove
\ben
\item If $\alpha \in \Q$, then $\{n\alpha\}$ is periodic.
\item If $\alpha \not\in \Q$, then no two $\{n\alpha\}$ are equal.
\een
\end{exe}





%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\chapter{Liouville's Theorem Constructing Transcendentals}



\ben
\item We prove Liouville's Theorem for the order of approximation by
rationals of real algebraic numbers.

\item We construct several transcendental numbers.

\item We define Poissonian Behaviour, and study the spacings
between the ordered fractional parts of $\{n^k \alpha\}$. \een

Lecture by Steven J. Miller; notes for the first two by Steven J.
Miller and Florin Spinu; notes for the third by Steven J. Miller.


\section{Review of Approximating by Rationals}

\begin{defi}[Approximated by rationals to order $n$]
A real number $x$ is approximated by rationals to order $n$ if
there exist a constant $k(x)$ (possibly depending on $x$) such
that there are infinitely many rational $\frac{p}{q}$ with

\be \left|x-\frac{p}{q}\right|<\frac{k(x)}{q^n}. \ee

\end{defi}

Recall that Dirichlet's Box Principle gaves us:

\be \left|x-\frac{p}{q}\right|<\frac{1}{q^2} \ee

for infintely many fractions $\frac{p}{q}$. This was proved by
choosing a large parameter $Q$, and considering  the $Q+1$
fractionary parts $\{qx\}\in [0,1)$ for $q \in \{0,\dots,Q\}$. The
box principle ensures us that there must be two different $q$'s,
say:

\be 0\leq q_1<q_2\leq Q \ee

such that both $\{q_1x\}$ and $\{q_2x\}$ belong to the same
interval $[\frac{a}{Q},\frac{a+1}{Q})$, for some $0\leq a\leq
Q-1$. Note that there are exactly $Q$ such intervals partitioning
$[0,1)$, and $Q+1$ fractionary parts! Now, the length of such an
interval is $\frac{1}{Q}$ so we get

\be \left|\{q_2x\}-\{q_1x\}\right|< \frac{1}{Q}. \ee

There exist integers $p_1$ and $p_2$ such that

\be \{q_1 x\} = q_1x - p, \ \{q_2 x\} = q_2 x - p. \ee

Letting $p = p_2 - p_1$ we find

\be \left|(q_2-q_1)x-p\right|\leq \frac{1}{Q} \ee

Let $q =q_2-q_1$, so $1\leq q\leq Q$, and the previous equation
can be rewriten as

\be \label{xx} \left|x-\frac{p}{q}\right|< \frac{1}{qQ}\leq
\frac{1}{q^2} \ee

Now, letting $Q\rightarrow \infty$, we get an infinite collection
of rational fractions $\frac{p}{q}$ satisfying the above equation.
If this collection contains only finitely many distinct fractions,
then one of these fractions, say $\frac{p_0}{q_0}$, would occur
for infintely many choices $Q_k$ of $Q$, thus giving us:

\be \left|x-\frac{p_0}{q_0}\right|< \frac{1}{qQ_k} \rightarrow 0,
\ee

as $k\rightarrow \infty$. This implies that $x=\frac{p_0}{q_0}\in
\Q$. So, unles $x$ is a rational number, we can find infinitely
many \textit{distinct} rational numbers $\frac{p}{q}$ satisfying
Equation \ref{xx}. This means that any real, irrational  number
can be approximated to order $n=2$ by rational numbers.


\section{Liouville's Theorem}

\begin{thm}[Liouville's Theorem] Let $x$ be a real algebraic number
of degree $n$. Then $x$ is approximated by rationals to order at
most $n$.
\end{thm}

\begin{proof}
Let

\be f(X) \ = \ a_nX^n+\cdots a_1X+a_0 \ee

be the polynomial with integer coefficients of smallest degree
(minimal polynomial) such that $x$ satisfies

\be f(x) \ = \ 0. \ee

Note that $\deg x=\deg f$ and the condition of minimality implies
that $f(X)$ is irreducible over $\Z$. Further, a well known result
from algebra states that a polynomial irreducible over $\Z$ is
also irreducible over $\Q$.

In particular, as $f(X)$ is irreducible over $\Q$, $f(X)$ does not
have any rational roots. If it did, then $f(X)$ would be divisible
by a linear polynomial $(X - \frac{a}{b})$. Let $G(X) =
\frac{f(X)}{X - \frac{a}{b}}$. Clear denominators (multiply
throughout by $b$), and let $g(X) = bG(X)$. Then $\deg g = \deg f
- 1$, and $g(x) = 0$. This contradicts the minimality of $f$ (we
choose $f$ to be a polynomial of smallest degree such that $f(x) =
0$). Therefore, $f$ is non-zero at every rational.

Let

\be M \ = \ \displaystyle{\sup _{|z-x|<1}|f'(z)|}. \ee

Let now $\frac{p}{q}$ be a rational such that
$\left|x-\frac{p}{q}\right|<1$. The Mean Value Theorem gives us
that

\be \label{zut} \left|f\left(\frac{p}{q}\right)-f(x)\right|=
\left|f'(c)\left(x-\frac{p}{q}\right)\right|\leq
M\left|x-\frac{p}{q}\right| \ee

where $c$ is some real number between $x$ and $\frac{p}{q}$;
$|c-x|<1$ for $\frac{p}{q}$ moderately close to $x$.

Now we use the fact that $f(X)$ does not have any rational roots:

\be 0\neq f\left(\frac{p}{q}\right)=
a_n\left(\frac{p}{q}\right)^n+\cdots + a_0 =\frac{a_np^n+\cdots
a_1p^{n-1}q+a_0q^n}{q^n} \ee

The numerator of the last term is a nonzero integer, hence it has
absolute value at least $1$. Since we also know that $f(x)=0$ it
follows that

\be \label{zutt} \left|f\left(\frac{p}{q}\right)-f(x)\right|=
\left|f\left(\frac{p}{q}\right)\right| =\frac{\left|a_np^n+\cdots
a_1p^{n-1}q+a_0q^n\right|}{q^n} \geq \frac{1}{q^n}. \ee

Combining the equations \ref{zut} and  \ref{zutt}, we get:

\be \label{zuttt} \frac{1}{q^n}\leq M\left|x-\frac{p}{q}\right| \
\Rightarrow \ \frac{1}{Mq^n}\leq \left|x-\frac{p}{q}\right| \ee

whenever $|x-\frac{p}{q}|<1$. This last equation shows us that $x$
can be approximated by rationals to order at most $n$. For assume
it was otherwise, namely that $x$ can be approximated to order
$n+\epsilon$. Then we would have an infinite sequence of distinct
rational numbers $\{\frac{p_i}{q_i}\}_{i\geq 1}$ and a constant
$k(x)$ depending only on $x$ such that

\be \left|x-\frac{p_i}{q_i}\right| \ < \
\frac{k(x)}{q_i^{n+\epsilon}}. \ee

Since the numbers $\frac{p_i}{q_i}$ converge to $x$ we can assume
that they already are in the interval $(x-1,x+1)$. Hence they also
satisfy Equation \ref{zuttt}:

\be \frac{1}{q_i^n}\leq M \left|x-\frac{p_i}{q_i}\right|. \ee

Combining the last two equations we get

\be \frac{1}{Mq_i^n}\leq \left|x-\frac{p_i}{q_i}\right| \ < \
\frac{k(x)}{q_i^{n+\epsilon}}, \ee

hence

\be q_i^{\epsilon} \ < \ M \ee

and this is clearly impossible for arbitrarily large $q$ since
$\epsilon > 0$ and $q_i\rightarrow \infty$.

\end{proof}

\begin{exe}
Justify the fact that if  $\{\frac{p_i}{q_i}\}_{i\geq 1}$ is a
rational approximation to order $n\geq 1$ of $x$, then
$q_i\rightarrow \infty$.
\end{exe}

\begin{rek}
\label{zzut} So far we have seen that the order to which an
algebraic number can be approximated by rationals is bounded by
its degree. Hence if a real, irrational number $\alpha \notin \Q$
can be approximated by rationals to an arbitrary large order, then
$\alpha$ must be transcendental! This provides us with a recipe
for constructing transcendental numbers.
\end{rek}



\section{Constructing Transcendental Numbers}

\subsection{$\sum_m 10^{-m!}$}

The following construction of transcendental numbers is due to
Liouville.

\begin{thm}
The number

\be x\ = \ \sum_{m=1}^{\infty}\frac{1}{10^{m!}} \ee

is transcendental.
\end{thm}

\begin{proof}
The series defining $x$ is convergent, since it is dominated by
the geometric series $\sum\frac{1}{10^m}$. In fact, the series
converges very rapidly and it is this high rate of convergence
that will yield $x$ is transcendental.

Fix $N$ large, and let $n>N$. Write

\be \frac{\pn}{\qn} \ = \ \sum_{m=1}^n\frac{1}{10^{m!}} \ee

with $\pn,\qn>0$ and $(\pn,\qn)=1$. Then
$\{\frac{\pn}{\qn}\}_{n\geq 1}$ is a monotone increasing sequence
converging to $x$. In particular, all these rational numbers are
distinct. Not also that $\qn$ must divide $10^{n!}$, which implies

\be \qn\leq 10^{n!}. \ee

Using this, we get

\bea 0<x-\frac{\pn}{\qn}&=& \sum_{m>n}\frac{1}{10^{m!}}=
\frac{1}{10^{(n+1)!}}\left(1+\frac{1}{10^{n+2}}+
\frac{1}{10^{(n+2)(n+3)}}+\cdots\right)\nonumber\\
&<&\frac{2}{10^{(n+1)!}}=\frac{2}{(10^{n!})^{n+1}}\nonumber\\
&<&\frac{2}{\qn ^{n+1}}\leq \frac{2}{\qn ^N}. \eea

This gives an approximation by rationals of order $N$ of $x$.
Since $N$ can be chosen arbitrarily large, this implies that $x$
can be approximated by rationals to arbitrary order. We can
conclude, in view of our precious remark \ref{zzut} that $x$ is
transcendental.
\end{proof}


\subsection{$[10^{1!},10^{2!},\dots]$}

\begin{thm}

The number

\be y=[10^{1!},10^{2!},\dots] \ee

is transcendental.
\end{thm}

\begin{proof}
Let $\frac{\pn}{\qn}$ be the continued fraction of $[10^{1!}\cdots
10^{n!}]$. Then

\bea \label{zm} \left|y-\frac{\pn}{\qn}\right|&=&\frac{1}{q_n
q'_{n+1}}=
\frac{1}{\qn (a'_{n+1}\qn +q_{n-1})}\nonumber\\
&<&\frac{1}{a_{n+1}}=\frac{1}{10^{(n+1)!}}. \eea

Since $q_k=a_nq_{k-1}+q_{n-2}$, it implies that $q_k>q_{k-1}$
Also, $q_{k+1}=a_{k+1}q_n+q_{k-1}$, so we get

\be \frac{q_{k+1}}{q_k}=a_{k+1}+\frac{q_{k-1}}{q_k} <a_{k+1}+1.\ee

Hence writing this inequality for  $k=1,\cdots, n-1$ we obtain

\bea \label{zzm}
q_n=q_1\frac{q_2}{q_1}\frac{q_3}{q_2}\cdots\frac{q_n}{q_{n-1}}
&<&(a_1+1)(a_2+1)\cdots(a_n+1)\nonumber\\
&=&(1+\frac{1}{a_1})\cdots(1+\frac{1}{a_n})a_1\cdots a_n\nonumber\\
&<&2^na_1\cdots a_n=2^n10^{1!+\cdots+n!}\nonumber\\
&<&10^{2n!}=a_n^2 \eea

Combining equations \ref{zm} and \ref{zzm} we get:

\bea \left|y-\frac{\pn}{\qn}\right|&<&\frac{1}{a_{n+1}}
=\frac{1}{a_n^{n+1}}\nonumber\\
&<&\left(\frac{1}{a_n^2}\right)^{\frac{n}{2}}
<\left(\frac{1}{q_n^2}\right)^{\frac{n}{2}}\nonumber\\
&=&\frac{1}{q_n^{n/2}}. \eea

In this way we get, just as in the previous theorem, an
approximation of $y$ by rationals to arbitrary order. This proves
that $y$ is transcendental.

\end{proof}

\subsection{Buffon's Needle and $\pi$}

Consider a collection of infinitely long parallel lines in the
plane, where the spacing between any two adjacent lines is $d$.
Let the lines be located at $x = 0, \pm d, \pm 2d, \dots$.
Consider a rod of length $l$, where for convenience we assume $l <
d$.

If we were to \emph{randomly} throw the rod on the plane, what is
the probability it hits a line? This question was first asked by
Buffon in $1733$.

Because of the vertical symmetry, we may assume the center of the
rod lies on the line $x = 0$, as shifting the rod (without
rotating it) up or down will not alter the number of
intersections. By the horizontal symmetry, we may assume
$-\frac{d}{2} \le x < \frac{d}{2}$. We posit that all values of
$x$ are equally likely. As $x$ is continuous distributed, we may
add in $x = \frac{d}{2}$ without changing the probability. The
probability density function of $x$ is $\frac{dx}{d}$.

Let $\theta$ be the angle the rod makes with the $x$-axis. As each
angle is equally likely, the probability density function of
$\theta$ is $\frac{d\theta}{2\pi}$.

We assume that $x$ and $\theta$ are chosen independently. Thus,
the probability density for $(x,\theta)$ is $\frac{dx d\theta}{d
\cdot 2\pi}$.

The projection of the rod (making an angle of $\theta$ with the
$x$-axis) along the $x$-axis is $l \cdot |\cos \theta|$. If $|x|
\le l \cdot |\cos \theta|$, then the rod hits exactly one vertical
line exactly once; if $x > l \cdot |\cos \theta|$, the rod does
not hit a vertical line. Note that if $l > d$, a rod could hit
multiple lines, making the arguments more involved.

Thus, the probability a rod hits a line is

\bea p & \ = \ &  \int_{\theta = 0}^{2\pi} \int_{x=-l \cdot |\cos
\theta|}^{l \cdot |\cos \theta|} \frac{dxd\theta}{d \cdot 2\pi}
\nonumber\\ & \ = \ & \int_{\theta = 0}^{2\pi} \frac{l \cdot |\cos
\theta|}{d} \frac{d\theta}{2\pi} \nonumber\\ & \ = \ &
\frac{2l}{\pi d}. \eea

\begin{exe} Show
\be \frac{1}{2\pi} \int_{0}^{2\pi} |\cos \theta| d\theta \ = \
\frac{2}{\pi}. \ee
\end{exe}

Let $A$ be the random variable which is the number of
intersections of a rod of length $l$ thrown against parallel
vertical lines separated by $d > l$ units. Then

\be \twocase{A =}{1}{with probability $\frac{2l}{\pi d}$}{0}{with
probability $1 - \frac{2l}{\pi d}$}. \ee

If we were to throw $N$ rods independently, since the expected
value of a sum is the sum of the expected values (Lemma
\ref{lemmeanofsum}), we expect to observe

\be N \cdot \frac{2l}{\pi d} \ee

intersections.

Turning this around, let us throw $N$ rods, and let $I$ be the
number of observed intersections of the rods with the vertical
lines. Then

\be I \ \approx \ N \cdot \frac{2l}{\pi d} \ \ \ \to \ \ \ \pi \
\approx \ \frac{N}{I} \cdot \frac{2l}{d}. \ee

The above is an \emph{experimental} formula for $\pi$!



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\chapter{Poissonian Behavior and $\{n^k \alpha\}$}

We now define Poissonian Bevahior, and investigate the normalized
spacings of the fractional parts of $n^2 \alpha$. Lecture and
notes by Steven J. Miller.


\section{Equidistribution}

We say a sequence of number $x_n \in [0,1)$ is equidistributed if

\be \lim_{N \to \infty} \frac{ \#\{n: 1 \le n \le N \ \text{and} \
x_n \in [a,b] \} }{N} \ = \ b - a \ee

for any subinterval $[a,b]$ of $[0,1]$.

Recall Weyl's Result: If $\alpha \not\in \Q$, then the fractional
parts $\{n \alpha\}$ are equidistributed. Equivalently, $n\alpha$
mod $1$ is equidistributed.

Similarly, one can show that for any integer $k$, $\{n^k \alpha\}$
is equidistributed. See Robert Lipshitz's paper for more details.


\section{Point Masses and Induced Probability Measures}

Recall from physics the concept of a unit point mass located at $x
= a$. Such a point mass has no length (or, in higher dimensions,
width or height), but finite mass. As mass is the integral of the
density over space, a finite mass in zero volume (or zero length
on the line) implies an infinite density.

We can make this more precise by the notion of an Approximation to
the Identity.

\begin{defi}[Approximation to the Identity] A sequence of
functions $g_n(x)$ is an approximation to the identity (at the
origin) if

\ben
\item $g_n(x) \ge 0$.
\item $\int g_n(x)dx = 1$.
\item Given $\epsilon, \delta > 0$ there exists $N > 0$ such that for all
$n > N$, $\int_{|x| > \delta} g_n(x) dx < \epsilon$. \een

We represent the limit of any such family of $g_n(x)$s by
$\delta(x)$.
\end{defi}

If $f(x)$ is a nice function (say near the origin its Taylor
Series converges) then

\be\label{eqapproxidfg} \int f(x) \delta(x) dx \ = \ \lim_{n \to
\infty} \int f(x) g_n(x)\ =  \ f(0). \ee

\begin{exe} Prove Equation \ref{eqapproxidfg}. \end{exe}

Thus, in the limit the functions $g_n$ are acting like point
masses. We can consider the probability densities $g_n(x)dx$ and
$\delta(x)dx$. For $g_n(x)dx$, as $n \to \infty$, almost all the
probability is concentrated in a narrower and narrower band about
the origin; $\delta(x)dx$ is the limit with all the mass at one
point. It is a discrete (as opposed to continuous) probability
measure.

Note that $\delta(x-a)$ acts like a point mass; however, instead
of having its mass concentrated at the origin, it is now
concentrated at $a$.

\begin{exe} Let
\be \twocase{g_n(x) \ = \ }{n}{if $|x| \le
\frac{1}{2n}$}{0}{otherwise} \ee Prove $g_n(x)$ is an
approximation to the identity at the origin. \end{exe}

\begin{exe} Let
\be g_n(x) \ = \ c \frac{\frac{1}{n}}{\frac{1}{n^2} + x^2}. \ee
Find $c$ such that the above is an approximation to the identity
at the origin. \end{exe}

Given $N$ point masses located at $x_1$, $x_2, \dots, x_N$, we can
form a probability measure

\be \mu_N(x)dx = \frac{1}{N} \sum_{n=1}^N \delta(x - x_n)dx. \ee

Note $\int \mu_N(x) dx = 1$, and if $f(x)$ is a nice function,

\be\label{eqfxpxinduced} \int f(x)\mu_N(x)dx \ = \
\frac{1}{N}\sum_{n=1}^N f(x_n). \ee

\begin{exe} Prove Equation \ref{eqfxpxinduced} for nice $f(x)$.
\end{exe}

Note the right hand side of Equation \ref{eqfxpxinduced} looks
like a Riemann sum. Or it \emph{would} look like a Riemann sum if
the $x_n$s were equidistributed. In general the $x_n$s will not be
equidistributed, but assume for any interval $[a,b]$ that as $N
\to \infty$, the fraction of $x_n$s ($1 \le n \le N$) in $[a,b]$
goes to $\int_a^b p(x)dx$ for some nice function $p(x)$:

\be\label{eqconvtomeasure} \lim_{N \to \infty} \frac{\# \{n: 1 \le
n \le N \ \text{and} \ x_n \in [a,b] \} }{N} \ \to \ \int_a^b
p(x)dx. \ee

In this case, if $f(x)$ is nice (say twice differentiable, with
first derivative uniformly bounded), then

\bea \int f(x)\mu_N(x)dx & \ = \ & \frac{1}{N}\sum_{n=1}^N f(x_n)
\nonumber\\ & \ \approx \ & \sum_{k=-\infty}^\infty f\Big(
\frac{k}{N} \Big) \frac{\# \{n: 1 \le n \le N \ \text{and} \ x_n
\in \Big[\frac{k}{N},\frac{k+1}{N}\Big]\} }{N} \nonumber\\ & \to \
& \int f(x) p(x)dx. \eea

\begin{defi}[Convergence to $p(x)$] If the sequence of points
$x_n$ satisfies Equation \ref{eqconvtomeasure} for some nice
function $p(x)$, we say the probability measures $\mu_N(x)dx$
converge to $p(x)dx$. \end{defi}

\section{Neighbor Spacings}

We now consider finer questions. Let $\alpha_n$ be a collection of
points in $[0,1)$. We order them by size:

\be 0 \le \alpha_{\sigma(1)} \le \alpha_{\sigma(2)} \le \cdots \le
\alpha_{\sigma(N)}, \ee

where $\sigma$ is a permutation of $123\cdots N$. Note the
ordering depends crucially on $N$. Let $\beta_j =
\alpha_{\sigma(j)}$.

We consider how the differences $\beta_{j+1} - \beta_j$ are
distributed. We will use a slightly different definition of
distance, however.

Recall $[0,1)$ is equivalent to the unit circle under the map $x
\to e^{2\pi i x}$. Thus, the numbers $.999$ and $.001$ are
actually very close; however, if we used the standard definition
of distance, then $|.999 - .001| = .998$, which is quite large.
Wrapping $[0,1)$ on itself (identifying $0$ and $1$), we see that
$.999$ and $.001$ are separated by $.002$.


\begin{defi}[mod $1$ distance] Let $x, y \in [0,1)$. We define the
mod $1$ distance from $x$ to $y$, $||x - y||$, by

\be ||x - y|| \ = \ \min\Big\{ |x - y|, \ 1 - |x - y| \Big\}. \ee
\end{defi}

\begin{exe} Show that the mod $1$ distance between any two numbers
in $[0,1)$ is at most $\foh$. \end{exe}



In looking at spacings between the $\beta_j$s, we have $N-1$ pairs
of neighbors:

\be (\beta_2,\beta_1),\ (\beta_3,\beta_2), \ \dots, \
(\beta_N,\beta_{N-1}). \ee

These pairs give rise to spacings $\beta_{j+1} - \beta_j \in
[0,1)$.

We can also consider the pair $(\beta_1,\beta_N)$. This gives rise
to the spacing $\beta_1 - \beta_N \in [-1,0)$; however, as we are
studying this sequence mod $1$, this is equivalent to $\beta_1 -
\beta_N + 1 \in [0,1)$.

\tbf{Henceforth, whenever we perform any arithmetic operation, we
always mean mod $1$; thus, our answers always live in $[0,1)$}

\begin{defi}[Neighbor Spacings] Given a sequence of numbers
$\alpha_n$ in $[0,1)$, fix an $N$ and arrange the numbers
$\alpha_n$ ($n \le N$) in increasing order. Label the new sequence
$\beta_j$; note the ordering will depend on $N$. Let $\beta_{-j} =
\beta_{N-j}$ and $\beta_{N+j} = \beta_{j}$.

\ben
\item The nearest neighbor spacings are the numbers $\beta_{j+1} -
\beta_j$, $j = 1$ to $N$.
\item The $k^{\text{th}}$-neighbor spacings are the numbers
$\beta_{j+k} - \beta_j$, $j = 1$ to $N$. \een

Remember to take the differences $\beta_{j+k} - \beta_j$ mod $1$.
\end{defi}

\begin{exe} Let $\alpha = \sqrt{2}$, and let $\alpha_n = \{n
\alpha\}$ or $\{n^2 \alpha\}$. Calculate the nearest neighbor and
the next-nearest neighbor spacings in each case for $N = 10$.
\end{exe}

\begin{defi}[wrapped unit interval] We call $[0,1)$, when all arithmetic
operations are done mod $1$, the wrapped unit interval. \end{defi}

\section{Poissonian Behavior}

Let $\alpha \not\in \Q$. Fix a positive integer $k$, and let
$\alpha_n = \{n^k \alpha\}$. As $N \to \infty$, look at the
ordered $\alpha_n$s, denoted by $\beta_n$. How are the nearest
neighbor spacings of $\beta_n$ distributed? How does this depend
on $k$? On $\alpha$? On $N$?

Before discussing this problem, we consider a simpler case. Fix
$N$, and consider $N$ independent random variables $x_n$. Each
random variable is chosen from the uniform distribution on
$[0,1)$; thus, the probability that $x_n \in [a,b)$ is $b-a$.

Let $y_n$ be the $x_n$s arranged in increasing order. How do the
neighbor spacings behave?

First, we need to decide what is the correct scale to use for our
investigations. As we have $N$ objects on the wrapped unit
interval, we have $N$ nearest neighbor spacings. Thus, we expect
the average spacing to be $\frac{1}{N}$.

\begin{defi}[Unfolding] Let $z_n = N y_n$. The numbers
$z_n = N y_n$ have unit mean spacing. Thus, while we expect the
average spacing between adjacent $y_n$s to be $\frac{1}{N}$ units,
we expect the average spacing between adjacent $z_n$s to be $1$
unit.
\end{defi}

So, the probability of observing a spacing as large as
$\frac{1}{2}$ between adjacent $y_n$s becomes negligible as $N \to
\infty$. What we should ask is what is the probability of
observing a nearest neighbor spacing of adjacent $y_n$s that is
\emph{half} the average spacing. In terms of the $z_n$s, this will
correspond to a spacing between adjacent $z_n$s of $\foh$ a unit.


\subsection{Nearest Neighbor Spacings}

By symmetry, on the wrapped unit interval the expected nearest
neighbor spacing is independent of $j$. Explicitly, we expect
$\beta_{j+1} - \beta_j$ to have the same distribution as
$\beta_{i+1} - \beta_i$.

What is the probability that, when we order the $x_n$s in
increasing order, the next $x_n$ after $x_1$ is located between
$\frac{t}{N}$ and $\frac{t+\Delta t}{N}$? Let the $x_n$s in
increasing order be labeled $y_1 \le y_2 \le \cdots \le y_N$, $y_n
= x_{\sigma(n)}$.

As we are choosing the $x_n$s independently, there are
$\ncr{N-1}{1}$ choices of subscript $n$ such that $x_n$ is nearest
to $x_1$. This can also be seen by symmetry, as each $x_n$ is
equally likely to be the first to the \emph{right} of $x_1$
(where, of course, $.001$ is just a little to the right of
$.999$), and we have $N-1$ choices left for $x_n$.

The probability that $x_n \in \Big[\frac{t}{N}, \frac{t+\Delta
t}{N}\Big]$ is $\frac{\Delta t}{N}$.

For the remaining $N-2$ of the $x_n$s, each must be further than
$\frac{t+\Delta t}{N}$ from $x_n$. Thus, they must \emph{all} lie
in an interval (or possibly two intervals if we wrap around) of
length $1 - \frac{t+\Delta t}{N}$. The probability that they all
lie in this region is $\Big( 1 - \frac{t+\Delta t}{N}
\Big)^{N-2}$.

Thus, if $x_1 = y_l$, we want to calculate the probability that
$||y_{l+1} - y_l|| \in \Big[\frac{t}{N}, \frac{t+\Delta
t}{N}\Big]$. This is

\bea \text{Prob}\Bigg( ||y_{l+1} - y_l|| \in \Big[\frac{t}{N},
\frac{t+\Delta t}{N}\Big] \Bigg) &=& \ncr{N-1}{1} \cdot
\frac{\Delta t}{N} \cdot \Big( 1 - \frac{t+\Delta t}{N}
\Big)^{N-2} \nonumber\\ &=& \Big(1 - \frac{1}{N}\Big) \cdot \Big(
1 - \frac{t+\Delta t}{N} \Big)^{N-2} \Delta t. \nonumber\\ & &
\eea

For $N$ enormous and $\Delta t$ small,

\bea \Big(1 - \frac{1}{N}\Big) \ \ \ \ \ \ \ \ \ \ \ & \ \approx \ & 1 \nonumber\\
\Big( 1 - \frac{t+\Delta t}{N} \Big)^{N-2} & \ \approx \ &
e^{-(t+\Delta t)} \ \approx \ e^{-t}. \eea

Thus

\be \text{Prob}\Bigg( ||y_{l+1} - y_l|| \in \Big[\frac{t}{N},
\frac{t+\Delta t}{N}\Big] \Bigg) \ \to \ e^{-t}\Delta t. \ee

\begin{rek}\label{rekpossion} The above argument is infinitesimally wrong. Once
we've located $y_{l+1}$, the remaining $x_n$s do not need to be
more than $\frac{t+ \Delta t}{N}$ units to the right of $x_1 =
y_l$; they only need to be further to the right than $y_{l+1}$. As
the incremental gain in probabilities for the locations of the
remaining $x_n$s is of order $\Delta t$, these contributions will
not influence the large $N$, small $\Delta t$ limits. Thus, we
ignore these effects. \end{rek}

To rigorously derive the limiting behavior of the nearest neighbor
spacings using the above arguments, one would integrate over $x_m$
ranging from $\frac{t}{N}$ to $\frac{t+\Delta t}{N}$, and the
remaining events $x_n$ would be in the a segment of length $1 -
x_m$. As

\be \Big| \Big(1 - x_m\Big) - \Big( 1 - \frac{t+\Delta t}{N} \Big)
\Big| \ \le \ \frac{\Delta t}{N}, \ee

this will lead to corrections of higher order in $\Delta t$, hence
negligible.

We can rigorously avoid this by instead considering the following:

\ben
\item Calculate the probability that all the other $x_n$s
are at least $\frac{t}{N}$ units to the right of $x_1$. This is

\be p_t \ = \ \Big( 1 - \frac{t}{N} \Big)^{N-1} \ \to \ e^{-t}.
\ee

\item Calculate the probability that all the other $x_n$s
are at least $\frac{t+\Delta t}{N}$ units to the right of $x_1$.
This is

\be p_{t+ \Delta t} \ = \ \Big( 1 - \frac{t+\Delta t}{N}
\Big)^{N-1} \ \to \ e^{-(t+\Delta t)}. \ee

\item The probability that no $x_n$s are within $\frac{t}{N}$
units to the right of $x_1$ but at least one $x_n$ is between
$\frac{t}{N}$ and $\frac{t+\Delta t}{N}$ units to the right is
$p_{t+\Delta t} - p_t$:

\bea p_t - p_{t+\Delta t} & \ \to \ & e^{-t} - e^{-(t+\Delta t)}
\nonumber\\ & \ = \ & e^{-t} \Big(1 - e^{-\Delta t} \Big)
\nonumber\\ & \ = \ & e^{-t} \Bigg(1 - 1 + \Delta t + O\Big(
(\Delta t)^2 \Bigg) \nonumber\\ & \ \to \ & e^{-t} \Delta t. \eea

\een

\begin{defi}[Unfolding Spacings] If $y_{l+1} - y_l \in \Big[\frac{t}{N},
\frac{t+\Delta t}{N}\Big]$, then $N(y_{l+1} - y_l) \in [t,t+\Delta
t]$. The new spacings $z_{l+1} - z_l$ have unit mean spacing.
Thus, while we expect the average spacing between adjacent $y_n$s
to be $\frac{1}{N}$ units, we expect the average spacing between
adjacent $z_n$s to be $1$ unit.
\end{defi}


\subsection{$k^{\text{th}}$ Neighbor Spacings}

Similarly, one can easily analyze the distribution of the
$k^{\text{th}}$ neighbor spacings when each $x_n$ is chosen
independently from the uniform distribution on $[0,1)$.

Again, consider $x_1 = y_l$. Now we want to calculate the
probability that $y_{l+k}$ is between $\frac{t}{N}$ and $\frac{t +
\Delta t}{N}$ units to the \emph{right} of $y_l$.

Therefore, we need exactly $k-1$ of the $x_n$s to lie between $0$
and $\frac{t}{N}$ units to the right of $x_1$, exactly one $x_n$
(which will be $y_{l+k}$) to lie between $\frac{t}{N}$ and
$\frac{t+\Delta t}{N}$ units to the right of $x_1$, and the
remaining $x_n$s to lie at least $\frac{t+\Delta t}{N}$ units to
the right of $y_{l+k}$.

\begin{rek} We face the same problem discussed in Remark
\ref{rekpossion}; a similar argument will show that ignoring these
affects will not alter the limiting behavior. Therefore, we will
make these simplifications. \end{rek}

There are $\ncr{N-1}{k-1}$ ways to choose the $x_n$s that are at
most $\frac{t}{N}$ units to the right of $x_1$; there is then
$\ncr{(N-1)-(k-1)}{1}$ ways to choose the $x_n$ between
$\frac{t}{N}$ and $\frac{t+\Delta t}{N}$ units to the right of
$x_1$.

Thus,

\bea & & \text{Prob}\Bigg( ||y_{l+k} - y_l|| \in \Big[\frac{t}{N},
\frac{t+\Delta t}{N}\Big] \Bigg) \ = \ \nonumber\\ &=&
\ncr{N-1}{k-1}\Big(\frac{t}{N}\Big)^{k-1} \cdot
\ncr{(N-1)-(k-1)}{1} \frac{\Delta t}{N} \cdot \Big( 1 -
\frac{t+\Delta t}{N} \Big)^{N-(k+1)} \nonumber\\ &=&
\frac{(N-1)\cdots(N-1-(k-2))}{N^{k-1}} \frac{(N-1)-(k-1)}{N}
\frac{t^{k-1}}{(k-1)!} \Big( 1 - \frac{t+\Delta t}{N}
\Big)^{N-(k+1)} \Delta t \nonumber\\ & \to &
\frac{t^{k-1}}{(k-1)!} e^{-t} \Delta t. \eea

Again, one way to avoid the complications is to integrate over
$x_m$ ranging from $\frac{t}{N}$ to $\frac{t+\Delta t}{N}$.

Or, similar to before, we can proceed more rigorously as follows:

\ben
\item Calculate the probability that exactly $k-1$ of the other $x_n$s
are at most $\frac{t}{N}$ units to the right of $x_1$, and the
remaining $(N-1)-(k-1)$ of the $x_n$s are at least $\frac{t}{N}$
units to the right of $x_1$. As there are $\ncr{N-1}{k-1}$ ways to
choose $k-1$ of the $x_n$s to be at most $\frac{t}{N}$ units to
the right of $x_1$, this probability is

\bea p_t & \ = \ & \ncr{N-1}{k-1} \Big( \frac{t}{N} \Big)^{k-1}
\Big(1 - \frac{t}{N} \Big)^{(N-1)-(k-1)} \nonumber\\ & \ \to \ &
\frac{N^{k-1}}{(k-1)!} \frac{t^{k-1}}{N^{k-1}} e^{-t} \nonumber\\
&  \ \to \ & \frac{t^{k-1}}{{k-1}!} e^{-t}. \eea

\item Calculate the probability that exactly $k-1$ of the other $x_n$s
are at most $\frac{t}{N}$ units to the right of $x_1$, and the
remaining $(N-1)-(k-1)$ of the $x_n$s are at least $\frac{t+\Delta
t}{N}$ units to the right of $x_1$. Similar to the above, this
gives

\bea p_t & \ = \ & \ncr{N-1}{k-1} \Big( \frac{t}{N} \Big)^{k-1}
\Big(1 - \frac{t+\Delta t}{N} \Big)^{(N-1)-(k-1)} \nonumber\\ & \
\to \ & \frac{N^{k-1}}{(k-1)!} \frac{t^{k-1}}{N^{k-1}}
e^{-(t+\Delta t)} \nonumber\\ &  \ \to \ & \frac{t^{k-1}}{(k-1)!}
e^{-(t+\Delta t)}. \eea


\item The probability that exactly $k-1$ of the $x_n$s are within
$\frac{t}{N}$ units to the right of $x_1$ and at least one $x_n$
is between $\frac{t}{N}$ and $\frac{t+\Delta t}{N}$ units to the
right is $p_{t+\Delta t} - p_t$:

\be p_t - p_{t+\Delta t}  \ \to \ \frac{t^{k-1}}{(k-1)!} e^{-t} -
\frac{t^{k-1}}{(k-1)!} e^{-(t+\Delta t)} \ \to \
\frac{t^{k-1}}{(k-1)!} e^{-t} \Delta t. \ee

\een

Note that when $k = 1$, we recover the nearest neighbor spacings.

\section{Induced Probability Measures}

We have proven the following:

\begin{thm} Consider $N$ independent random variables $x_n$ chosen
from the uniform distribution on the wrapped unit interval
$[0,1)$. For fixed $N$, arrange the $x_n$s in increase order,
labeled $y_1 \le y_2 \le \cdots \le y_N$.

Form the induced probability measure $\mu_{N,1}$ from the nearest
neighbor spacings. Then as $N \to \infty$ we have

\be \mu_{N,1}(t)dt \ = \ \frac{1}{N} \sum_{n=1}^N \delta\Big(t -
N(y_n - y_{n-1})\Big)dt \ \to \ e^{-t}dt. \ee

Equivalently, using $z_n = N y_n$:

\be \mu_{N,1}(t)dt \ = \ \frac{1}{N} \sum_{n=1}^N \delta\Big(t -
(z_n - z_{n-1})\Big)dt \ \to \ e^{-t}dt. \ee

More generally, form the probability measure from the
$k^{\text{th}}$ nearest neighbor spacings. Then as $N \to \infty$
we have

\be \mu_{N,k}(t)dt \ = \ \frac{1}{N} \sum_{n=1}^N \delta\Big(t -
N(y_{n} - y_{n-k})\Big)dt \ \to \ \frac{t^{k-1}}{(k-1)!}e^{-t}dt.
\ee

Equivalently, using $z_n = N y_n$:

\be \mu_{N,k}(t)dt \ = \ \frac{1}{N} \sum_{n=1}^N \delta\Big(t -
(z_n - z_{n-k})\Big)dt \ \to \ \frac{t^{k-1}}{(k-1)!} e^{-t}dt.
\ee
\end{thm}

\begin{defi}[Poissonian Behavior] We say a sequence of points
$x_n$ has Poissonian Behavior if in the limit as $N \to \infty$
the induced probability measures $\mu_{N,k}(t)dt$ converge to
$\frac{t^{k-1}}{(k-1)!} e^{-t}dt$. \end{defi}

\begin{exe} Let $\alpha \in \Q$, and define $\alpha_n = \{n^m \alpha\}$
for some positive integer $m$. Show the sequence of points
$\alpha_n$ does not have Poissonian Behavior. \end{exe}

\begin{exe} Let $\alpha \not\in \Q$, and define $\alpha_n =
\{n \alpha\}$. Show the sequence of points $\alpha_n$ does not
have Poissonian Behavior. Hint: for each $N$, show the nearest
neighbor spacings take on at most three distinct values (the three
values depend on $N$). As only three values are ever assumed for a
fixed $N$, $\mu_{N,1}(t)dt$ cannot converge to $e^{-t}dt$.
\end{exe}



\section{Non-Poissonian Behavior}

\begin{conj} With probability one (with respect to Lebesgue Measure),
if $\alpha \not\in \Q$, if
$\alpha_n = \{n^2 \alpha\}$ then the sequence of points $\alpha_n$
is Poissonian.
\end{conj}

There are constructions which show certain irrationals give rise
to non-Poissonian behavior.

\begin{thm}\label{thmnonpoisson} Let $\alpha \in \Q$ such that $\Big| \alpha -
\frac{\pn}{\qn} \Big| < \frac{a_n}{q_n^3}$ holds infinitely often,
with $a_n \to 0$. Then there exist integers $N_j \to \infty$ such
that $\mu_{N_j,1}(t)$ does not converge to $e^{-t}dt$. \end{thm}

As $a_n \to 0$, eventually $a_n < \frac{1}{10}$ for all $n$ large.
Let $N_n = q_n$, where $\frac{\pn}{\qn}$ is a good rational
approximation to $\alpha$:

\be\label{eqpnqngoodapproxalpha} \Big| \alpha - \frac{\pn}{\qn}
\Big| < \frac{a_n}{q_n^3}. \ee

Remember that all subtractions are performed on the wrapped unit
interval. Thus, $||.999 - .001|| = .002$.

We look at $\alpha_k = \{k^2 \alpha\}$, $1 \le k \le N_n =\qn$.
Let the $\beta_k$s be the $\alpha_k$s arranged in increasing
order, and let the $\gamma_k$s be the numbers $\{k^2
\frac{\pn}{\qn} \}$ arranged in increasing order:

\bea \beta_1 & \ \le \ & \beta_2 \  \le \ \cdots \ \le \ \beta_N
\nonumber\\ \gamma_1 & \ \le \ & \gamma_2 \  \le \ \cdots \ \le \
\gamma_N. \eea

\subsection{Preliminaries}

\begin{lem} If $\beta_l = \alpha_k = \{k^2 \alpha\}$, then $\gamma_l =
 \{k^2 \frac{\pn}{\qn}\}$. Thus, the same permutation orders both the
$\alpha_k$s and the $\gamma_k$s. \end{lem}

\begin{proof}
Multiplying both sides of Equation \ref{eqpnqngoodapproxalpha} by
$k^2 \le q_n^2$ yields

\be \Big| k^2 \alpha - k^2 \frac{\pn}{\qn} \Big| \ < \ k^2
\frac{a_n}{q_n^2} \ \le \ \frac{a_n}{q_n} \ < \ \frac{1}{2q_n}.
\ee

Thus, $k^2 \alpha$ and $k^2 \frac{\pn}{\qn}$ differ by at most
$\frac{1}{2\qn}$. Therefore

\be \Big|\Big| \Big\{k^2 \alpha\Big\} - \Big\{k^2
\frac{\pn}{\qn}\Big\} \Big|\Big| \ < \ \frac{1}{2q_n}. \ee

As the numbers $\{m^2 \frac{\pn}{\qn} \}$ all have denominators of
size at most $\frac{1}{\qn}$, we see that $\{k^2
\frac{\pn}{\qn}\}$ is the closest of the $\{m^2 \frac{\pn}{\qn}
\}$ to $\{k^2 \alpha\}$.

This implies that if $\beta_l = \{k^2 \alpha\}$, then $\gamma_l =
\{k^2 \frac{\pn}{\qn}\}$, completing the proof.

\end{proof}

\begin{exe} Prove the ordering is as claimed. Hint: about each
$\beta_l = \{k^2 \alpha\}$, the closest number of the form $\{c^2
\frac{\pn}{\qn}\}$ is $\{k^2 \frac{\pn}{\qn}\}$. \end{exe}


\subsection{Proof of Theorem \ref{thmnonpoisson}}

\begin{exe}\label{exemodonedist} Assume $||a - b||, ||c - d||
< \frac{1}{10}$. Show \be || (a-b) - (c-d) || \ < \ ||a-b|| + ||c
- d||. \ee
\end{exe}

Proof of Theorem \ref{thmnonpoisson}: We have shown

\be ||\beta_l - \gamma_l|| \ < \ \frac{a_n}{\qn}. \ee

Thus, as $N_n = q_n$:

\be \Big|\Big|N_n(\beta_l - \gamma_l)\Big|\Big| \ < \ a_n, \ee

and the same result holds with $l$ replaced by $l-1$.

By Exercise \ref{exemodonedist},

\be \Big|\Big|N_n(\beta_l - \gamma_l) - N_n(\beta_{l-1} -
\gamma_{l-1}) \Big|\Big| \ < \ 2a_n. \ee

Rearranging gives

\be \Big|\Big|N_n(\beta_l - \beta_{l-1}) - N_n(\gamma_l -
\gamma_{l-1}) \Big|\Big| \ < \ 2a_n. \ee

As $a_n \to 0$, this implies the difference between
$\Big|\Big|N_n(\beta_l - \beta_{l-1})\Big|\Big|$ and $\Big|\Big|
N_n(\gamma_l - \gamma_{l-1}) \Big|\Big|$ goes to zero.

The above distance calculations were done mod $1$. The actual
differences will differ by an integer. Thus,

\be \mu_{N_n,1}^{\alpha}(t)dt \ = \ \frac{1}{N_n} \sum_{l=1}^{N_n}
\delta\Big(t - N_n(\beta_l - \beta_{l-1})\Big) \ee

and

\be \mu_{N_n,1}^{\frac{\pn}{\qn} }(t)dt \ = \ \frac{1}{N_n}
\sum_{l=1}^{N_n} \delta\Big(t - N_n(\gamma_l - \gamma_{l-1})\Big)
\ee

are extremely close to one another; each point mass from the
difference between adjacent $\beta_l$s is either within $a_n$
units of a point mass from the difference between adjacent
$\gamma_l$s, or is within $a_n$ units of a point mass an integer
number of units from a point mass from the difference between
adjacent $\gamma_l$s. Further, $a_n \to 0$.

Note, however, that if $\gamma_l = \{k^2 \frac{\pn}{\qn}\}$, then

\be N_n \gamma_l \ = \ q_n \Big\{k^2 \frac{\pn}{\qn} \Big\} \ \in
\ \N. \ee

Thus, the induced probability measure
$\mu_{N_n,1}^{\frac{\pn}{\qn}}(t)dt$ formed from the $\gamma_l$s
is supported on the integers! Thus, it is impossible for
$\mu_{N_n,1}^{\frac{\pn}{\qn}}(t)dt$ to converge to $e^{-t}dt$.

As $\mu_{N_n,1}^{\alpha}(t)dt$, modulo some possible integer
shifts, is arbitrarily close to
$\mu_{N_n,1}^{\frac{\pn}{\qn}}(t)dt$, the sequence $\{k^2
\alpha\}$ is \emph{not} Poissonian along the subsequence of $N$s
given by $N_n$, where $N_n = q_n$, $q_n$ is a denominator in a
good rational approximation to $\alpha$. $\Box$


\subsection{Measure of $\alpha \not\in \Q$ with Non-Poissonian
Behavior along a sequence $N_n$}

What is the (Lebesgue) measure of $\alpha \not\in \Q$ such that
there are infinitely many $n$ with

\be \Big| \alpha - \frac{\pn}{\qn} \Big| \ < \ \frac{a_n}{\qn}, \
a_n \to 0. \ee

If the above holds, then for any constant $k(\alpha)$, for $n$
large (large depends on both $\alpha$ and $k(\alpha)$) we have

\be \Big| \alpha - \frac{\pn}{\qn} \Big| \ < \
\frac{k(\alpha)}{\qn^{2+\epsilon}}. \ee

\begin{exe} Show this set has
(Lebesgue) measure or size $0$.
\end{exe}

Thus, almost no irrational numbers satisfy the conditions of
Theorem \ref{thmnonpoisson}, where \emph{almost no} is relative to
the (Lebesgue) measure.

\begin{exe} In a topological sense, how many algebraic numbers
satisfy the conditions of Theorem \ref{thmnonpoisson}? How many
transcendental numbers satisfy the conditions? \end{exe}

\begin{exe} Let $\alpha$ satisfy the conditions of Theorem
\ref{thmnonpoisson}. Consider the sequence $N_n$, where $N_n =
\qn$, $\qn$ the denominator of a good approximation to $\alpha$.
We know the induced probability measures
$\mu_{N_n,1}^{\frac{\pn}{\qn}}(t)dt$ and
$\mu_{N_n,1}^{\alpha}(t)dt$ do not converge to $e^{-t}dt$. Do
these measures converge to anything?
\end{exe}

\begin{rek} In \emph{The Distribution of Spacings Between the Fractional
Parts of $\{n^2 \alpha\}$} (Z. Rudnick, P. Sarnak, A. Zaharescu),
it is shown that for most $\alpha$ satisfying the conditions of
Theorem \ref{thmnonpoisson}, there \emph{is} a sequence $N_j$
along which $\mu_{N_n,1}^{\alpha}(t)dt$ \emph{does} converge to
$e^{-t}dt$.
\end{rek}





%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\chapter{More Graphs, Kloosterman, Randomness of $x \to \overline{x}$ mod $p$}

More on Graphs, Kloosterman, and the Third Problem on the
Randomness of $x \mapsto \overline{x}$ mod $p$. Review of
projective geometry and fractional linear transformations. Lecture
by Peter Sarnak; notes by Steven J. Miller.


\section{Kloosterman Sums}

Recall

\be S(a,b,p) \ = \ \ \sum_{x \ \text{mod} \ p}^* \  e\Big(
\frac{ax + b\overline{x}}{p} \Big), \ee

where $\sum^*$ means sum over all $x$ relatively prime to $p$, and
$\overline{x} \equiv x^{-1}$ mod $p$.

\begin{thm}[Weil] For $p$ an odd prime and $a,b \in \Z$,

\be |S(a,b,p)| \ \le \ 2 \sqrt{p}. \ee
\end{thm}

The above captures the randomness. We add $p-1$ numbers of modulus
$1$, and we see square-root cancellation. Weil's Theorem says the
cancellation is "like" random numbers. Recall when we added $\pm
1$, we expected to observe a sum around $\sqrt{N}$ if we had $N$
summands.

\section{Projective Geometry}

Consider the map

\bea \C & \ \to \ &  \C \cup \{\infty\} \nonumber\\   z & \
\mapsto \ & \frac{az + b}{cz+d}. \eea

We define the above as the action of the matrix

\be  \left(\begin{array}{cc}
                        a &  b  \\
                        c   & d
                          \end{array}\right)
\ee

on $z$.


$\Pone(\R)$ is identified with the perimeter of a circle, with
antipodal points (points on a diagonal, separated by $\pi$
radians) identified.



\section{Example}

\begin{defi}[$\Pone(\F_p)$] $\Pone(\F_p)$ is the projective line,

\be \Pone(\F_p) \ = \ \{0,1,\dots,\infty\}. \ee
\end{defi}

We construct a $3$-regular graph $G_p$ on $p+1$ vertices as
follows:

\ben
\item Join $x$ to $x+1$ mod $p$.
\item Join $x$ to $x-1$ mod $p$.
\item Join $x$ to $-\overline{x}$ mod $p$,
\een

where $\frac{1}{\infty} = 0$.

Form the adjacency matrix of the above graph. Is there a spectral
graph?

\begin{thm} There is a spectral gap!

\be \lambda_1(G_p) \ \le \ 2.99. \ee

These graphs are \emph{not} Ramanujan in general.
\end{thm}

We can look at the three maps as matrices

\be T = \left(\begin{array}{cc}
                        1 &  1  \\
                        0   & 1
                          \end{array}\right), \ \
S = \left(\begin{array}{cc}
                        0 &  1  \\
                        -1   & 0
                          \end{array}\right), \ \
 T^{-1} = \left(\begin{array}{cc}
                        1 &  -1  \\
                        0   & 1
                          \end{array}\right)
\ee



\begin{exe} $(ST)^3 = \pm I$. \end{exe}

\begin{exe} Show the three maps we used to create $G_p$ can be
given by

\ben
\item $x \to Tx$ (corresponding to $x \to x+1$),
\item $x \to T^{-1} x$ (corresponding to $x \to x-1$),
\item $x \to Sx$ (corresponding to $x \to -\overline{x}$.
\een
\end{exe}

\section{Stereographic Projections and Fractional Linear Transformations}

What are the analytic, $1-1$ invertible maps from $\C \to \C$?
What if we include $\infty$.

First, one might ask what is $\infty$?

Take a sphere, call the north pole $N$. Consider the infinite
plane $z = 0$.

To each point $P$ on the sphere, draw the line from $N$ to $P$,
and write down the point of intersection on the plane $z = 0$.
Call this map $S$ (stereographic projection; preserves angles),
and call the sphere $S^2 = \Pone$.

Thus,

\be S(P) \in \C, \ S(\infty) \leftrightarrow N, \ S^2 \cong \C
\cup \{\infty\}. \ee

Recall $\text{GL}_2$ are the $2\times 2$ matrices with non-zero
determinant. $SL_2(\C)$ is the group of $2 \times 2$ matrices with
determinant $1$.

Fractional Linear Transformation: $z \mapsto \frac{az+b}{cz+d}$.

If we have two linear transformations

\be \gamma = \left(\begin{array}{cc}
                        a &  b  \\
                        c  & d
                          \end{array}\right), \
\delta = \left(\begin{array}{cc}
                        a_1 &  b_1  \\
                        c_1   & d_1
                          \end{array}\right)
\ee

then

\be \gamma (\delta z) \ = \ (\gamma \delta)z, \ee

where $(\gamma \delta)$ is usual matrix multiplication.


\section{More Kesten}

Let $\Gamma$ be a group generated by

\be A_1,\ A_1^{-1}, \ A_2,\ A_2^{-1}, \ \dots \ ,\ A_k,\ A_k^{-1}.
\ee

We make a $2k$-regular graph by joining $x$ and $y$ with an edge
($x \sim y$) if and only if $x = A_j^{\pm 1} y$ for some $j$.

Define, for $f: \Gamma \to \C$,

\be Bf(x) \ = \ \sum_{x \sim y} f(y). \ee

To make the sum make sense if we have infinitely many vertices we
require $f \in l^2(\Gamma)$, the space of functions $g$ where
$\sum |g(\gamma)|^2 < \infty$.

One could ask about the spectrum of $B$ on this space. The HW
problem was on a $k$-regular tree. Make words using the
generators. Have the notion of a free group: no relation (ie, no
word is the identity word) except the trivial ones ($A_i A_i^{-1}
= I$).

\be \Gamma \ = \ \langle A_1, A_2, \dots, A_k \rangle. \ee

\begin{defi}[Free Group] $\Gamma$ is a free group if the only
relations in $\Gamma$ giving the identity are the obvious ones
(ie, the only word in $A_1, \dots, A_k$ that is the identity word
is words of the form $A_k^{-1} A_i A_i^{-1} A_j^{-1} A_j A_k$, and
so on).
\end{defi}

The graph we just spoke about, $G(\Gamma)$ on $A_1,A_1^{-1}$ up to
$A_k,A_k^{-1}$ is a $2k$-regular tree if and only if $\Gamma$ is a
free group on $A_1,\dots,A_k$. This is called a Cayley Graph
(relative to the given generators).

\begin{thm}[Kesten] The spectrum of $B$ when $\Gamma$ is free on
$k$ generators is

\be \text{spectrum}(B) \ = \ \Big[-2\sqrt{2k-1},2\sqrt{2k-1}\Big].
\ee

Further, $\Gamma$ is free on $A_1, \dots, A_k$ if and only if

\be \text{spectrum}(B) \ \subset \
\Big[-2\sqrt{2k-1},2\sqrt{2k-1}\Big]. \ee

Thus, the spectrum contains a point \emph{outside} this interval
if and only if $\Gamma$ is not free.

Finally, $2k$ is in the spectrum if and only if $\Gamma$ is
amenable (for example, abelian).

\end{thm}


\begin{cor} Our graphs $G_p$ cannot be Ramanujan, as there
\emph{is} a relation among the generators, namely $(ST)^3 = \pm
I$. \end{cor}


\section{$\text{SL}_2(\Z)$}

\begin{defi}[$\text{SL}_2(\Z)$] $\text{SL}_2(\Z)$ is the group of $2 \times 2$
matrices with unit determinant and integer coefficients.
\end{defi}

\begin{exe} Consider the matrices
\be T = \left(\begin{array}{cc}
                        1 &  1  \\
                        0   & 1
                          \end{array}\right), \ \
S = \left(\begin{array}{cc}
                        0 &  1  \\
                        -1   & 0
                          \end{array}\right), \ \
 T^{-1} = \left(\begin{array}{cc}
                        1 &  -1  \\
                        0   & 1
                          \end{array}\right)
\ee

Show these three matrices generate $\text{SL}_2(\Z)$; as $(ST)^3 =
\pm I$, this shows $\text{SL}_2(\Z)$ is not a free group.

\end{exe}

Where should $\text{SL}_2(\Z)$ act? Lubotsky: you don't understand
a group until it acts on something you know. Galois had groups
acting on roots of polynomials (permuting roots).

What does $\text{SL}_2(\Z)$ act on? It does act on the sphere, but
it is too big a space. We want to study the smallest space where
it acts reasonably.

Look at $\text{SL}_2(\R)$, the group of $2 \times 2$ matrices with
determinant $1$ and real entries. Let $z$ be in the upper half
plane, so $z = x + iy$, $y > 0$. Then

\be z \ \mapsto \ \frac{az+b}{cz+d}, \
\text{Im}\Big(\frac{az+b}{cz+d}\Big) \ > \ 0. \ee

A similar statement holds for $z$ in the lower half plane. Thus,
$\text{SL}_2(\R)$ maps the upper (lower) half plane to itself.

By shifting by an integer, we can bring any $x \in \R$ to $x' \in
[0,1)$.

Gauss was the first to draw the fundamental domain for
$\text{SL}_2(\Z)$ acting on the upper half plane:

Draw a circle of radius $1$ with center $0$; draw vertical lines
at $x = \pm \foh$, going from the point on the circle to infinity.
The region formed is called the fundamental domain for
$\text{SL}_2(\Z)$ on the upper half plane. This means that any $z$
in the upper half plane can be brought into this region.

\section{Is $x \to \overline{x}$ mod $p$ Random?}


\subsection{First Test}

\begin{que} To what extent is / how random is $x \to \overline{x}$ mod
$p$? \end{que}

There are $p$ numbers.

If look at all numbers $1 \le x \le p-1$, we get all the numbers
back (in some new order). We can look at a long segment (on what
scale?), say

\be \sqrt{p} \ \le x \le 2\sqrt{p}. \ee

Now $\overline{x}$ will be all over $[1,p-1]$. This generates
$\sqrt{p}$ numbers between $1$ and $p-1$. The average spacing is
is approximately $\frac{p-1}{\sqrt{p} \approx \sqrt{p}}$. How are
numbers spaced?

First, we need to write them in increasing order

\be 1 \le a_1 \ < \ a_2 \ < \ \cdots \ < a_l \ \le p-1. \ee

Let us denote the nearest neighbor spacings by $\Delta_1 = a_2 -
a_1$, $\Delta_2 = a_3 - a_2$ and so on. Let

\be \delta_j = \frac{\Delta_j}{\sqrt{p}}. \ee

Note the $\delta_j$s have unit mean spacing.

\begin{conj}[Naive Conjecture] We expect the spacings to follow
Poissonian Statistics. Explicitly, the distribution of spacings we
see here should be the same as that from choosing $\sqrt{p}$
numbers independently from the uniform distribution on $[0,1)$.
\end{conj}


\subsection{Second Test}

\begin{que}[Jim Propp] Does $x \to \overline{x}$ behave like a
random permutation of order $2$? \end{que}

What is the distribution of the longest increasing sub-sequence?

Given a permutation of $1,2, \dots, N$, we have $i \mapsto
\sigma(i)$. Permutations are often denoted by

\be \Bigg( {1 \atop \sigma(1)}\ {2 \atop \sigma(2)} \ \ \ \cdots \
\ \ {N \atop \sigma(N)} \Bigg) \ee

Look at the distribution of the longest increasing sub-sequence
about the mean. Normalized appropriately, what does it look like?


\subsection{Third Test: Hooley's $R^*$}

\be \sum_{x = A}^{A+N} e\Big( \frac{\overline{x}}{p}\Big), \ee

where $1 \le A \le A+N \le p-1$.

When we add numbers of modulus one, we expect square-root
cancellation.

Then

\begin{conj}[Hooley's $R^*$]
For every $\epsilon > 0$,

\be \Bigg| \sum_{x = A}^{A+N} e\Big( \frac{\overline{x}}{p}\Big)
\Bigg| \ \ll_\epsilon \ N^\foh p^\epsilon. \ee

Note $\ll_\epsilon$ means the left hand side is less than
$c_\epsilon$ times the right hand side (for some $c_\epsilon$ >
0).

Note there is no dependence on $A$ -- the only dependence is on
the size of summation $N$ and the prime $p$.

\end{conj}

If $N > \sqrt{p}$, the above can be proven. by Weil's bound.



\section{Note on Non-trivial Bound of Fourth Powers of Kloosterman
Sums}

Note on conditions arising in non-trivial bound on sum of fourth
powers of Kloosterman sums (Heath-Brown review). Supplemental
notes by Alex Barnett.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Please refer to Professor Sarnak's lecture of 10/16/02, and
Heath-Brown's review article on Kloosterman Sums.

There are six summations inherent in the desired sum \be
\sum_{a=0}^{p-1} \sum_{b=0}^{p-1} \left| \mbox{Kl} (a,b,p)
\right|^4 , \ee namely the two sums shown and one internal sum in
each of the Kl's. Using the result \be \sum_{a=0}^{p-1}
e\left(\frac{an}{p}\right) \; = \; \left\{\begin{array}{ll}
p-1, & n=0\;\; (\mbox{mod} p)\\
0, & \mbox{otherwise}
\end{array}\right.
\ee twice, turns the two $e(\cdot)$ functions into counting
conditions on the variables internal to the four Kl's. Calling
these variables $w,x,y,z$, we want the total number of ways that,
with $1 \le w,x,y,z \le p-1$ (the four surviving sums), we satisfy
both \be \label{1}
    w + x - y -z \; = 0 \;\;(p)
\ee and \be \label{2}
    \bar{w} + \bar{x} - \bar{y} -\bar{z} \; = 0 \;\; (p) .
\ee

Multiplying Eq.~\ref{2} by $wxyz$ gives \be (w+x)yz - (y+z)wx \; =
\; 0 \;\;(p). \ee Substituting Eq.~\ref{1} gives \be (w+x)(yz -
wx) \; = \; 0 \;\;(p). \ee So either $w+x = 0 \;\; (p)$ or $yz -
wx = 0 \;\; (p)$, or both.

The first set of cases has $x=-w$ from (\ref{1}) giving $z=-y$, so
there are $(p-1)^2$ choices of $w$ and $y$. For each choice $x$
and $z$ are fixed uniquely. Therefore these cases contribute
$(p-1)^2$ ways.

For the second set, we have two equations \bea
y + z &= &w + x \;\; (p) \\
yz &= &wx \;\; (p) \eea for two unknowns $y,z$, for any of the
arbitrary choices of $w,x$. You could combine these equations into
the single quadratic \be y^2 - y(w+x) + wx \; = 0 \;\; (p). \ee
Two solutions for $y$ are $y=w$ and $y=x$ (check by substitution).
Since it is a quadratic, these are the only two solutions.
Therefore the number of ways contributed is at most 2 times the
$(p-1)^2$ ways of choosing $w,x$.

Over-counting due to the and/or is at least of order $p$ or
smaller, but also can only reduce the number of ways. Therefore
the total number of ways $\le 3(p-1)^2$, which is $O(p^2)$. From
this follows the bound on the sum given in the article and
lecture.




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\chapter{Introduction to the Hardy-Littlewood Circle Method}

Introduction to the Hardy-Littlewood Circle Method. Lecture and
notes by Steven J. Miller.

\section{Problems where the Circle Method is Useful}

For each $N$, let $A_N$ be a set of non-negative integers such
that

\ben
\item $A_N \subset A_{N+1}$,
\item $|A_N| \to \infty$ as $N \to \infty$.
\een

Let $A = \lim_{N \to \infty} A_N$.

\begin{que} Let $s$ be a fixed positive integer. What can one say
about $a_1 + \cdots + a_s$? Ie, what numbers $n$ are representable
as a sum of $s$ summands from $A$? \end{que}

We consider three problems; we will mention later why we are
considering sets $A_N$.

\subsection{Waring's Problem}

Let $A$ be the set of $k^{\text{th}}$ powers of non-negative
numbers, and let

\be A_N \ = \ \{0^k, 1^k, \dots, N^k\}. \ee

\begin{que} Fix a positive integer $k$. For what positive integers
$s$ can every integer be written as a sum of $s$ numbers, each
number a $k^{\text{th}}$ power? \end{que}

Thus, in this case, we are trying to solve

\be n \ = \ a_1^k + \cdots + a_N^k. \ee

\subsection{Goldbach's Problem}

Let $A$ be the set of all prime numbers, and let $A_N$ be the set
of all primes at most $N$.

\begin{que} Can every even number be written as the sum of two
primes? \end{que}

In this example, we are trying to solve

\be 2n \ = \ a_1 + a_2, \ee

or, in more suggestive notation,

\be 2n \ = \ p_1 + p_2. \ee


\subsection{Sum of Three Primes}

Again, let $A$ be the set of all primes, and $A_N$ all primes up
to $N$.

\begin{que} Can every odd number be written as the sum of three
primes? \end{que}

Again, we are studying

\be 2n+1 \ = \ p_1 + p_2 + p_3. \ee



\section{Idea of the Circle Method}

\subsection{Introduction}

\begin{defi}[$e(z)$] We define $e(z) = e^{2\pi i z}$.
\end{defi}

\begin{exe} Let $m, n \in \Z$. Prove

\be \twocase{\int_0^1 e(nx) e(-mx)dx =}{1}{if $n =
m$}{0}{otherwise} \ee
\end{exe}

Let $A$, $A_N$ be as in any of the three problems above. Consider

\be f_N(x) \ = \ \sum_{a \in A_N} e(ax). \ee

We investigate $\Big(f_N(x)\Big)^s$:

\bea \Big(f_N(x)\Big)^s & \ = \ & \prod_{j=1}^s \sum_{a_j \in A_N}
e(a_jx) \nonumber\\ & \ = \ & \sum_m r_N(m) e(mx). \eea

The last result follows by collecting terms. When you multiply two
exponentials, you add the exponents.

Thus, when we multiply the $s$ products, how can we get a product
which gives $e(mx)$?

We have $s$ products, say $e(a_1x)$ through $e(a_Nx)$. Thus,

\be e(a_1x) \cdots e(a_Nx) \ = \ e\Big( (a_1 + \cdots + a_N)x\Big)
\ = \ e(mx). \ee

Thus, the coefficient $r_N(m)$ in $\Big(f_N(x)\Big)^s$ is the
number of ways of writing

\be m \ = \ a_1 + \cdots + a_N, \ee

with each $a_j \in A_N$.

As the elements of $A_N$ are non-negative, if $N$ is sufficiently
large $r_N(m)$ is equal to the number of ways of writing $m$ as
the sum of $s$ elements of $A$.

The problem is, if $m$ is larger than the largest term in $A_N$,
then there may be other ways to write $m$ as a sum of $s$ elements
of $A$.

\begin{lem}
\be r_N(m) \ = \ \int_0^1 \Big(f_N(x)\Big)^s e(-mx)dx. \ee
\end{lem}

Proof: direct calculation.

Note that, just because we have a closed form expression for
$r_N(m)$, this does not mean we can actually \emph{evaluate} the
above integral. Recall, for example, the inclusion - exclusion
formula for the number of primes at most $N$. This is an exact
formula, but very hard to evaluate.

\subsection{Useful Number Theory Results}

We will use the following statements freely:

\begin{thm}[Prime Number Theorem] Let $\pi(x)$ denote the number
of primes at most $x$. Then

\be \pi(x) \ = \ \sum_{p \le x} 1 \ = \  \frac{x}{\log x} \ + \
\text{smaller}. \ee

Upon applying Partial Summation, we may rewrite the above as

\be \sum_{p \le x} \log p \ = \ x \ + \ \text{smaller}. \ee

\end{thm}

\begin{thm}[Siegel-Walfisz] Let $C, B > 0$, and let $a$ and $q$ be relatively
prime. Then

\be \sum_{ {p \le x} \atop {p \equiv a (q)} } \log p \ = \
\frac{x}{\phi(q)} + O\Big( \frac{x}{\log^C x} \Big) \ee

for $q \le \log^B x$, and the constant above does not depend on
$x$, $q$ or $a$ (ie, it only depends on $C$ and $B$).
\end{thm}

For completeness, we include a review of partial summation as an
appendix to these notes.


\subsection{Average Sizes of $\Big(f_N(x)\Big)^s$}

Henceforth we will consider $f_N(x)$ arising from the three prime
case. Thus, $s = 3$.

For analytic reasons, it is more convenient to instead analyze the
function

\be F_N(x) \ = \ \sum_{p\le N} \log p \cdot e(px). \ee

Working analogously as before, we are led to

\be R_N(m) \ = \ \int_0^1 \Big( F_N(x) \Big)^3 e(-mx) dx. \ee

By partial summation, it is very easy to go from $R_N(m)$ to
$r_N(m)$.

\begin{exe} Prove the trivial bound for $|F_N(x)|$ is
$N$. Take absolute values and use the Prime Number Theorem.
\end{exe}

We can, however, show that the average square of $F_N(x)$ is
significantly smaller:

\begin{lem} The average value of $|F_N(x)|^2$ is
$N \log N$. \end{lem}

Proof: The following trivial observation will be extremely useful
in our arguments. Let $g(x)$ be a complex-valued function, and let
$\overline{g}(x)$ be its complex conjugate. Then $|g(x)|^2 =
g(x)\overline{g}(x)$.

In our case, as $\overline{F}_N(x) = F_N(-x)$ we have

\bea \int_0^1 | F_N(x) |^2  & \ = \ & \int_0^1 F_N(x) F_N(-x)dx
\nonumber\\ & \ = \ &  \int_0^1 \sum_{p \le N} \log p \cdot e(px)
\sum_{q \le N} \log q \cdot e(-qx) dx \nonumber\\ & \ = \ &
\sum_{p \le N} \sum_{q \le N} \log p \log q \int_0^1 e\Big((p -
q)x\Big)dx \nonumber\\ & \ = \ & \sum_{p \le N} \log^2 p. \eea

Using $\sum_{p \le N} \log p = N \ + \ \text{small}$ and Partial
Summation, we can show

\be \sum_{p \le N} \log^p \ = \ N\log N \ + \ \text{smaller}. \ee

Thus,

\be \int_0^1 |F_N(x) |^2  \ =  \ N\log N \ + \ \text{smaller}. \ee

Thus, taking square-roots, we see on average $|\Big( F_N(x)
\Big)|^2$ is $N \log N$, significantly smaller than the maximum
possible value ($N^2$). Thus, we see we are getting almost
square-root cancellation on average. $\Box$

\subsection{Definition of the Major and Minor Arcs}

We split the unit interval $[0,1)$ into two disjoint parts, the
Major and the Minor arcs.

Roughly, the Major arcs will be a union of very small intervals
centered at rationals with small denominator (relative to $N$).
Near these rationals, we will be able to approximate $F_N(x)$ very
well, and $F_N(x)$ will be of size $N$.

The minor arcs will be the rest of $[0,1)$; we will show that
$F_N(x)$ is significantly smaller than $N$ here.

\subsubsection{Major Arcs}

Let $B > 0$, and let $Q = (\log N)^B \ll N$.

For each $q \in \{1,2,\dots,Q\}$ and $a \in \{1,2,\dots,q\}$ with
$a$ and $q$ relatively prime, consider the set

\be \mathcal{M}_{a,q} \ = \ \Big\{ x \in [0,1): \ \Big|x -
\frac{a}{q} \Big| \ < \ \frac{Q}{N} \Big\}. \ee

We also add in one interval centered at either $0$ or $1$, ie, the
"interval" (or wrapped-around interval)

\be \Bigg[0, \frac{Q}{N}\Bigg] \ \cup \ \Bigg[1 - \frac{Q}{N}, 1
\Bigg]. \ee

\begin{exe} Show, if $N$ is large, that the major arcs
$\mathcal{M}_{a,q}$ are disjoint for $q \le Q$ and $a \le q$, $a$
and $q$ relatively prime. \end{exe}

We define the Major Arcs to be the union of each arc
$\mathcal{M}_{a,q}$:

\be \mathcal{M} \ = \ \bigcup_{q=1}^Q \bigcup_{ {a = 1} \atop
{(a,q) = 1} } \mathcal{M}_{a,q}, \ee

where $(a,q)$ is the greatest common divisor of $a$ and $q$.

\begin{exe} Show $|\mathcal{M}| < \frac{2Q^3}{N}$. As $Q = \log^B
N$, this implies as $N \to \infty$, the major arcs are zero
percent of the unit interval. \end{exe}

\subsubsection{Minor Arcs}

The Minor Arcs, $\mathrm{m}$, are whatever is \emph{not} in the
Major Arcs. Thus,

\be \mathrm{m} \ = \ [0,1) - \mathcal{M}. \ee

Clearly, as $N \to \infty$, almost all of $[0,1)$ is in the Minor
Arcs.


\section{Contributions from the Major and Minor Arcs}


\subsection{Contribution from the Minor Arcs}

We bound the contribution from the minor arcs to $r_N(m)$:

\bea \Big| \int_{\mathrm{m}} F_N^3(x) e(-mx)dx \Big| & \ \le \ &
\int_{\mathrm{m}} |F_N(x)|^3 dx \nonumber\\ & \ \le \ & \Big(
\max_{x \in \mathrm{m} } |F_N(x)| \Big) \int_{\mathrm{m}}
|F_N(x)|^2 dx \nonumber\\ & \ \le \ & \Big( \max_{x \in \mathrm{m}
} |F_N(x)| \Big)  \int_0^1 F_N(x) F_N(-x) dx \nonumber\\ & \ \le \
& \Big( \max_{x \in \mathrm{m} } |F_N(x)| \Big) N \log N. \eea

As the minor arcs are most of the unit interval, replacing
$\int_{\mathrm{m}}$ with $\int_0^1$ doesn't introduce much of an
over-estimation.

\emph{In order for the Circle Method to succeed, we need a
non-trivial, good bound for}

\be \max_{x \in \mathrm{m} } |F_N(x)|  \ee

\emph{This is where most of the difficulty arises, showing that
there is good cancellation in $F_N(x)$ if we stay away from
rationals with small denominator.}

We will show that the contribution to the major arcs is

\be \goth{S}(N) \frac{N^2}{2} \ + \ \text{smaller}, \ee

where $\exists c_1, c_2 > 0$ such that, for all $N$, $c_1 <
\goth{S}(N) < c_2$.

Thus, we need the estimate that

\be \max_{x \in \mathrm{m} } |F_N(x)| \ \le \
\frac{N}{\log^{1+\epsilon} N}. \ee

Relative to the average size of $|F_N(x)|^2$, this is
significantly smaller; however, as we are showing that the maximum
value of $|F_N(x)|$ is bounded, this is a significantly more
delicate question. We know such a bound cannot be true for all $x
\in [0,1)$ (see below, and not that $F_N(0) = N$). The hope is
that if $x$ is not near a rational with small denominator, we will
get moderate cancellation.

While this is very reasonable to expect, it is not easy to prove.

\subsection{Contribution from the Major Arcs}

Fix a $q \le Q$ and an $a \le q$ with $a$ and $q$ relatively
prime. We evaluate $F\Big(\frac{a}{q}\Big)$.

\bea F\Big(\frac{a}{q}\Big) & \ = \ & \sum_{p\le N} \log p \cdot
e^{2\pi i p \frac{a}{q} } \nonumber\\ & \ = \ & \sum_{r=1}^q
\sum_{ {p \equiv r (q)} \atop {p \le N} } \log p \cdot e^{2\pi i
\frac{ap}{q} } \nonumber\\ & \ = \ & \sum_{r=1}^q \sum_{ {p \equiv
r (q)} \atop {p \le N} } \log p \cdot e^{2\pi i \frac{ar}{q} }
\nonumber\\ & \ = \ & \sum_{r=1}^q  e^{2\pi i \frac{ar}{q} }
\sum_{ {p \equiv r (q)} \atop {p \le N} } \log p \eea

Note the beauty of the above. The dependence on $p$ in the
original sums is very weak -- there is a $\log p$ factor, and
there is $e\Big( \frac{ap}{q} \Big)$. In the exponential, we only
need to know $p$ mod $q$. Now, $p$ runs from $2$ to $N$, and $q$
is at most $\log^B N$. Thus, in general $p \gg q$.

We use the Siegel-Walfisz Theorem. We first remark that we may
assume $r$ and $q$ are relatively prime. Why? If $p \equiv r$ mod
$q$, this means $p = \alpha q + r$ for some $\alpha \in \N$. If
$r$ and $q$ have a common factor, there can be at most one prime
$p$ (namely $r$) such that $p \equiv r$ mod $q$, and this can
easily be shown to give a negligible contribution.

For any $C > 0$

\be \sum_{ {p \equiv r (q)} \atop {p \le N} } \log p \ = \
\frac{N}{\phi(q)} \ + \ O\Big( \frac{N}{\log^C N} \Big). \ee

Now, as $\phi(q)$ is at most $q$ which is at most $\log^B N$, we
see that the main term is significantly greater than the error
term (choose $C$ much greater than $B$).

Note the Siegel-Walfisz Theorem would be useless if $q \approx
N^\epsilon$. Then the main term would be like $N^{1 - \epsilon}$,
which would be smaller than the error term.

This is one reason why, in constructing the major arcs, we take
the denominators to be small.

Thus, we find

\bea F\Big(\frac{a}{q}\Big) &\ = \  & \sum_{ {r=1} \atop {(r,q)=1}
}^q e^{2\pi i \frac{ar}{q} } \frac{N}{\phi(q)} \ + \
\text{smaller} \nonumber\\ &\ = \  &  \frac{N}{\phi(q)} \sum_{
{r=1} \atop {(r,q)=1} }^q e^{2\pi i \frac{ar}{q} }. \eea


We merely sketch what happens now.

First, one shows that for $x \in \mathcal{M}_{a,q}$ that $F_N(x)$
is very close to $F\Big(\frac{a}{q}\Big)$. This is a standard
analysis (Taylor Series Expansion -- the constant term is a good
approximation if you are sufficiently close).

Thus, as the major arcs are distinct,

\be \int_{\mathcal{M}} F_N^3(x) e(-mx)dx \ = \ \sum_{q=1}^Q \sum_{
{a=1} \atop {(a,q)=1} } \int_{ \mathcal{M}_{a,q} } F_N^3(x)
e(-mx)dx. \ee

We can approximate $F_N^3(x)$ by $F\Big(\frac{a}{q}\Big)$;
integrating a constant gives the constant times the length of the
interval. Each of the major arcs has length $\frac{2Q^3}{N}$. Thus
we find that, up to a smaller correction term, the contribution
from the Major Arcs is

\bea \int_{\mathcal{M}} F_N^3(x) e(-mx)dx & \ = \ & \frac{2Q^3}{N}
\sum_{q=1}^Q \sum_{ {a=1} \atop {(a,q)=1} } \Bigg(
\frac{N}{\phi(q)} \sum_{ {r=1} \atop {(r,q)=1} }^q e^{2\pi i
\frac{ar}{q} } \Bigg)^3 e\Big( \frac{-2\pi i ma}{q} \Big)
\nonumber\\ & \ = \ & N^2 \cdot 2Q^3 \sum_{q=1}^Q
\frac{1}{\phi(q)^3} \sum_{ {a=1} \atop {(a,q)=1} } \Bigg( \sum_{
{r=1} \atop {(r,q)=1} }^q e^{2\pi i \frac{ar}{q} } \Bigg)^3 e\Big(
\frac{-2\pi i ma}{q} \Big). \nonumber\\ & & \eea

To complete the proof, we need to show that what is multiplying
$N^2$ is non-negative, and not too small.

We will leave this for another day, as it is getting quite late
here.



\section{Why Goldbach is Hard}

Using

\be F_N(x) \ = \ \sum_{p \le N} \log p \cdot e^{2\pi i px}, \ee

we find we must study

\be \int_0^1 F_N^s(x)dx, \ee

where $s = 3$ if we are looking at $p_1 + p_2 + p_3 = 2n+1$ and $s
= 2$ if we are looking at $p_1 + p_2 = 2n$. Why does the circle
method work for $s = 3$ but fail for $s = 2$?

\subsection{$s = 3$ Sketch}

Let us recall \emph{briefly} the $s = 3$ case. Near rationals
$\frac{a}{q}$ with \emph{small} denominator (\emph{small} means $q
\le \log^B N$), we can evaluate $F_N(\frac{a}{q})$. Using Taylor,
if $x$ is very close to $\frac{a}{q}$, we expect $F_N(x)$ to be
close to $F_N(\frac{a}{q})$.

The Major Arcs have size $\frac{\log^B N}{N}$. As $F_N(x)$ is
around $N$ near such rationals, we expect the integral of
$F_N^3(x) e(-mx)$ to be $N^2$ times a power of $\log N$. Doing a
careful analysis of the singular series shows that the
contribution is actually $\goth{S}(N) N^2$, where there exist
constants independent of $N$ such that $0 < c_1 < \goth{S}(N) <
c_2 < \infty$.


A direct calculation shows that

\be\label{eqfnsquaredisN} \int_0^1 |F_N(x)|^2dx \ = \ \int_0^1
F_N(x) F_N(-x)dx \ = \ N. \ee

Thus, if $\mathrm{m}$ denotes the minor arcs,

\bea \Big|\int_{\mathrm{m}} F_N^3(x)e(-mx)dx \Big| & \ \le \ &
\max_{x \in \mathrm{m}} |F_N(x)| \int_0^1 |F_N(x)|^2dx\nonumber\\
& \ \le \ & N \max_{x \in \mathrm{m}} |F_N(x)|. \eea

As the major arcs contribute $\goth{S}(N) N^2$, we need to show

\be \max_{x \in \mathrm{m}} |F_N(x)| \ \ll \ \ \frac{N}{\log^D N}.
\ee

Actually, we just need to show the above is $\ll o(N)$. This is
the main difficulty -- the trivial bound is $|F_N(x)| \le N$. As
$F_N(0) = N$ plus lower order terms, we cannot do better in
general.

\begin{exe} Show $F_N(\foh) = N-1$ plus lower order terms.
\end{exe}

The key observation is that, if we stay away from rationals with
small denominator, we can prove there is cancellation in $F_N(x)$.
While we don't go into details here (see, for example, Nathanson's
Additive Number Theory: The Classical Bases, Chapter $7$), the
savings we obtain is small. We show

\be \max_{x \in \mathrm{m} } |F_N(x)| \ \ll \ \frac{N}{\log^D N}.
\ee

Note that Equation \ref{eqfnsquaredisN} gives us significantly
better cancellation on average, telling us that $|F_N(x)|^2$ is
usually of size $N$.

Thus, it is our dream to be so lucky as to see $\Big| \int_I
|F_N(x)|^2 dx \Big|$ for any $I \subset [0,1)$, as we can evaluate
this extremely well.

\subsection{$s = 2$ Sketch}

What goes wrong when $s = 2$? As a first approximation, if $s = 3$
has the Major Arcs contributing a constant times $N^2$ (and
$F_N(x)$ was of size $N$ on the Major Arcs), one might guess that
the Major Arcs for $s = 2$ will contribute a constant times $N$.

How should we estimate the contribution from the Minor Arcs? We
have $F_N^2(x)$. If we just throw in absolute values we get

\be \Big|\int_{\mathrm{m}} F_N^2(x)e(-mx)dx \Big| \ \le \ \int_0^1
|F_N(x)|^2dx \ = \ N. \ee

Note, unfortunately, that this is the same size as the expected
contribution from the Major Arcs!

We could try pulling a $\max_{x \in \mathrm{m}} |F_N(x)|$ outside
the integral, and hope to get a good savings. The problem is this
leaves us with $\int_{\mathrm{m}} |F_N(x)|dx$.

Recall

\begin{lem}

\be \int_0^1 |f(x)g(x)|dx \ \le \ \Big( \int_0^1 |f(x)|^2dx
\Big)^\foh \cdot \Big( \int_0^1 |g(x)|^2 dx\Big)^\foh. \ee
\end{lem}

For a proof, see Lemma \ref{lemcauchyshwarz}.

Thus,

\bea \Big|\int_{\mathrm{m}} F_N^2(x)e(-mx)dx \Big| &\ \le \ &
\max_{x \in \mathrm{m}} |F_N(x)| \int_0^1 |F_N(x)|dx \nonumber\\
&\ \le \ & \max_{x \in \mathrm{m}} |F_N(x)| \Big( \int_0^1
|F_N(x)|^2 dx \Big)^\foh \cdot \Big( \int_0^1 1^2 dx\Big)^\foh
\nonumber\\ &\ \le \ & \max_{x \in \mathrm{m}} |F_N(x)| \cdot
N^\foh \cdot 1. \eea

As the Major Arcs contribute something of size $N$, we would need

\be \max_{x \in \mathrm{m}} |F_N(x)| \ \ll \ o(\sqrt{N}). \ee

There is almost no chance of such cancellation. We know

\be \int_0^1 |F_N(x)|^2dx \ = \ N \ \text{plus lower order terms}.
\ee

Thus, the average size of $|F_N(x)|$ is $N$, so we expect
$|F_N(x)|$ to be about $\sqrt{N}$. To get $o(N)$ would be
unbelievably good fortune!

While the above sketch shows the Circle Method is not, at present,
powerful enough to handle the Minor Arc contributions, all is not
lost. The quantity we \emph{need} to bound is

\be \Big|\int_{\mathrm{m}} F_N^2(x)e(-mx)dx \Big|. \ee

However, we have instead been studying

\be \int_{\mathrm{m}} |F_N(x)|^2 dx \ee

and

\be  \max_{x \in \mathrm{m}} |F_N(x)| \int_0^1 |F_N(x)| dx. \ee

Thus, we are ignoring the probable oscillation / cancellation in
the integral $\int F_N(x) e(-mx)dx$. It is \emph{this
cancellation} that will lead to the Minor Arcs contributing
significantly less than the Major Arcs.

However, showing there is cancellation in the above integral is
very difficult. It is a lot easier to work with absolute values.


\section{Cauchy-Schwartz Inequality}

\begin{lem}\label{lemcauchyshwarz}[Cauchy-Schwarz]
\be \int_0^1 |f(x)g(x)|dx \ \le \ \Big( \int_0^1 |f(x)|^2dx
\Big)^\foh \cdot \Big( \int_0^1 |g(x)|^2 dx\Big)^\foh. \ee
\end{lem}

For notational simplicity, assume $f$ and $g$ are real-valued,
positive functions. Working with $|f|$ and $|g|$ we see there is
no harm in the above.

Let

\be h(x) = \ f(x) + \lambda g(x), \ \ \lambda \ = \
-\frac{\int_0^1 f(x)g(x)dx}{\int_0^1 g(x)^2 dx} \ee

As $\int_0^1 h(x)^2 dx \ge 0$, we have

\bea 0 & \ \le \ & \int_0^1 \Big( f(x) + \lambda g(x) \Big)^2dx
\nonumber\\ & \ = \ & \int_0^1 f(x)^2 dx \ + \ 2\lambda \int_0^1
f(x) g(x) dx \ + \ \lambda^2 \int_0^1 g(x)^2 dx \nonumber\\ & \ =
\ & \int_0^1 f(x)^2 dx \ - \ 2 \frac{\Big( \int_0^1 f(x)g(x)dx
\Big)^2}{\int_0^1 g(x)^2 dx} \ + \ \frac{\Big( \int_0^1 f(x)g(x)dx
\Big)^2}{\int_0^1 g(x)^2 dx} \nonumber\\ & \ = \ & \int_0^1 f(x)^2
dx \ - \ \frac{\Big( \int_0^1 f(x)g(x)dx \Big)^2}{\int_0^1 g(x)^2
dx} \nonumber\\ \frac{\Big( \int_0^1 f(x)g(x)dx \Big)^2}{\int_0^1
g(x)^2 dx} & \ \le \ & \int_0^1 f(x)^2 dx \nonumber\\ \Big(
\int_0^1 f(x)g(x)dx \Big)^2 & \ \le \ & \int_0^1 f(x)^2 dx  \cdot
\int_0^1 g(x)^2 dx \nonumber\\ \int_0^1 f(x)g(x)dx & \ \le \ &
\Big( \int_0^1 f(x)^2 dx \Big)^\foh \cdot \Big( \int_0^1 g(x)^2 dx
\Big)^\foh. \eea

Again, for general $f$ and $g$, replace $f(x)$ with $|f(x)|$ and
$g(x)$ with $|g(x)|$ above. Note there is nothing special about
$\int_0^1$. $\Box$

The Cauchy-Schwarz Inequality is often useful when $g(x) = 1$. In
this special case, it is important that we integrate over a finite
interval.

\begin{exe} For what $f$ and $g$ is the Cauchy-Schwarz Inequality
an equality? \end{exe}

\section{Partial Summation}

\begin{lem}[Partial Summation: Discrete
Version]\label{lempartialsummationdiscrete} Let $A_N =
\sum_{n=1}^N a_n$. then

\begin{eqnarray}
\sum_{n=M}^N a_n b_n = A_N b_N - A_{M-1} b_M + \sum_{n=M}^{N-1}
A_n (b_n - b_{n+1})
\end{eqnarray}

\end{lem}

\begin{proof}

Since $A_n - A_{n_1} = a_n$,

\bea
\sum_{n=M}^N a_n b_n &= & \sum_{n=M}^N (A_n-A_{n-1}) b_n \nonumber\\
&= & (A_N - A_{N-1}) b_N + (A_{N-1} - A_{N-2}) b_{N-1} +\dotsb +
(A_M - A_{M-1}) b_M \nonumber\\ &= & A_N b_N + (- A_{N-1} b_N +
A_{N-1} b_{N-1})
+ \dotsb + (-A_M b_{M+1} + A_M b_M) - a_{M-1} b_M \nonumber\\
&= & A_N b_N - a_{M_1} b_M + \sum_{n=M}^{N-1} A_n (b_n - b_{n+1}).
\eea

\end{proof}

\begin{lem}[Abel's Summation Formula - Integral
Version]\label{partialsummationintegral} Let $h(x)$ be a
continuously differentiable function. Let $A(x) = \sum_{n \leq x}
a_n$. Then
\begin{eqnarray}
\sum_{n \leq x} a_n h(n) = A(x) h(x) - \int_1^x A(u) h'(u) du
\end{eqnarray}
\end{lem}

See, for example, W. Rudin, \emph{Principles of Mathematical
Analysis}, page $70$.

Partial Summation allows us to take knowledge of one quantity and
convert that to knowledge of another.

For example, suppose we know that

\be \sum_{p\le x} \log p \ = \ x + O(x^{\foh + \epsilon}).
 \ee

We use this to glean information about $\sum_{p \le x} 1$.

Define

\be h(n) = \frac{1}{\log n} \ \ \ \mbox{and} \ \ \ a_n =
\begin{cases} \log n & \text{if $n$ is prime}\\ 0
&\text{otherwise.} \end{cases} \ee

Applying partial summation to $\sum_{p \le x} a_n h(n)$ will give
us knowledge about $\sum_{p \le x} 1$. Note as long as $h(n) =
\frac{1}{\log n}$ for $n$ prime, it doesn't matter how we define
$h(n)$ elsewhere; however, to use the integral version of Partial
Summation, we need $h$ to be a differentiable function.

Thus

\bea \sum_{p \le x} 1 & = & \sum_{p \le x} a_n h(n) \nonumber\\
&=& \Big(x + O(x^{\foh + \epsilon}) \Big) \frac{1}{\log x} -
\int_2^x \Big(u + O(u^{\foh + \epsilon}) \Big) h'(u)du. \eea

The main term ($A(x)h(x)$) equals $\frac{x}{\log x}$ plus a
significantly smaller error.

We now calculate the integral, noting $h'(u) = - \frac{1}{u \log^2
u}$. The error piece in the integral gives a constant multiple of

\be \int_2^x \frac{u^{\foh + \epsilon} }{u \log^2 u}du. \ee

As $\frac{1}{\log^2 u} \le \frac{1}{\log^2 2}$ for $2\le u \le x$,
the integral is bounded by

\be \frac{1}{\log^2 2} \int_2^x u^{-\foh + \epsilon} <
\frac{1}{\log^2 2} \frac{1}{\foh + \epsilon} x^{\foh + \epsilon},
\ee

which is significantly less than $A(x)h(x) = \frac{x}{\log x}$.

We now need to handle the other integral:

\be \int_2^x \frac{u}{u \log^2 u}du \ = \ \int_2^x \frac{1}{\log^2
u} du. \ee

The obvious approximation to try is $\frac{1}{\log^2 u} \le
\frac{1}{\log^2 2}$. Unfortunately, plugging this in bounds the
integral by $\frac{x}{\log^2 2}$. This is larger than the expected
main term, $A(x)h(x)$!

As a rule of thumb, whenever you are trying to bound something,
try the simplest, most trivial bounds first. Only if they fail
should you try to be clever.

Here, we need to be clever, as we are bounding the integral by
something larger than the observed terms.

We split the integral into two pieces:

\be \int_2^x \ = \ \int_2^{\sqrt{x}} \ + \ \int_{\sqrt{x}}^x \ee

For the first piece, we use the trivial bound for $\frac{1}{\log^2
u}$. Note the interval has length $\sqrt{x} - 2 < \sqrt{x}$. Thus,
the first piece contributes at most $\frac{x^\foh}{\log^2 2}$,
significantly less than $A(x)h(x)$.

The reason trivial bounds failed for the entire integral is the
length was too large (of size $x$); there wasn't enough decay in
the function.

The advantage of splitting the integral in two is that in the
second piece, even though most of the length of the original
interval is here (it is of length $x - \sqrt{x} \approx x$), the
function $\frac{1}{\log^2 u}$ is small here. Instead of bounding
it by a constant, we now bound it by substituting in the smallest
value of $u$ on this interval, $\sqrt{x}$. Thus, the contribution
from this integral is at most $\frac{x - \sqrt{x}}{\log^2
\sqrt{x}} < \frac{4x}{\log^2 x}$. Note that this is significantly
less than the main term $A(x)h(x) = \frac{x}{\log x}$.



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\chapter{Multiplicative Functions, Kloosterman, $p$-adic Numbers,
and Review of the Three Problems: Germain Primes, $\lambda_1(G)$
for Random Graphs, Randomness of $x \to \overline{x}$ mod $p$}

Multiplicative Functions, Kloosterman Sums and Bounds, $p$-adic
Numbers. Review of the Three Problems (Germain Primes, Randomness
of $x \to \overline{x}$ mod $p$, Random Graphs). Lecture by Peter
Sarnak; notes by Steven Miller.


\section{Multiplicative Functions, Kloosterman and $p$-adic
Numbers}

\subsection{Multiplicative Functions}

\begin{defi}[Multiplicative Functions] Let $f$ be defined on the
positive integers $\N$. $f$ is multiplicative if

\be f(mn) \ = \ f(m) f(n) \ \ \text{\emph{if $m$ and $n$ are
relatively prime}}. \ee
\end{defi}

This is the same as

\be \sum_{n=1}^\infty \frac{f(n)}{n^s} \ = \ \prod_p \Big(1 +
f(p)p^{-s} + f(p^2)p^{-2s} + \cdots \Big). \ee

We call the above an \tbf{Euler Product}. The standard example is
the \tbf{Riemann Zeta Function},

\be \zeta(s) \ = \ \sum_{n=1}^\infty \frac{1}{n^s} \ = \ \prod_p
\frac{1}{1 - p^{-s}}. \ee

There are many multiplicative functions (some, like Dirichlet
Characters, do not even require $m$ and $n$ to be relatively
prime).

The Kloosterman sums are \emph{not} multiplicative.



\subsection{Kloosterman Sums}

If $c_1$ and $c_2$ are relatively prime,

\be K(a,b,c_1c_2) \ = \ K( \ast, \ast, c_1) \cdot K(\ast, \ast,
c_2), \ee

where the $\ast$s are functions of $a$, $b$, $c_1$ and $c_2$.

Say we show $|K(a,b,p)| \ll p^\nu$, where $\nu$ does not depend on
$a$ or $b$ or $p$.

Then, if $c = \prod_i p_i^{r_i}$, we have

\be K\Big(a,b,\prod_i p_i^{r_i}\Big) \ = \ \prod_i K(\ast, \ast,
p_i^{r_i}). \ee

Thus, we just need to get bounds over prime powers to bound a
general Kloosterman sum.

\begin{thm}[Salie] If $\alpha \ge 2$,

\be K(a,b,p^\alpha) \ \le \ 2p^{\frac{\alpha}{2}}. \ee

\end{thm}

Proof: elementary.

\bea \sum_{\text{$x$ mod $p^2$}} &\ = \ & \sum_{x_1=0}^{p-1}
\sum_{x_2 = 0}^{p-1}, \eea

where $x = x_1 + x_2 p$ and $x_1, x_2 \in \{0, 1, \dots, p-1\}$.

Thus, when we encounter terms like $e\Big(\frac{
\overline{x_1+px_2} }{p^2} \Big)$, we need the inverse of
$\overline{x_1 + px_2}$.

Let $x_1^{-1}$ be the inverse of $x_1$ mod $p$. Then

\bea (x_1 + px_2)^{-1} & \ = \ & x_1^{-1}(1 + x_1^{-1}x_2p)^{-1}.
\eea

Note that

\be (1-pb)^{-1} \ = \ 1 + pb + O(p^2). \ee

Say we want the inverse mod $p^2$ of $(1-bp)$. Try multiplying by
$(1+bp)$. We get

\be (1-pb)(1+pb) \ = \ 1 - b^2 p^2 \ \equiv \ 1 \ \text{mod} \
p^2. \ee

The above arguments is Hensel's Lemma.

\subsection{$p$-adic numbers}

We define the $p$-adic norm of a rational $\alpha = \frac{a}{b}$,
$a$ and $b$ relatively prime, by

\be ||\alpha||_p \ = \ p^{-m}, \ \text{where $\frac{a}{b} = p^m
l$, $(p,l) = 1$}. \ee

Note that numbers that are highly divisible by $p$ are small
$p$-adically.

We have the rationals $\Q$ and the $p$-adic norm $|| \ast ||_p$.
Similar to completing the rationals $\Q$ with the usual norm to
get $\R$, we can complete the rationals with respect to this norm.
The resulting field is called the $p$-adic numbers, $\Q_p$.

$\Q \subset \R$, and $\Q$ is dense in $\R$. Similarly $\Q \subset
\Q_p$ and $\Q$ is dense in $\Q_p$.

Let $x \in \Q_p$. Then

\be x \ = \ \frac{a_{-m}}{p^m} + \frac{a_{-m+1}}{p^{m-1}} + \cdots
+ a_0 + a_1 p + a_2 p^2 + \cdots, \ee

where $0 \le a_i \le p - 1$.

Suppose we have a solution $f(x_0) \equiv 0$ mod $p$. We then try
and find $x_1$ such that $f(x_0 + px_1) \equiv 0$ mod $p^2$.
Hensel noted that all we need to find $x_1$ is some knowledge of
the derivative at the previous stage.




\section{Germain Primes}

$p-1 = 2q$, $p$ and $q$ prime. What are the statistics? How many
are there up to $x$? Do they know about each other? What are their
correlations? What about $p - 3$?

The Circle Method is a way of trying to solve additive equations
(Waring's Problem, Goldbach's Problem $p_1 + p_2 = 2n$,
Vinogradov's Three Primes Theorem $p_1 + p_2 + p_3 = 2n+1$, Twin
Primes $p_2 - p_1 = 2$).

\begin{defi}[Germain Primes] If $p$ is prime and $p - 1 = 2q$ for
some prime $q$, we say $p$ is a Germain Prime. \end{defi}

\begin{defi}[$\pi_G(x)$] Recall $\pi(x)$ is the number of primes
at most $x$. Then $\pi(x) \sim \frac{x}{\log x}$. Let $\pi_G(x)$
be the number of Germain primes at most $x$. If the probability of
getting a prime is $\log x$, then we might expect that

\be \pi_G(x) \ \ = \ \ \sum_{ {p \le x \atop p - 1 = 2q} } 1 \ \
\sim \ \ \text{\emph{Const}} \cdot \frac{x}{\log^2 x}. \ee

\end{defi}

Using the circle method, we will try and estimate the above
constant, and hope the minor arcs \emph{do not contribute to the
main term}.

Major McMahan (from the army, friendly with Hardy and Littlewood)
made tables of primes to the \emph{millions}. He checked, and
Hardy and Littlewood's constant (for twin primes) was correct and
Sylvester (who made a probabilistic argument) was shown to be
slightly off.

See Hardy and Littlewood, Acta Mathematica, v$44$, $1923$,
Partitio numerorum. III: \emph{On the expression of a number as a
sum of primes}.

We will then investigate the nearest neighbor and
$k^{\text{th}}$-nearest neighbor spacings.

Also look at Robert Vaughn, \emph{The Hardy Littlewood Method}.


\section{Randomness of $x \to \overline{x}$}

Given a prime $p$, look at $x \to \overline{x}$. How do we compute
$\overline{x}$?

One can compute $\overline{x}$ by using the Euclidean Algorithm
(very fast, $\log p$ steps). Recall the Euclidean Algorithm gives
$a$ and $b$ such that $ax + bp = 1$. Thus, mod $p$, $ax \equiv 1$,
or $a = \overline{x}$ mod $p$.

We now study the spacings between $\overline{x}$ as $x$ ranges in
some interval mod $p$. If the interval is very small, we don't
expect randomness. What if we take an interval of length
$\sqrt{p}$. Do we see Poissonian Behavior there for a fixed prime?

Now, fix a number $a$. Look at $\frac{ \text{$a$ mod $p$}}{p}$ as
you vary $p$.

\begin{thm}[Duke-Iwaniec] Suppose $p \equiv 1$ mod $4$. There is a square root
of $-1$ mod $p$; ie, $\exists x$ such that $x^2 \equiv 1$ mod $p$.
Now we can take $1 \le x \le \frac{p-1}{2}$, so $\frac{x}{p} \in
\Big[0, \foh \Big]$. Then $\frac{x}{p}$, as we vary $p$, is
equi-distributed.
\end{thm}

One can also look at the longest increasing sub-sequence.

Knuth, volume $2$ of the Art of Computer Programming. Look at the
stuff on generating random numbers.




\section{Random Graphs / Ramanujan Graphs}


Bollobas, \emph{Random Graphs}: he will have a model of the random
$3$-regular graphs (what it means, how to generate, how many are
there). \emph{Very} hard if you don't distinguish between
isomorphic graphs (which have the same spectrum).

Look at the distribution of the second largest eigenvalue
$\lambda_1$ of the random $3$-regular graph. Find the mean and the
variance, graph.

Professor Sarnak will give a lecture on the Tracy-Widom
distribution (which is the distribution of the biggest eigenvalue
in some random ensemble -- will we see the same distribution
here)?

What is the scale for normalizing?

Take an interval, see how many eigenvalues in it, slide the
interval down, and see how the number varies.



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\chapter{Random Graphs, Autocorrelations, Random Matrix Theory
and the Mehta-Gaudin Theorem}

Random Graphs (especially graphs with large girth and chromatic
number), Autocorrelation, Random Matrix Theory (Vandermonde
determinants, orthonormal polynomials) and the Mehta-Gaudin
Theorem (large $N$ limits of quantities related to the joint
density function of the eigenvalues). Lecture by Peter Sarnak;
notes by Steven Miller.

\section{Random Graphs}

\begin{defi}[Chromatic Number] The chromatic number is the
least number of colors such that each vertex has a different color
than all of its neighbors.
\end{defi}

\begin{exa} A bi-partite graph is $2$-colorable, as is a tree
(alternate colors as  you go through the generations).
\end{exa}

What can force you to have a lot of colors? If a vertex is joined
to $n_v$ vertices, you need a lot of colors \emph{if} the vertices
it is joined to are joined to each other.

\begin{defi}[Girth] The girth is the shortest closed cycle.
\end{defi}

If the girth is large, make a vertex blue, yellow next level, blue
on next level, et cetera until you come back on yourself.

\begin{que} Can you make a graph with large girth and large
chromatic number?
\end{que}

The two fight each other. Erdos solved this problem by showing
that if you take a \emph{Random Graph} (with suitable properties),
then that graph will have large girth and large chromatic number
with high probability.

Take a Random Graph with $n$ vertices and basically $n^{1 +
\epsilon}$ edges placed at random among the $\ncr{n}{2} = O(n^2)$
possible edges.

The random graph has short cycles, but the number of short cycles
is small. Erdos removes certain graphs with small girth, and shows
with high probability the graphs left have large girth and large
chromatic number.

See Mckay's paper: he proves Kesten's measure holds for the random
graph as the number of vertices goes to infinity.


\section{Baire Category}

Given $\alpha \not\in \Q$ and $C_\alpha$, how often can we find
$\frac{a}{q} \in \Q$ such that

\be \Big| \alpha - \frac{a}{q} \Big| \ \ge \ \frac{C_\alpha}{q^{2
+ \epsilon} }. \ee

In Lebesgue Measure, almost all $\alpha$ satisfy the above
infinitely often.

In the Baire Category, this inequality does not hold infinitely
often.



\section{Autocorrelation}

Note: Alex Barnett lectured on this section.

$x$-axis is number of swaps $n$, $y$-axis is number of graphs with
given $\lambda_1$, $\lambda_1(n)$. Say takes $100$ swaps to
randomize. Then the $y$-value at $101$ swaps should be independent
of the $y$-value at $1$ swap.

But we don't know the number of swaps before we have moved far
enough.

Let $\lambda_1'(n) = \lambda_1(n) - \overline{\lambda}_1$, where
$\overline{\lambda}_1$ is the average value.

Autocorrelation: Say the $x$-axis runs to $m$.

\be A(c) \ = \ \frac{1}{m}\sum_n \lambda_1'(n) \lambda_1'(n+c).
\ee

The above is a function of $c$, symmetric in $c$. As $c$ gets
large, $A(c)$ dies to zero, and has largest value at $c = 0$.

Look for $c$ such that say $10\%$ of the area is from $c$ onward.



\section{Gaudin's Method}

\subsection{Introduction}

From Random Matrix Theory: we have a probability distribution on
$\R^n$:

\be p_\beta(x_1,\dots,x_N) \ = \ c_N(\beta) \prod_{j<k} |x_j -
x_k|^\beta e^{-\sum_{j=1}^N x_j^2} dx_1\cdots dx_N. \ee

Start off with a real $N\times N$ matrix, diagonalize with
eigenvalues $x_1, \dots, x_N$. If you choose the matrix at random,
you get $N$ numbers, and you have a probability distribution on
the eigenvalues.

We've derived the probability above for $N \times N$ matrices. For
convenience, we order the eigenvalues.

If $\beta = 1$ we call the ensemble GOE (Gaussian Orthogonal
Ensembles); if $\beta = 2$ we have GUE (Unitary) and if $\beta =
4$ we have GSE (Symplectic).

What is the correlation between two eigenvalues? What is the
probability of observing a given spacing between two eigenvalues?
We've done this in the $2 \times 2$ case.

In the $N \times N$ case, we would need to integrate out most of
the eigenvalues. The difficulty is $\prod |x_j - x_k|^\beta$.

For $\beta = 1, 2$ or $4$, we can evaluate these integrals; we are
fortunate that these values are the ones that arise in practice.

In fact, even just determining $c_N(\beta)$ is difficult. This is
called the \emph{Selberg Integral}, which A. Selberg solved in
high school!

We will only consider $\beta = 2$, and will be interested in the
limit as $N \to \infty$ (under appropriate re-scaling).

\be R_N(x_1,\dots,x_N) \ = \ \int_\R \cdots \int_\R
p_2(x_1,x_2,\dots,x_n,x_{n+1},\dots,x_N)dx_{n+1} \cdots dx_N. \ee

This will be a symmetric function of the first $n$ variables. If
we integrate all but $1$ variable we get the density of
eigenvalues; if we integrate all but two we get information on
pairs of eigenvalues.

\begin{rek} $\beta = 0$ is Poissonian. \end{rek}


\subsection{Vandermonde Determinants}

Notation: $dp$ means

\bea dp(\theta_1,\dots,\theta_N) &=& c_N \prod_{j<k} \Big|e^{i
\theta_j} - e^{i \theta_k}\Big|^2 d\theta_1 \cdots d\theta_N. \eea

We are now working on the $N$-torus $[0,2\pi] \times \cdots \times
[0,2\pi]$. This goes under the name CUE (Circular Unitary
Ensemble).

Remember the group

\be U(N) = \{\text{$N \times N$ matrices $A$ with $A A^{\star} =
I$}\}. \ee

Similar to the diagonalization of symmetric matrices, for any
unitary matrix $U$ there is a unitary matrix $V$ such that $V^{-1}
U V$ is diagonal; further, the eigenvalues have absolute value
$1$, and hence can be written as $e^{i \theta}$.

Suppose we have $f_1, \dots, f_N$. We form the Vandermonde of the
$N$-variables

\be \text{Van}(f_1,\dots,f_N) \ = \ \prod_{i < j} (f_i - f_j). \ee

Today we will only use the square, so we don't worry about
ordering so that $f_i < f_j$.

\begin{exe}

\be \text{\emph{Van}}(f_1,\dots,f_N) \ = \ \det \Big( f_i^{j-1}
\Big)_{1 \le  i,j \le N}. \ee
\end{exe}

Thus, we have

\be \left|\begin{array}{cccc}
                        1 & 1 & \cdots & 1 \\
                        f_1 & f_2 & \cdots  & f_N \\
                        \vdots & \vdots &  \ddots & \vdots  \\
                        f_1^{N-1} & f_2^{N-1}  & \cdots  &
                        f_N^{N-1}
                          \end{array}\right|
\ee

\subsection{Orthonormal Polynomials}

On the unit circle $T$, we have the measure

\be  d\mu(t) \ = \ \frac{dt}{2\pi}. \ee

Let $f(t)$ be a function such that

\be \int_T f(t) d\mu(t) \ = \ 0, \ \ \ \int_T |f(t)|^2 d\mu(t) =
1. \ee

Define a sequence of monic polynomials $P_n(x)$ for $n \in \N$ and
$\phi_n(t)$ with

\be \phi_n(t) \ = \ P_n( f(t) ), \ \ \ \phi_0(t) \ = \
\frac{1}{\sqrt{\mu(T)}}, \ \ \ \int_T \phi_i(t)
\overline{\phi}_j(t) d\mu(t) = \delta_{ij}. \ee

This is Gramm-Schmidt, where the inner product between two
functions $f$ and $g$ is given by

\be \langle f, g \rangle \ = \ \int_T f(t) \overline{g}(t)
d\mu(t), \ee

and the `Kronecker delta' symbol (the discrete analog of the
continuous delta `function' $\delta(\cdot)$) is defined by

\be \twocase{ \delta_{ij} = }{1}{if $i = j$}{0}{otherwise} \ee

We introduce orthogonal polynomials to handle the integral. The
above process (constructing the $P_n$s and the $\phi_n$s) gives an
orthonormal sequence of polynomials.

\subsection{Kernel $K_N(x,y)$}

Define the kernel

\be K_N(x,y) \ = \ \sum_{j=0}^{N-1} \phi_j(x)
\overline{\phi}_j(y). \ee

\begin{exe} Prove the following:
\ben
\item $\int_T K_N(x,x)d\mu(x) \ = \ N$.
\item $\int_T K_N(x,y) K_N(y,z) d\mu(y) \ = \ K_N(x,z)$.
\een
\end{exe}


\begin{rek}
\be \int_T K_N(x,y) g(y) d\mu(y) \ = \ \sum_{j=0}^{N-1} \Big[
\int_T \overline{\phi}_j(y) g(y) d\mu(y) \Big] \phi_j(x). \ee
Thus, integrating $g$ against $K_N$ projects $g$ onto the first
$N$ vectors.
\end{rek}

Define, for $1 \le n \le N$,

\be D_{n,N}(t_1,\dots,t_n) \ = \ \det \Big( K_N(t_j,t_k) \Big)_{1
\le j,k \le n}. \ee

For example,

\be D_{1,N} \ = \ K_N(t_1,t_1) \ee

and

\be D_{2,N} \ = \ \left|\begin{array}{cc}
                         K_N(t_1,t_1) & K_N(t_1,t_2)   \\
                         K_N(t_2,t_1) & K_N(t_2,t_2)
                          \end{array}\right|
\ee


\subsection{Gaudin-Mehta Theorem}

\begin{thm}[Gaudin-Mehta] We have

\ben

\item

\be \frac{1}{\mu(T)}
\text{\emph{Van}}\Big(f(t_1),\dots,f(t_N)\Big) \ = \
\text{\emph{det}}_{N \times N} \Big( \phi_{i-1}(t_j) \Big)_{1 \le
i,j \le N}. \ee


\item

\be \frac{1}{\mu(T)}
\Big|\text{\emph{Van}}\Big(f(t_1),\dots,f(t_N)\Big)\Big|^2 \ = \
D_{N,N}(t_1,\dots,t_N). \ee


\item For $1 \le n \le N$,

\be \int_T D_{n,N}(t_1,\dots,t_n)d\mu(t_n) \ = \
(N+1-n)D_{n-1,N}(t_1,\dots,t_{n-1}). \ee


\een

\end{thm}

The third statement is the beef, allowing us to integrate out one
variable at a time by induction.

Remember

\be D_{n,N}(t_1,\dots,t_n) \ = \ \text{det}_{n \times n} \Big(
K_N(t_j,t_k) \Big)_{1 \le j,k \le n}. \ee


\begin{cor} Let $F$ be a symmetric function of $t_1, \dots, t_n$,
and define

\bea F_{n,N}(t_1,\dots,t_N) & \  = \ & \ \ \sum_{1 \le i_1 < i_2
\cdots < i_n < N} \ \ F(t_{i_1},\dots, t_{i_n}) \nonumber\\
d\mu_{n,N}(t_1,\dots,t_N) & \ = \ & \frac{1}{n!}
D_{n,N}(t_1,\dots,t_N)d\mu(t_1) \cdots d\mu(t_N). \eea

Then

\be \int_{T^N} F_{N,N}(t_1,\dots,t_N) d\mu_{n,N}(t_1,\dots,t_N) \
= \ \int_{T^n} F(t_1,\dots,t_n) d\mu_{n,N}(t_1,\dots,t_n). \ee

\end{cor}

How might we use the above? For example, consider for $1 \le j, k
\le N$, and consider for $f$ even

\be \sum_{1 \le j < k \le N} f(x_j - x_k). \ee

What is the expectation of the above? In this case, $F$ is a
function of two variables, and $F(x_1,x_2) = f(|x_1 - x_2|)$ and
we now just need to integrate $f(|x_1 - x_2|)$ against the
determinant of a $2 \times 2$ matrix, and this is the only place
where $N$ will arise.

Suppose we had

\be dp(x_1,\dots,x_N) \ = \ e^{- \sum x_j^2} \prod_{j<k} |x_j -
x_k|^2 dx_1 \cdots dx_N. \ee

Consider the expectation of

\be \sum_{1 \le j < k \le N} f(|x_j-x_k|). \ee

According to the corollary, the answer is just

\be \int_\R \int_\R f(|x_1-x_2|) \frac{1}{2!}
\dettwo{K_N(x_1,x_1)}{K_N(x_1,x_2)}{K_N(x_2,x_1)}{K_N(x_2,x_2)}
e^{-x_1^2 - x_2^2} dx_1dx_2. \ee

This is \emph{enormous} progress -- we started with $N$ variables;
we now have $2$ variables. We need to take the $N \to \infty$
limit of the determinant, a much easier question.


\subsection{Example}

$T = [0,2\pi]$, $d\mu(x) = \frac{dx}{2\pi}$, $f(x) = e^{ix}$,
$f^n(x) = e^{inx}$, and $P_n(x) = x^n$. These $P_n$s are monic,
$\phi_n(x) = P_n(f(x))$ is orthonormal, which gives $\phi_n(x) =
e^{inx}$, and clearly

\be \int_0^{2\pi} e^{inx} e^{-imx} dx \ = \ \delta_{ij}. \ee

Finally, we obtain a geometric progression

\bea K_N(x,y) & \ = \ & \sum_{n=0}^{N-1} e^{in(x-y)} \nonumber\\ &
\ = \ & \frac{1 - e^{iN(x-y)}}{1 - e^{i(x-y)}}. \eea

We will symmetrize (and go from $-N$ to $N$), and when we take the
$2 \times 2$ determinant, we get something like

\be \frac{ \sin\Big( \frac{N(x-y)}{2} \Big) }{ \sin \Big(
\frac{x-y}{2} \Big) }. \ee

The most famous pair correlation: we have $N$ eigenvalues so that
the mean spacing is $1$. The \emph{pair correlation} is

\be 1 - \Bigg[\frac{\sin\Big( \pi (x-y) \Big) }{ \pi (x-y) }
\Bigg]^2. \ee


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\chapter{Increasing Length Subsequences and Tracy-Widom}

More on increasing length subsequence and the Tracy-Widom
distribution. Lecture by Peter Sarnak, notes by Steven Miller.


\section{Increasing Length Subsequences}

Consider the set of permutations $S_n$ on $n$ numbers. Let $\sigma
\in S_n$ be a random permutation, and let $L_\sigma$ be the length
of the longest increasing sub-sequence. What is the expected value
of $L_\sigma$?

\begin{conj}[Ulam]
\be E[L_\sigma] \ \sim \ 2 \sqrt{n}. \ee
\end{conj}

Proved by several people (Schepp, Vircheck (?), ...).

At Bell Labs, many people (including Odlyzko) investigated.
Monte-Carlo simulations for variance (beginning $1993$). Dividing
variance by $n^{\frac{1}{3}}$ was good. Looking at the expectation
of

\be \frac{L_\sigma - 2 \sqrt{n}}{n^{\frac{1}{6}}} \ee

and investigated whether or not it went to a limit. Noticed this
distribution is negative (shifted to the left). Prefers to be
\emph{less} than $2\sqrt{n}$.

\section{Tracy-Widom}

On $\R^n$, we have

\be P_N(x) dx \ = \ e^{-\sum_{j=1}^N x_j^2} \prod_{j<k} \left| x_j
- x_i \right|^\beta dx_1 \cdots dx_n, \ \ \beta \in \{1,2,4\}. \ee


Will use the weight $e^{-x^2} dx$, which will give rise to Hermite
polynomials.

\begin{defi}[$F_{N,\beta}(s)$] $F_{N,\beta}(s)$ is the probability that there is
no $x \in [s,\infty)$.
\end{defi}

We can write this as a determinant (see the papers by Mehta and
Mehta-Gaudin). We will have again

\be K_N(x,y) \ = \ \sum_{0 \le j \le N-1} \phi_j(x)
\overline{\phi}_j(y). \ee

Remember the semi-circle rule, that (with some normalization) the
eigenvalues lie in $[-2\sqrt{N},2\sqrt{N}]$.

What is the expected value of the largest eigenvalue? We know most
are in  $[-2\sqrt{N},2\sqrt{N}]$. We haven't discussed whether or
not there are outliers. With probability one, we can show that
there will be no such outliers.

We have $N$ eigenvalues. Near $0$ is called \emph{the bulk}. As
there are $N$ numbers, we expect eigenvalues in an interval of
size $\frac{1}{N}$ near the origin.

What about eigenvalues near the edges, $\pm 2 \sqrt{N}$? Let $s
\in (-\infty,\infty)$. Look at

\be F_N\Big( 2 \sqrt{N} + \frac{s}{N^{\frac{1}{6}}}\Big). \ee

This is the scaling limit. Say $s = 0$. this would give us what is
happening at the origin.

\begin{thm}[Tracy-Widom]

\be \lim_{N\to \infty} F_N\Big( 2 \sqrt{N} +
\frac{s}{N^{\frac{1}{6}}}\Big) \ = \ F_\beta(s), \ \ F_\beta(s) =
\frac{\text{d}F_\beta(s)}{\text{d}s}. \ee

\end{thm}

Here the $N^{\frac{1}{6}}$ arises from the particular problem
we're interested in. Here, we are looking at eigenvalues from
random matrices.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\chapter{Circle Method and Germain Primes}

Using the Hardy-Littlewood Circle Method (and assuming no main
term contribution from the Minor Arcs), we calculate the expected
number of Germain primes. Calculations and notes by Steven Miller.



\section{Preliminaries}

\subsection{Definitions}

Let

\bea e(x) & \ = \ & e^{2\pi i x}  \eea

and

\be \twocase{\gl(n) \ = \ }{\log p}{if $n = p$ is
prime}{0}{otherwise} \ee

Finally, define

\be c_q(a) \ = \ \sum_{r = 1 \atop (r,q)=1}^q e\Big( r\frac{a}{q}
\Big). \ee

\subsection{Partial Summation}

\begin{lem}[Partial Summation: Discrete
Version] Let $A_N = \sum_{n=1}^N a_n$. then

\begin{eqnarray}
\sum_{n=M}^N a_n b_n = A_N b_N - A_{M-1} b_M + \sum_{n=M}^{N-1}
A_n (b_n - b_{n+1})
\end{eqnarray}

\end{lem}

\begin{proof}

Since $A_n - A_{n-1} = a_n$,

\bea
\sum_{n=M}^N a_n b_n &= & \sum_{n=M}^N (A_n-A_{n-1}) b_n \nonumber\\
&= & (A_N - A_{N-1}) b_N + (A_{N-1} - A_{N-2}) b_{N-1} +\dotsb +
(A_M - A_{M-1}) b_M \nonumber\\ &= & A_N b_N + (- A_{N-1} b_N +
A_{N-1} b_{N-1})
+ \dotsb + (-A_M b_{M+1} + A_M b_M) - a_{M-1} b_M \nonumber\\
&= & A_N b_N - a_{M_1} b_M + \sum_{n=M}^{N-1} A_n (b_n - b_{n+1}).
\eea

\end{proof}

\begin{lem}[Abel's Summation Formula - Integral
Version] Let $h(x)$ be a continuously differentiable function. Let
$A(x) = \sum_{n \leq x} a_n$. Then
\begin{eqnarray}
\sum_{n \leq x} a_n h(n) = A(x) h(x) - \int_1^x A(u) h'(u) du
\end{eqnarray}
\end{lem}

See, for example, W. Rudin, \emph{Principles of Mathematical
Analysis}, page $70$.



\subsection{Siegel-Walfisz}

\begin{thm}\label{thmsiegelw}[Siegel-Walfisz] Let $C, B > 0$,
and let $a$ and $q$ be relatively prime. Then

\be \sum_{ {p \le x} \atop {p \equiv a (q)} } \log p \ = \
\frac{x}{\phi(q)} + O\Big( \frac{x}{\log^C x} \Big) \ee

for $q \le \log^B x$, and the constant above does not depend on
$x$, $q$ or $a$ (ie, it only depends on $C$ and $B$).
\end{thm}


\subsection{Germain Integral}

Define

\bea f_{1N}(x) & \ = \ & \sum_{p_1 \le N} \log p_1 \cdot e(p_1x) \nonumber\\
f_{2N}(x) & \ = \ & \sum_{p_2 \le N} \log p_2 \cdot e(-2p_2 x)
\nonumber\\ f_N(x) & \ = \ & \sum_{p_1 \le N} \sum_{p_2 \le N}
\log p_1 \log p_2 \cdot e\Big( (p_1 - 2p_2)x \Big). \eea


Consider

\be \int_{-\foh}^\foh f_N(x) e(-x)dx \ = \ \sum_{p_1 \le N}
\sum_{p_2 \le N} \log p_1 \log p_2 \int_{-\foh}^\foh e\Big( (p_1 -
2p_2 - 1)x \Big)dx. \ee


Note

\be \twocase{\int_{-\foh}^\foh e\Big( (p_1 - 2p_2 - 1)x \Big)dx \
= \ }{1}{if $p_1 - 2p_2 - 1 = 0$}{0}{if $p_1 - 2p_2 - 1 \neq 0$}
\ee

Thus, we get a contribution of $\log p_1 \log p_2$ if $p_1$ and $
p_2 = \frac{p_1 - 1}{2}$ are both primes. Thus,

\be \int_{-\foh}^\foh f_N(x) e(-x)dx \ = \ \sum_{p_1 \le N \atop
p_2 = \frac{p_1-1}{2} \ \text{prime} } \log p_1 \log p_2. \ee

The above is a weighted counting of Germain primes.



\subsection{Major and Minor Arcs}

Let $B$ be a positive integer, $Q = \log^B N$, and define the
Major Arc $\mathcal{M}_{a,q}$

\be \mathcal{M}_{a,q} \ = \ \Big\{ x \in [0,1): \ \Big|x -
\frac{a}{q} \Big| \ < \ \frac{Q}{N} \Big\}. \ee

We also add in one interval centered at either $0$ or $1$, ie, the
"interval" (or wrapped-around interval)

\be \Bigg[0, \frac{Q}{N}\Bigg] \ \cup \ \Bigg[1 - \frac{Q}{N}, 1
\Bigg]. \ee

For convenience, we often use the interval $[-\foh,\foh]$ instead
of $[0,1]$, in which case we would have

\be \Bigg[ -\foh, -\foh + \frac{Q}{N} \Bigg] \ \bigcup \ \Bigg[
\foh - \frac{Q}{N}, \foh \Bigg]. \ee

For functions that are periodic of period one, we could instead
consider

\be \Bigg[ \foh - \frac{Q}{N}, \foh + \frac{Q}{N} \Bigg]. \ee

The Major Arcs are defined by

\be \mathcal{M} \ = \ \bigcup_{q \le Q} \bigcup_{a = 1 \atop (a,q)
= 1}^{q} \mathcal{M}_{a,q}. \ee

The Minor Arcs, $\mathrm{m}$, are whatever is \emph{not} in the
Major Arcs.

Then

\be \int_{-\foh}^{\foh}f_N(x)e(-x)dx \ = \
\int_{\mathcal{M}}f_N(x)e(-x)dx \ + \ \int_{\mathrm{m}}
f_N(x)e(-x)dx. \ee

We will assume that there is no net contribution over the minor
arcs. Thus, in the sequel we investigate

\be \int_{\mathcal{M}} f_N(x)e(-x)dx. \ee


\subsection{Reformulation of Germain Integral}

\bea f_{1N}(x) & \ = \ & \sum_{m_1 \le N} \gl(m_1) \cdot e(m_1x) \nonumber\\
f_{2N}(x) & \ = \ & \sum_{m_2 \le N} \gl(m_2) \cdot e(-2m_2 x)
\nonumber\\ f_N(x) & \ = \ & \sum_{m_1 \le N} \sum_{m_2 \le N}
\gl(m_1) \gl(m_2) \cdot e\Big( (m_1 - 2m_2)x \Big). \eea

We investigate

\be \int_{\mathcal{M}} f_N(x) e(-x)dx. \ee

We will show the Major Arcs contribute, up to lower order terms,
$T_2 N$, where $T_2$ is a constant independent of $N$. The length
of the Major Arc $\mathcal{M}_{a,q}$ is $\frac{Q}{N}$. We sum over
$(a,q) = 1$ and $q \le Q$. Thus, the total length is bounded by

\be \sum_{q\le Q} q \cdot \frac{Q}{N} \ \ll \ \frac{Q^3}{N} \ \ll
\ \frac{\log^B}{N}. \ee

By choosing $B$ sufficiently large, we will be able to make all
the errors from the Major Arc calculations less than the main term
from the Major Arcs. Of course, we have absolutely no control over
what happens on the minor arcs, and we will simply assume there is
no contribution from the minor arcs.

Thus, on the Major Arc $\mathcal{M}_{a,q}$, success will be in
finding a function of size $N^2$ such that the error from this
function to $f_N(x)$ on $\mathcal{M}_{a,q}$ is much smaller than
$N^2$, say $N^2$ divided by a large power of $\log N$.

Similarly, when we integrate over the Major Arcs, we will find the
main terms will be of size $N$; again, success will be in showing
the errors in the approximations are much smaller than $N$, say
$N$ divided by a large power of $\log N$.

We are able to do this because of the Siegel-Walfisz Theorem
(Theorem \ref{thmsiegelw}). Given \emph{any} $B > 0$, we can find
a $C > 0$ such that, if $q \le \log^B N$, then

\be \sum_{p \le N \atop p \equiv r (q) } \log p \ = \
\frac{N}{\phi(q)} + O\Big( \frac{N}{\log^C N} \Big), \ee

$(r,q) = 1$. Thus, we can take $C$ enormous, large enough so that
even when we multiply by the length of the Major Arcs (of size
$\frac{\log^{3B} N}{N}$, we still have something small.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{$f_N(x)$ and $u(x)$}

\subsection{$f\Big( \frac{a}{q} \Big)$}

We now calculate $f_N\Big( \frac{a}{q} \Big)$ for $q \le \log^B
N$.

Up to lower order terms,

\bea f_N\Big( \frac{a}{q} \Big) & \ = \ & \sum_{p_1 \le N} \log
p_1 \cdot e\Big(p_1\frac{a}{q}\Big) \sum_{p_2 \le N} \log p_2
\cdot e\Big(-2p_2\frac{a}{q}\Big) \nonumber\\ & \ = \ & \sum_{r_1
= 1}^q \sum_{p_1 \le N \atop p_1 \equiv r_1 (q)} \log p_1 \cdot
e\Big(p_1\frac{a}{q}\Big) \sum_{r_2 = 1}^q \sum_{p_2 \le N \atop
p_2 \equiv r_1 (q)} \log p_2 \cdot e\Big(-2p_2\frac{a}{q}\Big)
\nonumber\\ & \ = \ & \sum_{r_1 = 1}^q e\Big( r_1\frac{a}{q} \Big)
\sum_{r_2 = 1}^q e\Big( r_2\frac{-2a}{q} \Big) \sum_{p_1 \le N
\atop p_1 \equiv r_1 (q)} \log p_1 \sum_{p_2 \le N \atop p_2
\equiv r_2 (q)} \log p_2 \nonumber\\ & \ = \ &
\frac{N^2}{\phi^2(q)} \sum_{r_1 = 1 \atop (r_1,q)=1}^q e\Big(
r_1\frac{a}{q} \Big) \sum_{r_2 = 1 \atop (r_2,q)=1}^q e\Big(
r_2\frac{-2a}{q} \Big)\nonumber\\ & \ = \ & \frac{N^2}{\phi^2(q)}
c_q(a)c_q(-2a), \eea

where the second to last line follows from the Siegel-Walfisz
Theorem (Theorem \ref{thmsiegelw}). We restrict to $(r_i,q) = 1$
because if $(r_i,q) > 1$, there is at most one prime $p_i \equiv
r_i$ mod $q$.



\subsection{$u(x)$}

Let

\be u(x) \ = \ \sum_{m_1 \le N} \sum_{m_2 \le N} e\Big( (m_1 -
2m_2)x \Big). \ee

We will often look at

\be \frac{c_q(a) c_q(-2a)}{\phi^2(q)} u(x). \ee

Note

\be u(0) \ = \ N^2. \ee



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{$f_N(\alpha) - \frac{c_q(a) c_q(-2a)}{\phi^2(q)}
u(\alpha - \frac{a}{q})$, $\alpha \in \mathcal{M}_{a,q}$}

Let

\be C_q(a) \ = \ \frac{c_q(a) c_q(-2a)}{\phi^2(q)}. \ee

We write $\alpha$ as $\beta + \frac{a}{q}$, $\beta \in \Big[
-\frac{Q}{N},\frac{Q}{N} \Big]$, $Q = \log^B N$. As always, we
ignore lower order terms.

Note $f_N(x)$ is approximately $C_q(a) N^2$ for $x$ near
$\frac{a}{q}$. We now expand and show $f_N(\alpha)$ is $C_q(a)
u\Big( \alpha - \frac{a}{q} \Big)$ plus errors of size
$\frac{N^2}{\log^{C - 2B} N}$ for $\alpha \in \mathcal{M}_{a,q}$.

\subsection{Setup}

\bea S_{a,q}(\alpha) & = & f_N(\alpha) - C_q(a) u\Big(\alpha -
\frac{a}{q}\Big) \nonumber\\ &=& \sum_{m_1,m_2 \le N} \gl(m_1)
\gl(m_2) e\Big( (m_1 - 2m_2) \alpha \Big) \ - \ C_q(a)
\sum_{m_1,m_2 \le N} e\Big( (m_1 - 2m_2)\beta\Big) \nonumber\\ &=&
\sum_{m_1,m_2 \le N} \Bigg[ \gl(m_1)\gl(m_2)e\Big(
(m_1-2m_2)\frac{a}{q}\Big) - C_q(a) \Bigg] e\Big( (m_1 -
2m_2)\beta\Big) \nonumber\\ & = &  \sum_{m_1 \le N} \Bigg[
\sum_{m_2 \le N} \Big[ \gl(m_1)\gl(m_2)e\Big( (m_1 -
2m_2)\frac{a}{q}\Big) - C_q(a) \Big] e(-2m_2 \beta) \Bigg]
e(m_1\beta) \nonumber\\ & & \eea

We now apply Partial Summation multiple times. First, we apply
Partial Summation to the $m_2$-sum:

\bea S_{2;a,q} & = & \sum_{m_2 \le N} \Big[ \gl(m_1)\gl(m_2)e\Big(
(m_1 - 2m_2)\frac{a}{q}\Big)  - C_q(a) \Big] e(-2m_2 \beta)
\nonumber\\ & = & \sum_{m_2 \le N} a_{m_2} b_{m_2} \nonumber\\ &=&
A_2(N) e(-2N\beta) + 4\pi i \beta \int_0^N \sum_{m_2 \le u}
a_{m_2} e(-u \beta) du.  \eea

We hit the above with $e(m_1 \beta)$, and sum from $m_1 = 1$ to
$N$. We get two pieces:

\bea S_{1\sum;a,q} & \ = \ & \sum_{m_1 \le N} A_2(N) e(-2N \beta)
\cdot e(m_1 \beta) \nonumber\\ S_{1\int;a,q} & \ = \ & \sum_{m_1
\le N} 4 \pi i \beta \int_0^N \sum_{m_2 \le u} a_{m_2} e(-u\beta)
du \cdot e(m_1 \beta) \nonumber\\ S_{a,q} & \ = \ & S_{1\sum;a,q}
+ S_{1\int;a,q}. \eea


\subsection{$S_{1\sum;a,q}$}

\bea S_{1\sum;a,q} & = & \sum_{m_1 \le N} A_2(N) e(-2N \beta)
\cdot e(m_1 \beta) \nonumber\\ &=& e(-2N \beta) \sum_{m_1 \le N}
A_2(N) e(m_1 \beta) \nonumber\\ &=& e(-2N \beta) \sum_{m_1 \le N}
\sum_{m_2 \le N} \Big[ \gl(m_1)\gl(m_2)e\Big( (m_1 -
2m_2)\frac{a}{q}\Big)  - C_q(a) \Big] e(m_1 \beta) \nonumber\\ &=&
e(-2N\beta) \Bigg[A_1(N) e(N\beta) \nonumber\\ & & \ - 2\pi i
\beta \int_0^N \sum_{m_1 \le t} \sum_{m_2 \le N} \Big[
\gl(m_1)\gl(m_2)e\Big( (m_1 - 2m_2)\frac{a}{q}\Big)  - C_q(a)
\Big] e(t \beta)dt. \nonumber\\ & & \eea


\subsubsection{First Piece}

The first piece, the $A_1(N) e(N\beta)$ term, is small for $q \le
Q$. Why? We have (up to lower order terms)

\bea A_1(N) e(N\beta) &=& \sum_{m_1, m_2 \le N} \gl(m_1)\gl(m_2)
e\Big( (m_1 - 2m_2)\frac{a}{q}\Big)  - \sum_{m_1, m_2 \le N}
C_q(a) \nonumber\\ &=& C_q(a) N^2 - N^2 C_q(a) \ = \ 0. \eea

Thus, because of our choice of functions, the leading terms
vanish, and the remaining term is small.

\subsubsection{Second Piece}

We now study the second piece. Note $|\beta| \le \frac{Q}{N} =
\frac{\log^2 B}{N}$, and $C_q(a) = \frac{c_q(a)}{\phi^2(q)}
\frac{c_q(-2a)}{\phi^2(q)}$.

Up to lower order terms, the $m_2$-sum will leave us with

\be \beta \frac{c_q(-2a)N}{\phi(q)}\int_0^N \sum_{m_1\le t} \Bigg[
\gl(m_1) e\Big( m_1 \frac{a}{q} \Big) - \frac{c_q(a)}{\phi(q)}
\Bigg] e(t\beta) dt. \ee

Note $f_N(x)$ is a multiple of $N^2$ for $x$ near $\frac{a}{q}$.
Thus, we want to make sure the above is well dominated by $N^2$.

For $t \le \sqrt{N}$, this is immediate. For $t \ge \sqrt{N}$,
using Siegel-Walfisz (Theorem \ref{thmsiegelw}), we can make the
bracketed quantity in the integrant dominated by $\frac{N}{\log^C
N}$ for any $C$ when $q \le \log^B N$. Thus, we integrate a
quantity that is at most $\frac{N}{\log^C N}$ over an interval of
length $N$, we multiply by $N\beta \ll Q = \log^B N$.

Thus, choosing $C$ appropriately, the integral contributes
$\frac{N^2}{\log^{C-B} N}$, and hence is negligible.


\begin{rek} Note, of course, that the contribution is only
negligible while $|\beta| \le \frac{Q}{N}$.
\end{rek}


\begin{lem} $S_{1\sum;a,q}$ is a lower order correction. \end{lem}


\subsection{$S_{1\int;a,q}$}

We must evaluate

\bea S_{1\int;a,q} & \ = \ & \sum_{m_1 \le N} 4 \pi i \beta
\int_0^N \sum_{m_2 \le u} a_{m_2} e(-u\beta) du \cdot e(m_1
\beta), \eea

where

\be a_{m_2} \ = \ \Big[ \gl(m_1)\gl(m_2)e\Big( (m_1 -
2m_2)\frac{a}{q}\Big)  - C_q(a) \Big]. \ee

We bring the sum over $m_1$ inside the integral and again use
Partial Summation.

We will ignore the integration and $\beta$ for now, as these will
contribute $\beta N \ll Q = \log^B N$ times the maximum value of
the integrand. We will leave the $e(-u\beta) du$ with this
integration.

When $u \le \sqrt{N}$, we can immediately show the above is a
lower order correction. Thus, below we always assume $u \ge
\sqrt{N}$.


\subsubsection{First Piece}

We have

\bea S_{1\int\sum;a,q} &=& \sum_{m_1 \le N \atop m_2\le u} \Big[
\gl(m_1)\gl(m_2)e\Big( (m_1 - 2m_2)\frac{a}{q}\Big)  - C_q(a)
\Big] e(N\beta) \nonumber\\ &=& e(N\beta) \Bigg[ \sum_{m_1 \le N
\atop m_2\le u} \gl(m_1)\gl(m_2)e\Big( (m_1 -
2m_2)\frac{a}{q}\Big) - C_q(a) \sum_{m_1 \le N \atop m_2\le u} 1
\Bigg]. \nonumber\\ & = &  e(N\beta) \Bigg[ C_q(a) uN - C_q(a)uN +
\text{Lower Order Terms} \Bigg], \eea

where by the Siegel-Walfisz Theorem (Theorem \ref{thmsiegelw}),
the error in the bracketed quantity is of size $\frac{uN}{\log^C
N}$.

We then integrate from $u = \sqrt{N}$ to $N$ and multiply by
$\beta$, giving a contribution bounded by

\be \beta N \cdot \frac{N^2}{\log^C N} \ \ll \ \frac{\log^B}{N}
\frac{N^3}{\log^C N} \ \ll \ \frac{N^2}{\log^{C-B} N}, \ee

again getting a lower order correction to $f_N(x)$ for $x$ near
$\frac{a}{q}$ (remember $f_N(x)$ is of size $N^2$).


\subsubsection{Second Piece}

Again, $u \ge \sqrt{N}$, and we have

\be 2\pi i \beta \int_0^N \sum_{m_1 \le t} \Bigg[ \sum_{m_2 \le u}
\Big[ \gl(m_1)\gl(m_2)e\Big( (m_1 - 2m_2)\frac{a}{q}\Big)  -
C_q(a) \Big]  \Bigg]e(t\beta)dt.  \ee

Again, for $t \le \sqrt{N}$, the contribution will be a lower
order correction. For $t, u \ge \sqrt{N}$,

Again, executing the sum over $m_1$ and $m_2$ will give us

\be C_q(a) ut - C_q(a)ut + \text{Lower Order Terms}, \ee

with the lower order terms of size $\frac{ut}{\log^C N}$.

Integrating over $t$ (from $\sqrt{N}$ to $N$), then integrating
over $u$ (from $\sqrt{N}$ to $N$) and then multiplying by
$\beta^2$ gives an error bounded by

\be \beta^2 N^2 \cdot \frac{N^2}{\log^C N} \ \ll \ \frac{\log^{2B}
N}{N^2} \frac{N^4}{\log^C N} \ \ll \ \frac{N^2}{\log^{C - 2B} N},
\ee

again a lower order correction.



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Integrals of $u(x)$}

\subsection{Formulations}

Remember

\be u(x) \ = \ \sum_{m_1,m_2 \le N} e\Big( (m_1 - 2m_2)x \Big).
\ee

We need to study $\int_{-\foh}^\foh f_N(x) e(-x)dx$. We have shown
that

\be f_N(\alpha) \ = \ C_q(a) u\Big( \alpha - \frac{a}{q} \Big) +
O\Big( \frac{N^2}{\log^{C-2B} N} \Big), \ \ \alpha \in
\mathcal{M}_{a,q}. \ee

Thus, we must evaluate

\bea \int_{\mathcal{M}_{a,q}} u\Big( \alpha - \frac{a}{q} \Big)
\cdot e(-\alpha)d\alpha  &=&
\int_{\frac{a}{q}-\frac{Q}{N}}^{\frac{a}{q}+\frac{Q}{N}} u\Big(
\alpha - \frac{a}{q} \Big) \cdot e(-\alpha) d\alpha \nonumber\\
&=& \int_{-\frac{Q}{N}}^{\frac{Q}{N}} u(\beta) \cdot
e\Big(-\frac{q}{q} - \beta\Big) d\beta \nonumber\\ &=& e\Big(
-\frac{a}{q} \Big) \int_{-\frac{Q}{N}}^{\frac{Q}{N}} u(\beta)
e(-\beta)d\beta. \eea


\subsection{$\int_{-\foh}^{\foh} u(x)
e(-x)dx$}

\bea \int_{-\foh}^{\foh} u(x) e(-x)dx &=& \int_{-\foh}^{\foh}
\sum_{m_1,m_2 \le N} e\Big( (m_1 - 2m_2)x \Big) \cdot e(-x)dx
\nonumber\\ &=& \sum_{m_1,m_2 \le N} \int_{-\foh}^{\foh} e\Big(
(m_1 - 2m_2 - 1)x \Big)dx. \eea

If $m_1 - 2m_2 - 1 = 0$, the integral gives $1$. There are
approximately $\frac{N}{2}$ ways to choose $m_1, m_2 \le N$ such
that $m_1 - 2m_2 - 1 = 0$.

Assume now $m_1 - 2m_2 - 1 \neq 0$. Then the integral vanishes.

Hence,

\begin{lem}
\be \int_{-\foh}^{\foh} u(x) e(-x)dx \ = \ \frac{N}{2} + O(1). \ee
\end{lem}


\subsection{ $\int_{-\foh}^{-\frac{Q}{N}} + \int_{\frac{Q}{N}}^{\foh} u(x)
e(-x)dx$}

Define

\bea I_1 & \ = \ & \Big[ -\foh, -\foh + \frac{Q}{N} \Big]
\nonumber\\ I_2 & \ = \ & \Big[ -\foh + \frac{Q}{N}, -\frac{Q}{N}
\Big] \nonumber\\ I_3 & \ = \ & \Big[ \frac{Q}{N}, \foh -
\frac{Q}{N} \Big] \nonumber\\ I_4 & \ = \ & \Big[ \foh -
\frac{Q}{N}, \foh \Big] \nonumber\\ I & \ = \ & I_1 \cup I_2 \cup
I_3 \cup I_4 . \eea

\subsection{Integral over $I_2, I_3$}

We have

\bea \int_{I_i} u(x) e(-x)dx &=& \int_{I_i} \sum_{m_1,m_2 \le N}
e\Big( (m_1 -2m_2 - 1)x\Big)dx \nonumber\\ &=& \int_{I_i}
\sum_{m_1\le N} e(m_1x) \sum_{m_2\le N} e(-2m_2 x) \cdot e(-x)dx
\nonumber\\ &=& \int_{I_i} \frac{e(x) - e( (N+1)x)}{1 - e(x)}
\frac{e(-2x) - e(-2(N+1)x)}{1 - e(-2x)} e(-x)dx. \nonumber\\ & &
\eea

On $I_2$ and $I_3$, the integral is

\be \ll \int_{I_i} \frac{2}{x} \frac{2}{x} dx \ \ll \ \frac{N}{Q}
\ = \ \frac{N}{\log^B N}, \ee

see, for example, Nathanson (Additive Number Theory: The Classical
Bases, Chapter $8$).

\subsection{Integral over $I_1, I_4$}

Each of these intervals has length $\frac{Q}{N} = \frac{\log^B
N}{N}$. There are $\frac{N}{2} + O(1)$ pairs such that $m_1 -2m_2
- 1 = 0$. Each of these pairs will contribute (bound the integrand
by $1$) $\frac{Q}{N}$. As there are at most $\frac{N}{2}$ pairs,
these contribute at most $\frac{N}{2} \frac{Q}{N}$ $\ll \log^B N$.

Henceforth we assume $m_1 - 2m_2 - 1 \neq 0$. We write

\be I_1 \cup I_4 \ = \ \Big[ \foh - \frac{Q}{N}, \foh +
\frac{Q}{N} \Big] \ = \ I'. \ee


We have

\bea\label{eqioneifour} & & \sum_{m_1,m_2 \le N \atop m_1 - 2m_2 -
1 \neq 0} \int_{I'} e\Big( (m_1 - 2m_2 - 1)x \Big) dx\nonumber\\ &
\ = \ & e\Big(-\foh\Big) \sum_{m_1,m_2 \le N \atop m_1 - 2m_2 - 1
\neq 0} (-1)^{m_1} \int_{-\frac{Q}{N}}^{\frac{Q}{N}} e\Big( (m_1 -
2m_2 - 1)x \Big) dx \nonumber\\ &=& e\Big(-\foh\Big)\frac{1}{2\pi
i} \sum_{m_1,m_2 \le N \atop m_1 - 2m_2 - 1 \neq 0} (-1)^{m_1}
\frac{2 \sin\Big( (m_1 - 2m_2 - 1)\frac{Q}{N}\Big)}{m_1 - 2m_2 -
1}, \eea

because, changing variables by sending $x$ to $(x - \foh) + \foh$
gives factors of $e\Big( (m_1 - 2m_2 - 1)\foh\Big)$ $= e(-\foh)
e(\frac{m_1}{2}) e(-m_2)$, and $e(\frac{m_1}{2}) = (-1)^{m_1}$.


\subsubsection{$0 < |m_1 - 2m_2 - 1| \le N^{1-\epsilon}$}

Let $w = m_1 - 2m_2 - 1$. We will do the case $0 < w \le
N^{1-\epsilon}$, the case with $-N^{1-\epsilon} > w > 0$ being
handled similarly.

For each $w$, there are at most $N$ pairs of $m_1, m_2$ giving
rise to such a $w$. For such $w$, $\frac{\sin( w \frac{Q}{N} )}{w}
\ll \frac{Q}{N}$ (because we are taking the sin of a quantity very
close to zero).

Thus, these pairs contribute at most

\be \ll \ N \cdot \frac{Q}{N} \ \ll \ Q \ = \ \log^B N. \ee

Inserting absolute values in Equation \ref{eqioneifour} gives a
contribution of at most $\log^B N$ for such $w$, $0 < w < N^{1 -
\epsilon}$.

\subsubsection{$N^{1-\epsilon} < |m_1 - 2m_2 - 1| \le N$}

Again, let $w = m_1 - 2m_2 - 1$ and assume $N^{1-\epsilon} < |w|
\le N$. We will only consider $w > 0$; $w < 0$ is handled
similarly.

The cancellation is due to the presence of the factor
$(-1)^{m_1}$; note that for the pair $(m_1,m_2)$ we only care
about the parity of $m_1$.

Consider $w$ and $w-1$.

For $m_1 - 2m_2 - 1 = w$, the solutions are

\bea m_1 = w + 3, & \ & m_2 = 1 \nonumber\\
m_1 = w + 5, & \ & m_2 = 2 \nonumber\\ m_1 = w + 7, & \ & m_2 = 3
\eea

and so on; thus there are about $\frac{N-w}{2}$ pairs, all with
parity $-(-1)^w$.

For $m_1 - 2m_2 - 1 = w - 1$, we again have about $\frac{N-w}{2}$
pairs, but now the parity is $(-1)^w$. Thus,  each of the
$\frac{N-w}{2}$ pairs with $m_1 - 2m_2 - 1 = w$ is matched with
one of the $\frac{N-w}{2}$ pairs with $m_1 - 2m_2 - 1 = w-1$, and
we are off by at most $O(1)$ pairs, which will contribute

\be \ll \ \sum_{w= N^{1-\epsilon}}^N \frac{1}{w} \ \ll \ \log N.
\ee

For the remaining terms, we subtract in pairs, using the first
order Taylor Expansion of $\sin(x)$. We have

\be \sum_{w= N^{1-\epsilon}}^N \Bigg[ \frac{\sin\Big( w
\frac{Q}{N} \Big)}{w} - \frac{\sin\Big( w\frac{Q}{N} - \frac{Q}{N}
\Big)}{w-1} \Bigg]. \ee

The Main Term of the Taylor Expansion gives $\ll \frac{1}{w^2}$,
which when summed over $w$ gives $\frac{1}{N^{1 - \epsilon}}$. As
we have about $\frac{N-w}{2} \ll N$ pairs for each $w$, this
contributes at most $N \cdot \frac{1}{N^{1 - \epsilon}}$ $\ll
N^\epsilon$.

We also have the first order term from the Taylor Expansion:

\be \sin\Big( w\frac{Q}{N} - \frac{Q}{N} \Big) \ = \ \sin\Big(
w\frac{Q}{N} \Big) + O\Big( \frac{Q}{N} \Big). \ee

This error leads to (remembering there are $\frac{N-w}{2} \ll N$
pairs for each $w$)

\be \ll \ N \sum_{w= N^{1-\epsilon}}^N \frac{ \frac{Q}{N} }{w-1} \
\ll \ Q \log N^\epsilon \ \ll \ \log^{B + 1} N. \ee


\subsection{Collecting the Pieces}

We have shown

\bea \int_{[-\foh,\foh]} u(x)e(-x)dx & \ = \ & \frac{N}{2} + O(1)
\nonumber\\ \int_{[-\foh,\foh] - [-\frac{Q}{N},\frac{Q}{N}]}
u(x)e(-x)dx & \ = \ & O\Big( \frac{N}{\log^B N} \Big). \eea

Therefore

\begin{lem}

\bea  \int_{-\frac{Q}{N}}^{\frac{Q}{N}} u(x) e(-x)dx & \ = \ &
\frac{N}{2} + O\Big(\frac{N}{\log^B N}\Big). \eea
\end{lem}


Remembering that we had

\bea \int_{\mathcal{M}_{a,q}} u\Big( \alpha - \frac{a}{q} \Big)
\cdot e(-\alpha)d\alpha  &=&
\int_{\frac{a}{q}-\frac{Q}{N}}^{\frac{a}{q}+\frac{Q}{N}} u\Big(
\alpha - \frac{a}{q} \Big) \cdot e(-\alpha) d\alpha \nonumber\\
&=& \int_{-\frac{Q}{N}}^{\frac{Q}{N}} u(\beta) \cdot
e\Big(-\frac{q}{q} - \beta\Big) d\beta \nonumber\\ &=& e\Big(
-\frac{a}{q} \Big) \int_{-\frac{Q}{N}}^{\frac{Q}{N}} u(\beta)
e(-\beta)d\beta, \eea

we see that

\begin{lem}

\be \int_{\mathcal{M}_{a,q}} u\Big( \alpha - \frac{a}{q} \Big)
\cdot e(-\alpha)d\alpha  \ = \ e\Big( -\frac{a}{q} \Big)
\frac{N}{2}. \ee

\end{lem}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Determination of the Main Term}

We now calculate the contribution from the Major Arcs. Up to lower
order terms,

\bea \int_{ \mathcal{M} } f_N(x) e(-x)dx &=& \sum_{q \le Q}
\sum_{a = 1 \atop (a,q)=1}^{q} \int_{ \frac{a}{q} - \frac{Q}{N}
}^{\frac{a}{q} + \frac{Q}{N}} f_N(\alpha) e(-\alpha) d\alpha
\nonumber\\ &=& \sum_{q \le Q} \sum_{a = 1 \atop (a,q)=1}^{q}
\int_{ \frac{a}{q} - \frac{Q}{N} }^{\frac{a}{q} + \frac{Q}{N}}
C_q(a) u\Big( \alpha - \frac{a}{q} \Big) e(-\alpha)d\alpha \nonumber\\
&=& \sum_{q \le Q} \sum_{a = 1 \atop (a,q)=1}^{q} e\Big( -
\frac{a}{q} \Big) \int_{ -
\frac{Q}{N} }^{\frac{Q}{N}} C_q(a) u(\beta) e(-\beta) \nonumber\\
&=& \sum_{q \le Q} \sum_{a = 1 \atop (a,q)=1}^{q} C_q(a) e\Big( -
\frac{a}{q} \Big) \frac{N}{2} \nonumber\\ &=&  \frac{N}{2} \sum_{q
\le Q} \sum_{a = 1 \atop (a,q)=1}^{q}
\frac{c_q(a)c_q(-2a)}{\phi^2(q)} \cdot e\Big( - \frac{a}{q} \Big)
\nonumber\\ &=&  \frac{N}{2} \sum_{q=1}^Q \Bigg[\sum_{a = 1 \atop
(a,q)=1}^q C_q(a) e\Big( - \frac{a}{q} \Big) \Bigg] \nonumber\\
&=& \frac{N}{2} \sum_{q=1}^Q \rho_q \nonumber\\ &=& \goth{S}_N
\frac{N}{2}, \eea

where we have defined

\bea c_q(a) & \ = \ & \sum_{r = 1 \atop (r,q) = 1}^q e\Big( r
\frac{a}{q} \Big)
\nonumber\\ C_q(a) &\ = \ & \frac{c_q(a)c_q(-2a)}{\phi^2(q)} \nonumber\\
\rho_q &\ = \ & \sum_{a = 1 \atop
(a,q)=1}^q C_q(a) e\Big( - \frac{a}{q} \Big) \nonumber\\
\goth{S}_N & \ = \ & \sum_{a = 1 \atop (a,q)=1}^{q} \rho_q. \eea



\subsection{Properties of $C_q(a)$ and $\rho_q$}

We will follow the presentation of Nathanson (Additive Number
Theory: The Classical Bases, Chapter $8$ and Appendix $A$).

\subsubsection{$c_q(a)$ is Multiplicative}

We follow Nathanson, Pages $320-321$, Theorem $A.23$. Note that we
are labeling by $r$ what he labels $a$, and we are labeling by $a$
what he labels $n$.

\begin{lem} $c_q(a)$ is multiplicative; ie, if $(q,q') = 1$, then
$c_{qq'}(a) = c_q(a)c_{q'}(a)$. \end{lem}

Proof: We have

\be \sum_{\widetilde{r}=1 \atop (\widetilde{r},qq') = 1}^{qq'}
e\Big( \widetilde{r} \frac{a}{qq'} \Big). \ee

\begin{exe} Show that we can write the $\widetilde{r}$s above as
$\widetilde{r} \equiv rq' + r'q$ mod $qq'$, where $1 \le r \le q$,
$1 \le r' \le q'$, and $(r,q) = (r',q') = 1$. \end{exe}

Thus

\bea c_q(a)c_{q'}(a) &=& \sum_{r = 1 \atop (r,q)=1}^q e\Big(r
\frac{a}{q} \Big) \sum_{r' = 1 \atop (r',q')=1}^{q'} e\Big(r'
\frac{a}{q'} \Big) \nonumber\\ &=& \sum_{r = 1 \atop (r,q)=1}^q
\sum_{r' = 1 \atop (r',q')=1}^{q'} e\Big( \frac{(rq' + r'q)a}{qq'}
\Big) \nonumber\\ &=& \sum_{\widetilde{r} = 1 \atop
(\widetilde{r},qq')=1}^{qq'} e\Big( \widetilde{r}\frac{a}{q} \Big)
\ = \ c_{qq'}(a). \eea


\subsubsection{$c_q(a)$ for $(a,q) = 1$}

\begin{exe} Show that

\be \twocase{h_d(a) \ = \  \sum_{r=1}^d e\Big( r\frac{a}{d} \Big)
\ = }{d}{if $d|a$}{0}{otherwise} \ee

\end{exe}

Recall the moebius function:

\be \twocase{\mu(d) \ = \ }{(-1)^r}{if $d$ is the product of $r$
distinct primes}{0}{otherwise} \ee

\begin{exe} Prove
\be \twocase{\sum_{d|(r,q)} \mu(d) \ = \ }{1}{if $(r,q) =
1$}{0}{otherwise} \ee
\end{exe}



Then

\bea c_q(a) &=& \sum_{r=1 \atop (r,q)=1}^q e\Big( r\frac{a}{q}
\Big) \nonumber\\ &=& \sum_{r=1}^q e\Big( r\frac{a}{q} \Big)
\sum_{d|(r,q)} \mu(d) \nonumber\\ &=& \sum_{d|q} \mu(d) \sum_{r=1
\atop d|r}^q e\Big( r\frac{a}{q} \Big) \nonumber\\ &=& \sum_{d|q}
\mu(d) \sum_{l = 1}^{\frac{q}{d}} e\Big( l\frac{a}{ \frac{q}{d}}
\Big) \nonumber\\ &=& \sum_{d|q} \mu(d) h_{\frac{q}{d}}(a)
\nonumber\\ &=& \sum_{d|q} \mu\Big( \frac{q}{d} \Big) h_d(a)
\nonumber\\ &=& \sum_{d|q \atop d|a} \mu\Big( \frac{q}{d} \Big)
\cdot d \nonumber\\ &=& \sum_{d|(a,q)} \mu\Big( \frac{q}{d}
\Big)d. \eea

Note that if $(a,q) = 1$, then there is only one term above,
namely $d = 1$, which yields

\be c_q(a) \ = \ \mu(q) \ \ \text{if} \ (a,q) = 1. \ee

\begin{cor} If $q = p^k$, $k \ge 2$ and $(a,q) = 1$, then $c_q(a)
= 0$. \end{cor}


\subsubsection{$C_q(a)$ is Multiplicative}

We have shown $c_{qq'}(a) = c_q(a)c_{q'}(a)$ if $(q,q') = 1$.
Recall the Euler phi-function, $\phi(q)$, is the number of numbers
less than $q$ which are relatively prime to $q$.

\begin{exe} Prove that $\phi(q)$ is multiplicative; ie, if
$(q,q')=1$, then $\phi(qq') = \phi(q)\phi(q')$. \end{exe}

We now have

\begin{lem} $C_q(a)$ is multiplicative. \end{lem}

Proof: Assume $(q,q') = 1$. We have

\bea C_{qq'}(a) &=& \frac{ c_{qq'}(a) c_{qq'}(-2a)}{\phi^2(qq')}
\nonumber\\ &=& \frac{ c_q(a)c_{q'}(a) c_{q}(-2a)
c_{q'}(-2a)}{\phi^2(q) \phi^2(q')} \nonumber\\ &=& \frac{
c_q(a)c_{q}(-2a)}{\phi^2(q)} \cdot \frac{ c_{q'}(a)
c_{q'}(-2a)}{\phi^2(q')} \nonumber\\ &=& C_q(a) C_{q'}(a). \eea



\subsubsection{$\rho_q$ is Multiplicative}

We first prove a needed lemma.

\begin{lem} Consider $C_{q_1}(a_1q_2)$. Then
\be C_{q_1}(a_1q_2) \  = \  C_{q_1}(a_1) \ee if $(q_1,q_2) = 1$.
\end{lem}

Proof:

\bea C_{q_1}(a_1q_2) &=& \sum_{r_1 = 1 \atop (r_1,q_1)=1}^{q_1}
e\Big( r_1 \frac{a_1q_2}{q_1} \Big) \nonumber\\ &=& \sum_{r_1 = 1
\atop (r_1,q_1)=1}^{q_1} e\Big( r_1q_2 \frac{a_1}{q_1} \Big)
\nonumber\\ &=& \sum_{r = 1 \atop (r,q_1)=1}^{q_1} e\Big( r
\frac{a_1}{q_1} \Big) \ = \ C_{q_1}(a), \eea

because $(q_1,q_2) = 1$ implies that as $r_1$ goes through all
residue classes that are relatively prime to $q_1$, so too does $r
=  r_1q_2$. $\Box$

\begin{lem} $\rho_q$ is multiplicative. \end{lem}

Recall

\be \rho_q \ = \ \sum_{a = 1 \atop (a,q)=1}^q C_q(a) e\Big( -
\frac{a}{q} \Big). \ee

Assume $(q_1,q_2) = 1$. Then we can write the congruence classes
mod $q_1q_2$ as $a_1q_2 + a_2q_1$, with $1 \le a_1 \le q_1$, $1
\le a_2 \le q_2$ and $(a_1,q_1) = (a_2,q_2) = 1$.

\bea \rho_{q_1q_2} &=& \sum_{a = 1 \atop (a,q_1q_2) = 1}^{q_1q_2}
C_{q_1q_2}(a) e\Big( -\frac{a}{q_1q_2} \Big) \nonumber\\ &=&
\sum_{a = 1 \atop (a,q_1q_2) = 1}^{q_1q_2}
C_{q_1}(a) C_{q_2}(a) e\Big( -\frac{a}{q_1q_2} \Big) \nonumber\\
&=& \sum_{a_1 = 1 \atop (a_1,q_1) = 1}^{q_1} \sum_{a_2 = 1 \atop
(a_2,q_2) = 1}^{q_2} C_{q_1}(a_1q_2+a_2q_1) C_{q_2}(a_1q_2+a_2q_1)
e\Big( -\frac{a_1q_2+a_2q_1}{q_1q_2} \Big). \nonumber\\ & & \eea

\begin{exe} With $a_1, a_2, q_1, q_2$ as above,

\be C_{q_1}(a_1q_2+a_2q_1) \ = \ C_{q_1}(a_1q_2) \ \ \text{and} \
\ C_{q_2}(a_1q_2+a_2q_1) \ = \ C_{q_2}(a_2 q_1). \ee

\end{exe}

Thus, we have

\bea \rho_{q_1q_2} &=& \sum_{a_1 = 1 \atop (a_1,q_1) = 1}^{q_1}
\sum_{a_2 = 1 \atop (a_2,q_2) = 1}^{q_2} C_{q_1}(a_1q_2)
C_{q_2}(a_2 q_1) e\Big( -\frac{a_1q_2+a_2q_1}{q_1q_2} \Big)
\nonumber\\ &=& \sum_{a_1 = 1 \atop (a_1,q_1) = 1}^{q_1}
C_{q_1}(a_1q_2) e\Big( -\frac{a_1}{q_1} \Big) \sum_{a_2 = 1 \atop
(a_2,q_2) = 1}^{q_2} C_{q_2}(a_2q_1) e\Big( -\frac{a_2}{q_2} \Big)
\nonumber\\ &=& \sum_{a_1 = 1 \atop (a_1,q_1) = 1}^{q_1}
C_{q_1}(a_1) e\Big( -\frac{a_1}{q_1} \Big) \sum_{a_2 = 1 \atop
(a_2,q_2) = 1}^{q_2} C_{q_2}(a_2) e\Big( -\frac{a_2}{q_2} \Big)
\nonumber\\ &=& \rho_{q_1} \cdot \rho_{q_2}.  \eea

Thus, $\rho_q$ is multiplicative. $\Box$

\subsubsection{Calculation of $\rho_q$}

\begin{lem}\label{lemrhopk}
$\rho_{p^k} = 0$ if $k \ge 2$ and $p$ is a prime. \end{lem}

Proof: This follows immediately from $C_{p^k}(a) = 0$. $\Box$

\begin{lem}\label{lemrhop} If $p > 2$ is prime, $\rho_p = - \frac{1}{(p-1)^2}$.
\end{lem}

Proof:

\bea \rho_p &=& \sum_{a = 1\atop (a,p) = 1}^p C_p(a)
e\Big(-\frac{a}{p} \Big) \nonumber\\ &=& \sum_{a=1}^{p-1} \frac{
c_p(a) c_p(-2a)}{\phi^2(p)} e\Big(-\frac{a}{p} \Big). \eea

But as $p > 2$, $c_p(a) = c_p(-2a) = \mu(p)$ as $(a,p) = 1$. As
$\mu^2(p) = 1$ and $\phi(p) = p-1$ we have

\bea \rho_p &=& \sum_{a=1}^{p-1} \frac{1}{(p-1)^2}
e\Big(-\frac{a}{p} \Big) \nonumber\\ &=& \frac{1}{(p-1)^2} \Bigg[
- e\Big( -\frac{0}{p} \Big) + \sum_{a=0}^{p-1} e\Big( -
\frac{a}{p} \Big) \Bigg] \nonumber\\ &=& -\frac{1}{(p-1)^2}. \eea

\begin{lem}\label{lemrhotwo} If $p = 2$, then $\rho_2 = 1$. \end{lem}

Proof:

\bea \rho_2 &=& \sum_{a = 1\atop (a,2) = 1}^2 C_2(a)
e\Big(-\frac{a}{2} \Big) \nonumber\\ &=& C_2(1) e\Big(-
\frac{1}{2} \Big) \nonumber\\ &=& \frac{c_2(1) c_2(-2)}{\phi^2(2)}
\cdot e^{-\pi i} \nonumber\\ &=& \frac{e^{\pi i} e^{-2\pi i}}{1^2}
\cdot e^{-\pi i} \ = \ 1, \eea

where we have used $c_2(1) = e^{\pi i}$ and $c_2(-2) = e^{-2\pi
i}$.

\begin{exe} Prove  $c_2(1) = e^{\pi i}$ and $c_2(-2) = e^{-2\pi
i}$. \end{exe}


\subsection{Determination of $\goth{S}_N$ and $\goth{S}$}

Recall

\be \goth{S}_N \ = \ \sum_{q \le Q} \rho_q. \ee

We define

\be \goth{S} \ = \ \sum_q \rho_q. \ee

\begin{exe} Let $h_q$ be any multiplicative sequence (with
whatever growth conditions are necessary to ensure the convergence
of all sums below). Then

\be \sum_q h_q \ = \ \prod_{p \ \text{prime} } \Big( 1 +
\sum_{k=1}^\infty h_{p^k} \Big). \ee

\end{exe}

\subsubsection{$\goth{S}$}

We have

\bea \goth{S} &=& \sum_q \rho_q \nonumber\\ &=& \prod_{p \
\text{prime} } \Big( 1 + \sum_{k=1}^\infty \rho_{p^k} \Big)
\nonumber\\ &=& \prod_p \Big(1 + \rho_p\Big) \eea

because $\rho_{p^k} = 0$ for $k \ge 2$ and $p$ prime by Lemma
\ref{lemrhopk}. We have previously shown (see Lemmas \ref{lemrhop}
and \ref{lemrhotwo}) that $\rho_2 = 1$ and $\rho_p =
-\frac{1}{(p-1)}$ for $p > 2$ prime. Therefore

\bea \goth{S} &=& \prod_p \Big(1 + \rho_p\Big) \nonumber\\ &=& (1
+ \rho_2) \prod_{p > 2} (1 + \rho_p) \nonumber\\ &=& 2 \prod_{p>2}
\Big[ 1 - \frac{1}{(p-1)^2} \Big] \nonumber\\ &=& 2 T_2, \eea

where

\begin{defi}[Twin Prime Constant]

\be T_2 \ = \ \prod_{p > 2} \Big[ 1 - \frac{1}{(p-1)^2} \Big] \
\approx \ .6601618158 \ee

is the twin prime constant.


\subsubsection{$\goth{S}_N$}

We need to estimate $|\goth{S} - \goth{S}_N|$. As $\rho_q$ is
multiplicative and zero if $q = p^k$ ($k \ge 2$), we see we need
only look at sums of $\rho_p$. As $\rho_p = -\frac{1}{(p-1)^2}$,
one can show that the difference between $\goth{S}$ and
$\goth{S}_N$ tends to zero as $N \to \infty$.



\end{defi}

Thus,


\begin{lem}
\be \goth{S} \ = \ 2 T_2. \ee
\end{lem}




\subsection{Number of Germain Primes and Weighted Sums}

Combining the above arguments, we have shown that, up to lower
order terms,

\bea \sum_{p \le N \atop p, \frac{p-1}{2} \ \text{prime} } \log(p)
\cdot \log\Big( \frac{p-1}{2} \Big) & \ = \ & \goth{S} \frac{N}{2}
\nonumber\\ & \ = \ & 2 T_2 \frac{N}{2} \nonumber\\ & \ = \ & T_2
N. \eea

Note that we are counting Germain prime pairs by $\Big(
\frac{p-1}{2},p\Big)$ and not $(p, 2p+1)$. Such a difference in
counting will introduce a factor of $2$.

We can pass from this weighted sum to a count of the number of
Germain prime pairs $\Big( \frac{p-1}{2},p\Big)$ with $p \le N$.

Again we follow Nathanson, Chapter $8$. Define

\bea \pi_G(N) & \ = \ & \sum_{p \le N \atop p, \frac{p-1}{2} \
\text{prime} } 1 \nonumber\\ G(N) & \ = \ & \sum_{p \le N \atop p,
\frac{p-1}{2} \ \text{prime} } \log(p) \cdot \log\Big(
\frac{p-1}{2} \Big). \eea

Clearly

\be G(N) \ \le \ \log^2 N \cdot \pi_G(N). \ee

Therefore,

\begin{lem} Up to lower order terms,
\be \pi_G(N) \ \ge \ \frac{G(N)}{\log^2 N} \ = \ \frac{T_2
N}{\log^2 N}. \ee
\end{lem}

We now provide a bound in the opposite direction.

\bea \pi_G(N^{1-\delta}) & \ = \ &  \sum_{p \le N^{1-\delta} \atop
p, \frac{p-1}{2} \ \text{prime} } 1 \ \ll \
\frac{N^{1-\delta}}{\log N}. \eea

Then

\bea G(N) &\ge &  \sum_{p \ge N^{1-\delta} \atop p, \frac{p-1}{2}
\ \text{prime} } \log p \cdot \log\Big( \frac{p-1}{2} \Big)
\nonumber\\ &=& (1-\delta)^2 \log^2 N \sum_{p \ge N^{1-\delta}
\atop p, \frac{p-1}{2} \ \text{prime} } 1 \nonumber\\ &=&
(1-\delta)^2 \log^2 N \Big( \pi_G(N) - \pi_G(N^{1-\delta})\Big)
\nonumber\\ &\ge & (1-\delta)^2 \log^2 N \pi_G(N) + O\Bigg(
(1-\delta)^2 \log^2 N \cdot \frac{N^{1-\delta}}{\log N}\Bigg).
\eea

Therefore

\bea \log^2 N \cdot \pi_G(N) & \le & (1-\delta)^{-2} \cdot G(N) +
O\Bigg( \log^2 N \cdot \frac{N^{1-\delta}}{\log N}\Bigg)
\nonumber\\ 0 \ \le \ \log^2 N \cdot \pi_G(N) - G(N) & \le & \Big[
(1 - \delta)^{-2} - 1\Big] G(N) + O\Big(\log N \cdot N^{1 -
\delta} \Big). \eea

If $0 < \delta < \foh$, then $(1 - \delta)^{-2} - 1 \ll \delta$.
We thus have

\bea 0 \ \le \ \log^2 N \cdot \pi_G(N) - G(N) & \ll & N
\Bigg[\delta + O\Big( \frac{\log N}{N^\delta} \Big) \Bigg]. \eea

Choose $\delta = \frac{2 \log \log N}{\log N}$. Then we get

\bea 0 \ \le \ \log^2 N \cdot \pi_G(N) - G(N) & \le & O\Big( N
\frac{\log \log N}{\log N} \Big). \eea

Recalling $G(N) \approx T_2 N$ gives

\begin{lem} \be \pi_G(N) \ \le \ \frac{T_2 N}{\log^2 N}. \ee
\end{lem}

Combining with the other bound we have finally shown

\begin{thm} Assuming there is no contribution to the main term
from the Minor Arcs, up to lower order terms we have

\be \pi_G(N) \ = \ \frac{T_2 N}{\log^2 N}, \ee

where $T_2$ is the twin prime constant

\be T_2 \ = \ \prod_{p > 2} \Big[ 1 - \frac{1}{(p-1)^2} \Big] \
\approx \ .6601618158. \ee

\end{thm}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%




\end{document}
